{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Neural Network Tutorial\n",
    "\n",
    "A walkthrough of PyTorch's Neural Network Tutorial (https://pytorch.org/tutorials/beginner/pytorch_with_examples.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of samples, input dimension, number of nodes in the hidden layer, output dimension\n",
    "N, D1, H, D2 = 64, 1000, 100, 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randn(N, D1) # input data\n",
    "Y = np.random.randn(N, D2) # output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.random.randn(D1, H) # weights matrix for the first layer\n",
    "W2 = np.random.randn(H, D2) # weights matrix for the second layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1e-6 # learning rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, loss 28485072.36897279\n",
      "Iteration: 1, loss 21134344.020322382\n",
      "Iteration: 2, loss 16190726.429168245\n",
      "Iteration: 3, loss 12115100.345132908\n",
      "Iteration: 4, loss 8710814.104630012\n",
      "Iteration: 5, loss 6099054.674983146\n",
      "Iteration: 6, loss 4232124.953973087\n",
      "Iteration: 7, loss 2970356.2743779104\n",
      "Iteration: 8, loss 2135890.968554906\n",
      "Iteration: 9, loss 1585644.5430753976\n",
      "Iteration: 10, loss 1216481.250552211\n",
      "Iteration: 11, loss 961714.8800290413\n",
      "Iteration: 12, loss 779751.486102947\n",
      "Iteration: 13, loss 645503.0704887168\n",
      "Iteration: 14, loss 542971.6157200529\n",
      "Iteration: 15, loss 462413.8171251416\n",
      "Iteration: 16, loss 397706.5507901727\n",
      "Iteration: 17, loss 344738.5471359048\n",
      "Iteration: 18, loss 300708.45241367666\n",
      "Iteration: 19, loss 263659.83502893243\n",
      "Iteration: 20, loss 232196.59065438036\n",
      "Iteration: 21, loss 205262.15254597244\n",
      "Iteration: 22, loss 182070.06859280923\n",
      "Iteration: 23, loss 161993.1431249\n",
      "Iteration: 24, loss 144562.64248700213\n",
      "Iteration: 25, loss 129344.67006491194\n",
      "Iteration: 26, loss 116006.02655399722\n",
      "Iteration: 27, loss 104279.7764878183\n",
      "Iteration: 28, loss 93936.25713919933\n",
      "Iteration: 29, loss 84766.10981997359\n",
      "Iteration: 30, loss 76633.43215633035\n",
      "Iteration: 31, loss 69410.17436732311\n",
      "Iteration: 32, loss 62974.08189854819\n",
      "Iteration: 33, loss 57231.029690535914\n",
      "Iteration: 34, loss 52103.53142844926\n",
      "Iteration: 35, loss 47507.756823101074\n",
      "Iteration: 36, loss 43382.400714219446\n",
      "Iteration: 37, loss 39675.2950329604\n",
      "Iteration: 38, loss 36332.462359599966\n",
      "Iteration: 39, loss 33313.71741368772\n",
      "Iteration: 40, loss 30587.04036677564\n",
      "Iteration: 41, loss 28116.148629689153\n",
      "Iteration: 42, loss 25874.49493472188\n",
      "Iteration: 43, loss 23838.22243512463\n",
      "Iteration: 44, loss 21985.338350947328\n",
      "Iteration: 45, loss 20297.572131328787\n",
      "Iteration: 46, loss 18757.41161925973\n",
      "Iteration: 47, loss 17350.35749118031\n",
      "Iteration: 48, loss 16063.558812215935\n",
      "Iteration: 49, loss 14885.402418115878\n",
      "Iteration: 50, loss 13805.568545173635\n",
      "Iteration: 51, loss 12814.783903927062\n",
      "Iteration: 52, loss 11904.679757737114\n",
      "Iteration: 53, loss 11067.594942234442\n",
      "Iteration: 54, loss 10297.029067264943\n",
      "Iteration: 55, loss 9587.081224411275\n",
      "Iteration: 56, loss 8932.336953506687\n",
      "Iteration: 57, loss 8328.093997898606\n",
      "Iteration: 58, loss 7770.367505673261\n",
      "Iteration: 59, loss 7255.242367174915\n",
      "Iteration: 60, loss 6778.4780072511985\n",
      "Iteration: 61, loss 6336.881546870858\n",
      "Iteration: 62, loss 5927.564241558454\n",
      "Iteration: 63, loss 5547.824108035606\n",
      "Iteration: 64, loss 5195.240543223837\n",
      "Iteration: 65, loss 4867.707057110049\n",
      "Iteration: 66, loss 4563.168391896488\n",
      "Iteration: 67, loss 4279.8985574913095\n",
      "Iteration: 68, loss 4016.192893323605\n",
      "Iteration: 69, loss 3770.51887405243\n",
      "Iteration: 70, loss 3541.4873912409143\n",
      "Iteration: 71, loss 3327.905191310252\n",
      "Iteration: 72, loss 3128.568412978018\n",
      "Iteration: 73, loss 2942.4352311422217\n",
      "Iteration: 74, loss 2768.5697316395936\n",
      "Iteration: 75, loss 2605.9948920155775\n",
      "Iteration: 76, loss 2453.9449596079676\n",
      "Iteration: 77, loss 2311.652962866632\n",
      "Iteration: 78, loss 2178.4410140814352\n",
      "Iteration: 79, loss 2053.6644462599634\n",
      "Iteration: 80, loss 1936.7496250315207\n",
      "Iteration: 81, loss 1827.1204620392796\n",
      "Iteration: 82, loss 1724.3035070425128\n",
      "Iteration: 83, loss 1627.8474314839928\n",
      "Iteration: 84, loss 1537.3131126919209\n",
      "Iteration: 85, loss 1452.2540545112392\n",
      "Iteration: 86, loss 1372.3334969277485\n",
      "Iteration: 87, loss 1297.2184391217547\n",
      "Iteration: 88, loss 1226.6183715109628\n",
      "Iteration: 89, loss 1160.2186504305678\n",
      "Iteration: 90, loss 1097.7257365252196\n",
      "Iteration: 91, loss 1038.900803605556\n",
      "Iteration: 92, loss 983.4960285068003\n",
      "Iteration: 93, loss 931.3087722524499\n",
      "Iteration: 94, loss 882.1162972169119\n",
      "Iteration: 95, loss 835.737877382948\n",
      "Iteration: 96, loss 792.0059302545217\n",
      "Iteration: 97, loss 750.7492222119934\n",
      "Iteration: 98, loss 711.8214335752903\n",
      "Iteration: 99, loss 675.0756390548984\n",
      "Iteration: 100, loss 640.3797960468496\n",
      "Iteration: 101, loss 607.6008354965148\n",
      "Iteration: 102, loss 576.6385287666037\n",
      "Iteration: 103, loss 547.3713243479178\n",
      "Iteration: 104, loss 519.7060731107551\n",
      "Iteration: 105, loss 493.54274700868245\n",
      "Iteration: 106, loss 468.79208197386896\n",
      "Iteration: 107, loss 445.374902967804\n",
      "Iteration: 108, loss 423.2158721187907\n",
      "Iteration: 109, loss 402.2447693806687\n",
      "Iteration: 110, loss 382.3835176241407\n",
      "Iteration: 111, loss 363.577333908992\n",
      "Iteration: 112, loss 345.7577590473861\n",
      "Iteration: 113, loss 328.87486583123336\n",
      "Iteration: 114, loss 312.8734186988874\n",
      "Iteration: 115, loss 297.7059607893213\n",
      "Iteration: 116, loss 283.3266680343606\n",
      "Iteration: 117, loss 269.6874675067289\n",
      "Iteration: 118, loss 256.7493642160523\n",
      "Iteration: 119, loss 244.47271165667718\n",
      "Iteration: 120, loss 232.82512377825003\n",
      "Iteration: 121, loss 221.76927502842096\n",
      "Iteration: 122, loss 211.27140174789673\n",
      "Iteration: 123, loss 201.3025547012407\n",
      "Iteration: 124, loss 191.83456537787404\n",
      "Iteration: 125, loss 182.84190860731206\n",
      "Iteration: 126, loss 174.29785657302693\n",
      "Iteration: 127, loss 166.1788259361839\n",
      "Iteration: 128, loss 158.46082582401488\n",
      "Iteration: 129, loss 151.1249500787025\n",
      "Iteration: 130, loss 144.14954268666196\n",
      "Iteration: 131, loss 137.5148312705068\n",
      "Iteration: 132, loss 131.20463459085602\n",
      "Iteration: 133, loss 125.20276437991271\n",
      "Iteration: 134, loss 119.49163380393162\n",
      "Iteration: 135, loss 114.05600595994292\n",
      "Iteration: 136, loss 108.88250367962333\n",
      "Iteration: 137, loss 103.95804119487184\n",
      "Iteration: 138, loss 99.26974431793582\n",
      "Iteration: 139, loss 94.80600671281297\n",
      "Iteration: 140, loss 90.55345021116446\n",
      "Iteration: 141, loss 86.50249746402133\n",
      "Iteration: 142, loss 82.64374582864943\n",
      "Iteration: 143, loss 78.96639117614433\n",
      "Iteration: 144, loss 75.46229811074797\n",
      "Iteration: 145, loss 72.12304105829983\n",
      "Iteration: 146, loss 68.93962073352827\n",
      "Iteration: 147, loss 65.90445921026978\n",
      "Iteration: 148, loss 63.01009912301352\n",
      "Iteration: 149, loss 60.249891681022376\n",
      "Iteration: 150, loss 57.61784977064772\n",
      "Iteration: 151, loss 55.10675243978038\n",
      "Iteration: 152, loss 52.71109398486617\n",
      "Iteration: 153, loss 50.424903695707826\n",
      "Iteration: 154, loss 48.243202543717146\n",
      "Iteration: 155, loss 46.16078491860213\n",
      "Iteration: 156, loss 44.173248915753504\n",
      "Iteration: 157, loss 42.27595308155287\n",
      "Iteration: 158, loss 40.46517523018488\n",
      "Iteration: 159, loss 38.73570782376135\n",
      "Iteration: 160, loss 37.084131954911044\n",
      "Iteration: 161, loss 35.506713448120564\n",
      "Iteration: 162, loss 33.99973324634061\n",
      "Iteration: 163, loss 32.55995404355277\n",
      "Iteration: 164, loss 31.184313969577232\n",
      "Iteration: 165, loss 29.86955936838371\n",
      "Iteration: 166, loss 28.613078586524836\n",
      "Iteration: 167, loss 27.41206116828517\n",
      "Iteration: 168, loss 26.263926750474575\n",
      "Iteration: 169, loss 25.166131039584975\n",
      "Iteration: 170, loss 24.116532020078584\n",
      "Iteration: 171, loss 23.11274644463871\n",
      "Iteration: 172, loss 22.152841753598544\n",
      "Iteration: 173, loss 21.234570988340185\n",
      "Iteration: 174, loss 20.356220002635283\n",
      "Iteration: 175, loss 19.515863428980072\n",
      "Iteration: 176, loss 18.711876027697613\n",
      "Iteration: 177, loss 17.94265081409192\n",
      "Iteration: 178, loss 17.206373726520628\n",
      "Iteration: 179, loss 16.501698371147427\n",
      "Iteration: 180, loss 15.827239345318574\n",
      "Iteration: 181, loss 15.18156338123206\n",
      "Iteration: 182, loss 14.563397227138594\n",
      "Iteration: 183, loss 13.971599947319241\n",
      "Iteration: 184, loss 13.404831790251638\n",
      "Iteration: 185, loss 12.862038880418325\n",
      "Iteration: 186, loss 12.34219019420584\n",
      "Iteration: 187, loss 11.844270578663735\n",
      "Iteration: 188, loss 11.367337110249903\n",
      "Iteration: 189, loss 10.910422348952354\n",
      "Iteration: 190, loss 10.47268738869382\n",
      "Iteration: 191, loss 10.05317366984104\n",
      "Iteration: 192, loss 9.65122333970974\n",
      "Iteration: 193, loss 9.265994751573327\n",
      "Iteration: 194, loss 8.896765307967359\n",
      "Iteration: 195, loss 8.542869350318128\n",
      "Iteration: 196, loss 8.203642302279256\n",
      "Iteration: 197, loss 7.878447391615264\n",
      "Iteration: 198, loss 7.566604994661598\n",
      "Iteration: 199, loss 7.267628181468208\n",
      "Iteration: 200, loss 6.980911079094776\n",
      "Iteration: 201, loss 6.705959643498533\n",
      "Iteration: 202, loss 6.44227337623202\n",
      "Iteration: 203, loss 6.18935280350803\n",
      "Iteration: 204, loss 5.946774569986678\n",
      "Iteration: 205, loss 5.714017985301208\n",
      "Iteration: 206, loss 5.490742946643327\n",
      "Iteration: 207, loss 5.2765068866652864\n",
      "Iteration: 208, loss 5.07094378448928\n",
      "Iteration: 209, loss 4.873685750449201\n",
      "Iteration: 210, loss 4.684391742612127\n",
      "Iteration: 211, loss 4.502740644473131\n",
      "Iteration: 212, loss 4.328359325702606\n",
      "Iteration: 213, loss 4.1609702593997895\n",
      "Iteration: 214, loss 4.000290575060564\n",
      "Iteration: 215, loss 3.8460416120031264\n",
      "Iteration: 216, loss 3.697941285978056\n",
      "Iteration: 217, loss 3.555743062224664\n",
      "Iteration: 218, loss 3.4192204532682946\n",
      "Iteration: 219, loss 3.28811262611357\n",
      "Iteration: 220, loss 3.1621963036063736\n",
      "Iteration: 221, loss 3.041255034790683\n",
      "Iteration: 222, loss 2.925101296927374\n",
      "Iteration: 223, loss 2.813535124086678\n",
      "Iteration: 224, loss 2.7063652795012656\n",
      "Iteration: 225, loss 2.60342443432391\n",
      "Iteration: 226, loss 2.504515078527934\n",
      "Iteration: 227, loss 2.409479655098915\n",
      "Iteration: 228, loss 2.318168545746116\n",
      "Iteration: 229, loss 2.2304226871556194\n",
      "Iteration: 230, loss 2.1461103634684187\n",
      "Iteration: 231, loss 2.065075887422833\n",
      "Iteration: 232, loss 1.9872078688025476\n",
      "Iteration: 233, loss 1.9123577018717501\n",
      "Iteration: 234, loss 1.8404106398217914\n",
      "Iteration: 235, loss 1.7712594978166616\n",
      "Iteration: 236, loss 1.7047811195385667\n",
      "Iteration: 237, loss 1.6408677514397585\n",
      "Iteration: 238, loss 1.579422908164823\n",
      "Iteration: 239, loss 1.5203477888761239\n",
      "Iteration: 240, loss 1.4635550709178387\n",
      "Iteration: 241, loss 1.4089320236962593\n",
      "Iteration: 242, loss 1.3564051365312038\n",
      "Iteration: 243, loss 1.3058934413566772\n",
      "Iteration: 244, loss 1.2573150737171628\n",
      "Iteration: 245, loss 1.210594963802464\n",
      "Iteration: 246, loss 1.1656564399569131\n",
      "Iteration: 247, loss 1.1224388002680135\n",
      "Iteration: 248, loss 1.0808605509483264\n",
      "Iteration: 249, loss 1.0408660337723314\n",
      "Iteration: 250, loss 1.002389603046129\n",
      "Iteration: 251, loss 0.9653708125100697\n",
      "Iteration: 252, loss 0.9297525222057834\n",
      "Iteration: 253, loss 0.8954836854160151\n",
      "Iteration: 254, loss 0.8625142073218655\n",
      "Iteration: 255, loss 0.8307903114785825\n",
      "Iteration: 256, loss 0.8002569879760033\n",
      "Iteration: 257, loss 0.7708736328555896\n",
      "Iteration: 258, loss 0.7425969301835087\n",
      "Iteration: 259, loss 0.7153843613553059\n",
      "Iteration: 260, loss 0.6891916943784406\n",
      "Iteration: 261, loss 0.6639800399863673\n",
      "Iteration: 262, loss 0.6397141385340551\n",
      "Iteration: 263, loss 0.6163584539332311\n",
      "Iteration: 264, loss 0.5938720384124794\n",
      "Iteration: 265, loss 0.5722246884286857\n",
      "Iteration: 266, loss 0.5513833706661018\n",
      "Iteration: 267, loss 0.531319489807778\n",
      "Iteration: 268, loss 0.5120026063701277\n",
      "Iteration: 269, loss 0.49340163767475087\n",
      "Iteration: 270, loss 0.47549383401088774\n",
      "Iteration: 271, loss 0.45825062095968083\n",
      "Iteration: 272, loss 0.44164475080916704\n",
      "Iteration: 273, loss 0.4256523397569081\n",
      "Iteration: 274, loss 0.4102517509370216\n",
      "Iteration: 275, loss 0.3954192313823004\n",
      "Iteration: 276, loss 0.38113557186389724\n",
      "Iteration: 277, loss 0.36737813996306523\n",
      "Iteration: 278, loss 0.35412882591317\n",
      "Iteration: 279, loss 0.3413646353060017\n",
      "Iteration: 280, loss 0.32907099659564903\n",
      "Iteration: 281, loss 0.31722795652429936\n",
      "Iteration: 282, loss 0.30581997759175955\n",
      "Iteration: 283, loss 0.29482955351542145\n",
      "Iteration: 284, loss 0.28424220708663467\n",
      "Iteration: 285, loss 0.27404184037554213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 286, loss 0.2642157367308056\n",
      "Iteration: 287, loss 0.2547469291341164\n",
      "Iteration: 288, loss 0.24562419195857496\n",
      "Iteration: 289, loss 0.236834198269199\n",
      "Iteration: 290, loss 0.22836383556076675\n",
      "Iteration: 291, loss 0.2202017671259056\n",
      "Iteration: 292, loss 0.21233746097186568\n",
      "Iteration: 293, loss 0.2047591494424767\n",
      "Iteration: 294, loss 0.19745597078186777\n",
      "Iteration: 295, loss 0.19041693787950487\n",
      "Iteration: 296, loss 0.1836334098742534\n",
      "Iteration: 297, loss 0.17709551072258806\n",
      "Iteration: 298, loss 0.17079405886365465\n",
      "Iteration: 299, loss 0.16472030013182784\n",
      "Iteration: 300, loss 0.15886665654571963\n",
      "Iteration: 301, loss 0.15322402310696437\n",
      "Iteration: 302, loss 0.14778550908693677\n",
      "Iteration: 303, loss 0.14254268046375915\n",
      "Iteration: 304, loss 0.13748928053649423\n",
      "Iteration: 305, loss 0.1326170693647056\n",
      "Iteration: 306, loss 0.12792018272854697\n",
      "Iteration: 307, loss 0.12339220414535056\n",
      "Iteration: 308, loss 0.11902727772244849\n",
      "Iteration: 309, loss 0.11481871739108632\n",
      "Iteration: 310, loss 0.11076176798903131\n",
      "Iteration: 311, loss 0.10684973895049144\n",
      "Iteration: 312, loss 0.1030778370079505\n",
      "Iteration: 313, loss 0.0994409134021415\n",
      "Iteration: 314, loss 0.0959341739342443\n",
      "Iteration: 315, loss 0.09255300502949602\n",
      "Iteration: 316, loss 0.08929250978081577\n",
      "Iteration: 317, loss 0.08614822747424762\n",
      "Iteration: 318, loss 0.08311665273774264\n",
      "Iteration: 319, loss 0.0801930439066929\n",
      "Iteration: 320, loss 0.07737344038531721\n",
      "Iteration: 321, loss 0.07465423464123543\n",
      "Iteration: 322, loss 0.0720317978740174\n",
      "Iteration: 323, loss 0.06950285485742605\n",
      "Iteration: 324, loss 0.06706359765091294\n",
      "Iteration: 325, loss 0.06471118592494227\n",
      "Iteration: 326, loss 0.062442678339027904\n",
      "Iteration: 327, loss 0.060254297282964345\n",
      "Iteration: 328, loss 0.05814354710793186\n",
      "Iteration: 329, loss 0.05610749593422154\n",
      "Iteration: 330, loss 0.054143810935809625\n",
      "Iteration: 331, loss 0.0522494837347071\n",
      "Iteration: 332, loss 0.05042218185132995\n",
      "Iteration: 333, loss 0.048659595805497204\n",
      "Iteration: 334, loss 0.04695959682408367\n",
      "Iteration: 335, loss 0.04531939533334635\n",
      "Iteration: 336, loss 0.04373712925300459\n",
      "Iteration: 337, loss 0.04221079202581601\n",
      "Iteration: 338, loss 0.04073818825301062\n",
      "Iteration: 339, loss 0.0393174989599135\n",
      "Iteration: 340, loss 0.037946921540174164\n",
      "Iteration: 341, loss 0.036624698511717374\n",
      "Iteration: 342, loss 0.035349086211816365\n",
      "Iteration: 343, loss 0.03411822122445357\n",
      "Iteration: 344, loss 0.03293069049498376\n",
      "Iteration: 345, loss 0.03178491989083901\n",
      "Iteration: 346, loss 0.030679376079878452\n",
      "Iteration: 347, loss 0.029612714821961005\n",
      "Iteration: 348, loss 0.028583545703161427\n",
      "Iteration: 349, loss 0.027590374779289793\n",
      "Iteration: 350, loss 0.026632196431523775\n",
      "Iteration: 351, loss 0.025707523255659243\n",
      "Iteration: 352, loss 0.024815253900174042\n",
      "Iteration: 353, loss 0.0239541888827534\n",
      "Iteration: 354, loss 0.023123289588861083\n",
      "Iteration: 355, loss 0.02232155055368258\n",
      "Iteration: 356, loss 0.02154778599962802\n",
      "Iteration: 357, loss 0.020801083732323783\n",
      "Iteration: 358, loss 0.020080630421407716\n",
      "Iteration: 359, loss 0.019385253233504228\n",
      "Iteration: 360, loss 0.018714117547059206\n",
      "Iteration: 361, loss 0.01806640602344699\n",
      "Iteration: 362, loss 0.017441364959244393\n",
      "Iteration: 363, loss 0.016838070115227907\n",
      "Iteration: 364, loss 0.016255834511147288\n",
      "Iteration: 365, loss 0.015693909547668982\n",
      "Iteration: 366, loss 0.015151639069355584\n",
      "Iteration: 367, loss 0.014628167546453755\n",
      "Iteration: 368, loss 0.01412292281027121\n",
      "Iteration: 369, loss 0.013635324135791514\n",
      "Iteration: 370, loss 0.01316462672846382\n",
      "Iteration: 371, loss 0.012710300529648569\n",
      "Iteration: 372, loss 0.01227180295746594\n",
      "Iteration: 373, loss 0.011848528886448804\n",
      "Iteration: 374, loss 0.011439992545541268\n",
      "Iteration: 375, loss 0.011045626016493268\n",
      "Iteration: 376, loss 0.010664956817763012\n",
      "Iteration: 377, loss 0.010297478316973398\n",
      "Iteration: 378, loss 0.009942754476119791\n",
      "Iteration: 379, loss 0.009600364314180715\n",
      "Iteration: 380, loss 0.00926983206740314\n",
      "Iteration: 381, loss 0.008950743861731291\n",
      "Iteration: 382, loss 0.008642752538436242\n",
      "Iteration: 383, loss 0.008345420501565533\n",
      "Iteration: 384, loss 0.00805835727359203\n",
      "Iteration: 385, loss 0.007781232942790411\n",
      "Iteration: 386, loss 0.007513727574350342\n",
      "Iteration: 387, loss 0.0072554561360209\n",
      "Iteration: 388, loss 0.007006114461055911\n",
      "Iteration: 389, loss 0.006765408316122195\n",
      "Iteration: 390, loss 0.006533045289257899\n",
      "Iteration: 391, loss 0.0063086993482368535\n",
      "Iteration: 392, loss 0.006092092025543821\n",
      "Iteration: 393, loss 0.005882974717959441\n",
      "Iteration: 394, loss 0.005681061939964348\n",
      "Iteration: 395, loss 0.005486121326582002\n",
      "Iteration: 396, loss 0.005297927379707307\n",
      "Iteration: 397, loss 0.00511621032304834\n",
      "Iteration: 398, loss 0.0049407805570097964\n",
      "Iteration: 399, loss 0.004771392606437243\n",
      "Iteration: 400, loss 0.00460784043397955\n",
      "Iteration: 401, loss 0.004449923302403635\n",
      "Iteration: 402, loss 0.004297448945328223\n",
      "Iteration: 403, loss 0.004150234139064364\n",
      "Iteration: 404, loss 0.0040080781980314836\n",
      "Iteration: 405, loss 0.003870816930389105\n",
      "Iteration: 406, loss 0.0037383062690072746\n",
      "Iteration: 407, loss 0.0036103325510547088\n",
      "Iteration: 408, loss 0.0034867603386757404\n",
      "Iteration: 409, loss 0.0033674481067625488\n",
      "Iteration: 410, loss 0.003252234991329626\n",
      "Iteration: 411, loss 0.0031409790242905066\n",
      "Iteration: 412, loss 0.0030335525245404615\n",
      "Iteration: 413, loss 0.00292982502989787\n",
      "Iteration: 414, loss 0.0028296645433111096\n",
      "Iteration: 415, loss 0.0027329295344994684\n",
      "Iteration: 416, loss 0.0026395252592644114\n",
      "Iteration: 417, loss 0.0025493239715990053\n",
      "Iteration: 418, loss 0.0024622161957911313\n",
      "Iteration: 419, loss 0.002378102494828021\n",
      "Iteration: 420, loss 0.002296874781036765\n",
      "Iteration: 421, loss 0.0022184326126161295\n",
      "Iteration: 422, loss 0.0021426886175765383\n",
      "Iteration: 423, loss 0.002069541233938205\n",
      "Iteration: 424, loss 0.0019988954721061993\n",
      "Iteration: 425, loss 0.0019306701159185495\n",
      "Iteration: 426, loss 0.001864788994436994\n",
      "Iteration: 427, loss 0.0018011598975665477\n",
      "Iteration: 428, loss 0.0017397102962387859\n",
      "Iteration: 429, loss 0.0016803725019862224\n",
      "Iteration: 430, loss 0.0016230681747145827\n",
      "Iteration: 431, loss 0.0015677185465236185\n",
      "Iteration: 432, loss 0.001514266049297863\n",
      "Iteration: 433, loss 0.0014626450443086581\n",
      "Iteration: 434, loss 0.001412789188409403\n",
      "Iteration: 435, loss 0.0013646374199445545\n",
      "Iteration: 436, loss 0.0013181374515862088\n",
      "Iteration: 437, loss 0.001273226543678778\n",
      "Iteration: 438, loss 0.0012298505618467909\n",
      "Iteration: 439, loss 0.0011879581326026548\n",
      "Iteration: 440, loss 0.0011474966849138422\n",
      "Iteration: 441, loss 0.0011084172641989353\n",
      "Iteration: 442, loss 0.0010706751325551024\n",
      "Iteration: 443, loss 0.001034222864891815\n",
      "Iteration: 444, loss 0.000999014063096327\n",
      "Iteration: 445, loss 0.0009650141219963793\n",
      "Iteration: 446, loss 0.0009321726089889636\n",
      "Iteration: 447, loss 0.0009004493944100536\n",
      "Iteration: 448, loss 0.0008698097181583711\n",
      "Iteration: 449, loss 0.0008402183349777213\n",
      "Iteration: 450, loss 0.0008116351640367296\n",
      "Iteration: 451, loss 0.0007840267932358174\n",
      "Iteration: 452, loss 0.0007573629581230003\n",
      "Iteration: 453, loss 0.0007316100621768594\n",
      "Iteration: 454, loss 0.000706732459154053\n",
      "Iteration: 455, loss 0.0006827037913522212\n",
      "Iteration: 456, loss 0.0006594975420439945\n",
      "Iteration: 457, loss 0.0006370796825728921\n",
      "Iteration: 458, loss 0.0006154255445761658\n",
      "Iteration: 459, loss 0.0005945115250019722\n",
      "Iteration: 460, loss 0.0005743106931040189\n",
      "Iteration: 461, loss 0.0005547969041145554\n",
      "Iteration: 462, loss 0.0005359484956031933\n",
      "Iteration: 463, loss 0.0005177417375253081\n",
      "Iteration: 464, loss 0.0005001548578861107\n",
      "Iteration: 465, loss 0.0004831674989077421\n",
      "Iteration: 466, loss 0.00046675930930169664\n",
      "Iteration: 467, loss 0.0004509098663181216\n",
      "Iteration: 468, loss 0.0004356006077048817\n",
      "Iteration: 469, loss 0.0004208121857132539\n",
      "Iteration: 470, loss 0.0004065259149913146\n",
      "Iteration: 471, loss 0.0003927260569504014\n",
      "Iteration: 472, loss 0.0003793969309934165\n",
      "Iteration: 473, loss 0.0003665204848931229\n",
      "Iteration: 474, loss 0.0003540819028579003\n",
      "Iteration: 475, loss 0.0003420682226545639\n",
      "Iteration: 476, loss 0.00033046241654590455\n",
      "Iteration: 477, loss 0.00031925065233329587\n",
      "Iteration: 478, loss 0.00030842091894026113\n",
      "Iteration: 479, loss 0.0002979594295228137\n",
      "Iteration: 480, loss 0.00028785283735238156\n",
      "Iteration: 481, loss 0.00027808988815631756\n",
      "Iteration: 482, loss 0.00026866021672484683\n",
      "Iteration: 483, loss 0.0002595505646231071\n",
      "Iteration: 484, loss 0.00025074970547568474\n",
      "Iteration: 485, loss 0.00024224847950316832\n",
      "Iteration: 486, loss 0.00023403589574935826\n",
      "Iteration: 487, loss 0.0002261020350288756\n",
      "Iteration: 488, loss 0.0002184380891525888\n",
      "Iteration: 489, loss 0.00021103498332487804\n",
      "Iteration: 490, loss 0.00020388336085122558\n",
      "Iteration: 491, loss 0.0001969738522447024\n",
      "Iteration: 492, loss 0.00019029950809129376\n",
      "Iteration: 493, loss 0.00018385134358758808\n",
      "Iteration: 494, loss 0.00017762218575112314\n",
      "Iteration: 495, loss 0.00017160487831887377\n",
      "Iteration: 496, loss 0.00016579147130763402\n",
      "Iteration: 497, loss 0.00016017576350577022\n",
      "Iteration: 498, loss 0.0001547505954042701\n",
      "Iteration: 499, loss 0.00014950942817648955\n"
     ]
    }
   ],
   "source": [
    "# backpropagation algorithm\n",
    "for i in range(500):\n",
    "    # forward pass\n",
    "    Z = X.dot(W1)\n",
    "    phi = np.maximum(0, Z)\n",
    "    pred = phi.dot(W2)\n",
    "    # compute/print loss\n",
    "    loss = np.sum((Y - pred) ** 2)\n",
    "    print('Iteration: {}, loss {}'.format(i, loss))\n",
    "    # backwards pass\n",
    "    delta_l2 = 2 * (pred - Y)\n",
    "    grad_l2 = phi.T.dot(delta_l2)\n",
    "    delta_l1 = delta_l2.dot(W2.T)\n",
    "    delta_l1[phi == 0] = 0\n",
    "    grad_l1 = X.T.dot(delta_l1)\n",
    "    # updates\n",
    "    W1 -= gamma * grad_l1\n",
    "    W2 -= gamma * grad_l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using PyTorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(N, D1)\n",
    "Y = torch.randn(N, D2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.randn(D1, H, device=device, dtype=dtype)\n",
    "W2 = torch.randn(H, D2, device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, loss 30572468.0\n",
      "Iteration: 1, loss 26421654.0\n",
      "Iteration: 2, loss 24728238.0\n",
      "Iteration: 3, loss 22120718.0\n",
      "Iteration: 4, loss 17691872.0\n",
      "Iteration: 5, loss 12465142.0\n",
      "Iteration: 6, loss 7932702.5\n",
      "Iteration: 7, loss 4810428.5\n",
      "Iteration: 8, loss 2937997.0\n",
      "Iteration: 9, loss 1884814.25\n",
      "Iteration: 10, loss 1295121.25\n",
      "Iteration: 11, loss 953224.8125\n",
      "Iteration: 12, loss 741899.5625\n",
      "Iteration: 13, loss 601415.75\n",
      "Iteration: 14, loss 501177.84375\n",
      "Iteration: 15, loss 425482.21875\n",
      "Iteration: 16, loss 365699.25\n",
      "Iteration: 17, loss 317017.84375\n",
      "Iteration: 18, loss 276613.40625\n",
      "Iteration: 19, loss 242635.203125\n",
      "Iteration: 20, loss 213730.578125\n",
      "Iteration: 21, loss 188977.75\n",
      "Iteration: 22, loss 167647.765625\n",
      "Iteration: 23, loss 149178.546875\n",
      "Iteration: 24, loss 133107.0\n",
      "Iteration: 25, loss 119055.578125\n",
      "Iteration: 26, loss 106724.609375\n",
      "Iteration: 27, loss 95873.2109375\n",
      "Iteration: 28, loss 86295.5546875\n",
      "Iteration: 29, loss 77812.5234375\n",
      "Iteration: 30, loss 70290.9765625\n",
      "Iteration: 31, loss 63606.953125\n",
      "Iteration: 32, loss 57639.2890625\n",
      "Iteration: 33, loss 52308.07421875\n",
      "Iteration: 34, loss 47534.48046875\n",
      "Iteration: 35, loss 43252.484375\n",
      "Iteration: 36, loss 39406.51171875\n",
      "Iteration: 37, loss 35944.21484375\n",
      "Iteration: 38, loss 32823.5859375\n",
      "Iteration: 39, loss 30007.029296875\n",
      "Iteration: 40, loss 27464.369140625\n",
      "Iteration: 41, loss 25164.0078125\n",
      "Iteration: 42, loss 23078.634765625\n",
      "Iteration: 43, loss 21186.533203125\n",
      "Iteration: 44, loss 19466.60546875\n",
      "Iteration: 45, loss 17901.0390625\n",
      "Iteration: 46, loss 16475.201171875\n",
      "Iteration: 47, loss 15176.029296875\n",
      "Iteration: 48, loss 13989.9208984375\n",
      "Iteration: 49, loss 12906.28515625\n",
      "Iteration: 50, loss 11916.076171875\n",
      "Iteration: 51, loss 11009.7158203125\n",
      "Iteration: 52, loss 10179.02734375\n",
      "Iteration: 53, loss 9417.1689453125\n",
      "Iteration: 54, loss 8717.67578125\n",
      "Iteration: 55, loss 8075.1728515625\n",
      "Iteration: 56, loss 7484.9716796875\n",
      "Iteration: 57, loss 6942.3232421875\n",
      "Iteration: 58, loss 6442.70947265625\n",
      "Iteration: 59, loss 5982.17919921875\n",
      "Iteration: 60, loss 5557.48095703125\n",
      "Iteration: 61, loss 5165.50634765625\n",
      "Iteration: 62, loss 4803.4111328125\n",
      "Iteration: 63, loss 4468.83203125\n",
      "Iteration: 64, loss 4159.38134765625\n",
      "Iteration: 65, loss 3873.220703125\n",
      "Iteration: 66, loss 3608.294677734375\n",
      "Iteration: 67, loss 3363.658203125\n",
      "Iteration: 68, loss 3136.9091796875\n",
      "Iteration: 69, loss 2926.579833984375\n",
      "Iteration: 70, loss 2731.416015625\n",
      "Iteration: 71, loss 2550.181640625\n",
      "Iteration: 72, loss 2381.85009765625\n",
      "Iteration: 73, loss 2225.371337890625\n",
      "Iteration: 74, loss 2079.9111328125\n",
      "Iteration: 75, loss 1944.568115234375\n",
      "Iteration: 76, loss 1818.644287109375\n",
      "Iteration: 77, loss 1701.3743896484375\n",
      "Iteration: 78, loss 1592.1676025390625\n",
      "Iteration: 79, loss 1490.378173828125\n",
      "Iteration: 80, loss 1395.5205078125\n",
      "Iteration: 81, loss 1307.0794677734375\n",
      "Iteration: 82, loss 1224.549072265625\n",
      "Iteration: 83, loss 1147.541748046875\n",
      "Iteration: 84, loss 1075.638916015625\n",
      "Iteration: 85, loss 1008.4888305664062\n",
      "Iteration: 86, loss 945.7628173828125\n",
      "Iteration: 87, loss 887.167236328125\n",
      "Iteration: 88, loss 832.3878173828125\n",
      "Iteration: 89, loss 781.15625\n",
      "Iteration: 90, loss 733.2393188476562\n",
      "Iteration: 91, loss 688.4134521484375\n",
      "Iteration: 92, loss 646.4498901367188\n",
      "Iteration: 93, loss 607.1836547851562\n",
      "Iteration: 94, loss 570.3998413085938\n",
      "Iteration: 95, loss 535.9647216796875\n",
      "Iteration: 96, loss 503.69207763671875\n",
      "Iteration: 97, loss 473.45709228515625\n",
      "Iteration: 98, loss 445.1213073730469\n",
      "Iteration: 99, loss 418.5546875\n",
      "Iteration: 100, loss 393.6637878417969\n",
      "Iteration: 101, loss 370.3074645996094\n",
      "Iteration: 102, loss 348.4112548828125\n",
      "Iteration: 103, loss 327.8821105957031\n",
      "Iteration: 104, loss 308.60687255859375\n",
      "Iteration: 105, loss 290.514892578125\n",
      "Iteration: 106, loss 273.53460693359375\n",
      "Iteration: 107, loss 257.5763244628906\n",
      "Iteration: 108, loss 242.587890625\n",
      "Iteration: 109, loss 228.50103759765625\n",
      "Iteration: 110, loss 215.26065063476562\n",
      "Iteration: 111, loss 202.81857299804688\n",
      "Iteration: 112, loss 191.11996459960938\n",
      "Iteration: 113, loss 180.1241455078125\n",
      "Iteration: 114, loss 169.80487060546875\n",
      "Iteration: 115, loss 160.093505859375\n",
      "Iteration: 116, loss 150.95726013183594\n",
      "Iteration: 117, loss 142.35894775390625\n",
      "Iteration: 118, loss 134.26573181152344\n",
      "Iteration: 119, loss 126.64913940429688\n",
      "Iteration: 120, loss 119.4760513305664\n",
      "Iteration: 121, loss 112.72074890136719\n",
      "Iteration: 122, loss 106.35985565185547\n",
      "Iteration: 123, loss 100.36737823486328\n",
      "Iteration: 124, loss 94.7238998413086\n",
      "Iteration: 125, loss 89.4073486328125\n",
      "Iteration: 126, loss 84.39421081542969\n",
      "Iteration: 127, loss 79.67378234863281\n",
      "Iteration: 128, loss 75.22095489501953\n",
      "Iteration: 129, loss 71.02357482910156\n",
      "Iteration: 130, loss 67.06890869140625\n",
      "Iteration: 131, loss 63.33710479736328\n",
      "Iteration: 132, loss 59.82006072998047\n",
      "Iteration: 133, loss 56.502418518066406\n",
      "Iteration: 134, loss 53.37335205078125\n",
      "Iteration: 135, loss 50.42342758178711\n",
      "Iteration: 136, loss 47.63922882080078\n",
      "Iteration: 137, loss 45.01250076293945\n",
      "Iteration: 138, loss 42.53461456298828\n",
      "Iteration: 139, loss 40.19517517089844\n",
      "Iteration: 140, loss 37.98701095581055\n",
      "Iteration: 141, loss 35.90375900268555\n",
      "Iteration: 142, loss 33.93682098388672\n",
      "Iteration: 143, loss 32.08040237426758\n",
      "Iteration: 144, loss 30.327301025390625\n",
      "Iteration: 145, loss 28.671348571777344\n",
      "Iteration: 146, loss 27.10883331298828\n",
      "Iteration: 147, loss 25.63214683532715\n",
      "Iteration: 148, loss 24.238264083862305\n",
      "Iteration: 149, loss 22.92141342163086\n",
      "Iteration: 150, loss 21.677703857421875\n",
      "Iteration: 151, loss 20.502368927001953\n",
      "Iteration: 152, loss 19.392406463623047\n",
      "Iteration: 153, loss 18.343870162963867\n",
      "Iteration: 154, loss 17.352535247802734\n",
      "Iteration: 155, loss 16.416196823120117\n",
      "Iteration: 156, loss 15.530816078186035\n",
      "Iteration: 157, loss 14.694910049438477\n",
      "Iteration: 158, loss 13.903861999511719\n",
      "Iteration: 159, loss 13.156293869018555\n",
      "Iteration: 160, loss 12.44933795928955\n",
      "Iteration: 161, loss 11.781743049621582\n",
      "Iteration: 162, loss 11.150054931640625\n",
      "Iteration: 163, loss 10.553024291992188\n",
      "Iteration: 164, loss 9.988680839538574\n",
      "Iteration: 165, loss 9.455001831054688\n",
      "Iteration: 166, loss 8.950187683105469\n",
      "Iteration: 167, loss 8.472688674926758\n",
      "Iteration: 168, loss 8.020759582519531\n",
      "Iteration: 169, loss 7.593523979187012\n",
      "Iteration: 170, loss 7.189135551452637\n",
      "Iteration: 171, loss 6.807217597961426\n",
      "Iteration: 172, loss 6.44522762298584\n",
      "Iteration: 173, loss 6.103274822235107\n",
      "Iteration: 174, loss 5.779426574707031\n",
      "Iteration: 175, loss 5.473442077636719\n",
      "Iteration: 176, loss 5.183234691619873\n",
      "Iteration: 177, loss 4.909045219421387\n",
      "Iteration: 178, loss 4.649505138397217\n",
      "Iteration: 179, loss 4.403712749481201\n",
      "Iteration: 180, loss 4.171454429626465\n",
      "Iteration: 181, loss 3.9512579441070557\n",
      "Iteration: 182, loss 3.742976188659668\n",
      "Iteration: 183, loss 3.546093463897705\n",
      "Iteration: 184, loss 3.359147548675537\n",
      "Iteration: 185, loss 3.182589530944824\n",
      "Iteration: 186, loss 3.0152316093444824\n",
      "Iteration: 187, loss 2.8568806648254395\n",
      "Iteration: 188, loss 2.706835985183716\n",
      "Iteration: 189, loss 2.564831495285034\n",
      "Iteration: 190, loss 2.4304447174072266\n",
      "Iteration: 191, loss 2.30312180519104\n",
      "Iteration: 192, loss 2.1825575828552246\n",
      "Iteration: 193, loss 2.068380355834961\n",
      "Iteration: 194, loss 1.9601750373840332\n",
      "Iteration: 195, loss 1.8577461242675781\n",
      "Iteration: 196, loss 1.7607814073562622\n",
      "Iteration: 197, loss 1.669034481048584\n",
      "Iteration: 198, loss 1.581875205039978\n",
      "Iteration: 199, loss 1.4992544651031494\n",
      "Iteration: 200, loss 1.421236515045166\n",
      "Iteration: 201, loss 1.3475279808044434\n",
      "Iteration: 202, loss 1.277219533920288\n",
      "Iteration: 203, loss 1.2108651399612427\n",
      "Iteration: 204, loss 1.147838830947876\n",
      "Iteration: 205, loss 1.0881918668746948\n",
      "Iteration: 206, loss 1.0317065715789795\n",
      "Iteration: 207, loss 0.9782655835151672\n",
      "Iteration: 208, loss 0.927584171295166\n",
      "Iteration: 209, loss 0.8794419765472412\n",
      "Iteration: 210, loss 0.8339372873306274\n",
      "Iteration: 211, loss 0.7906856536865234\n",
      "Iteration: 212, loss 0.7498378157615662\n",
      "Iteration: 213, loss 0.7110770344734192\n",
      "Iteration: 214, loss 0.674316942691803\n",
      "Iteration: 215, loss 0.6394830942153931\n",
      "Iteration: 216, loss 0.6064084768295288\n",
      "Iteration: 217, loss 0.5751500129699707\n",
      "Iteration: 218, loss 0.5454814434051514\n",
      "Iteration: 219, loss 0.5173354744911194\n",
      "Iteration: 220, loss 0.49072134494781494\n",
      "Iteration: 221, loss 0.4654028117656708\n",
      "Iteration: 222, loss 0.441429078578949\n",
      "Iteration: 223, loss 0.41871723532676697\n",
      "Iteration: 224, loss 0.3971860408782959\n",
      "Iteration: 225, loss 0.37679970264434814\n",
      "Iteration: 226, loss 0.3574199378490448\n",
      "Iteration: 227, loss 0.33899983763694763\n",
      "Iteration: 228, loss 0.3215940594673157\n",
      "Iteration: 229, loss 0.3050554692745209\n",
      "Iteration: 230, loss 0.28944215178489685\n",
      "Iteration: 231, loss 0.2746428847312927\n",
      "Iteration: 232, loss 0.2605307996273041\n",
      "Iteration: 233, loss 0.2472018599510193\n",
      "Iteration: 234, loss 0.23453691601753235\n",
      "Iteration: 235, loss 0.22255146503448486\n",
      "Iteration: 236, loss 0.21109408140182495\n",
      "Iteration: 237, loss 0.20031219720840454\n",
      "Iteration: 238, loss 0.19011585414409637\n",
      "Iteration: 239, loss 0.18037152290344238\n",
      "Iteration: 240, loss 0.17116132378578186\n",
      "Iteration: 241, loss 0.16244816780090332\n",
      "Iteration: 242, loss 0.15415018796920776\n",
      "Iteration: 243, loss 0.14627298712730408\n",
      "Iteration: 244, loss 0.13886307179927826\n",
      "Iteration: 245, loss 0.13175372779369354\n",
      "Iteration: 246, loss 0.12500758469104767\n",
      "Iteration: 247, loss 0.11864978075027466\n",
      "Iteration: 248, loss 0.11260082572698593\n",
      "Iteration: 249, loss 0.1068962961435318\n",
      "Iteration: 250, loss 0.10146136581897736\n",
      "Iteration: 251, loss 0.09628032147884369\n",
      "Iteration: 252, loss 0.09138961136341095\n",
      "Iteration: 253, loss 0.08675611019134521\n",
      "Iteration: 254, loss 0.08232392370700836\n",
      "Iteration: 255, loss 0.0781615749001503\n",
      "Iteration: 256, loss 0.07419244199991226\n",
      "Iteration: 257, loss 0.07041817903518677\n",
      "Iteration: 258, loss 0.06682875752449036\n",
      "Iteration: 259, loss 0.06343509256839752\n",
      "Iteration: 260, loss 0.06023972108960152\n",
      "Iteration: 261, loss 0.05721278116106987\n",
      "Iteration: 262, loss 0.054323188960552216\n",
      "Iteration: 263, loss 0.05156702548265457\n",
      "Iteration: 264, loss 0.048952873796224594\n",
      "Iteration: 265, loss 0.0464722216129303\n",
      "Iteration: 266, loss 0.04413273185491562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 267, loss 0.04189303144812584\n",
      "Iteration: 268, loss 0.03978254646062851\n",
      "Iteration: 269, loss 0.03777296096086502\n",
      "Iteration: 270, loss 0.03588613495230675\n",
      "Iteration: 271, loss 0.034075140953063965\n",
      "Iteration: 272, loss 0.0323699526488781\n",
      "Iteration: 273, loss 0.030730124562978745\n",
      "Iteration: 274, loss 0.029187865555286407\n",
      "Iteration: 275, loss 0.027723176404833794\n",
      "Iteration: 276, loss 0.026325102895498276\n",
      "Iteration: 277, loss 0.02500830590724945\n",
      "Iteration: 278, loss 0.02375432848930359\n",
      "Iteration: 279, loss 0.02256469987332821\n",
      "Iteration: 280, loss 0.021428614854812622\n",
      "Iteration: 281, loss 0.020362328737974167\n",
      "Iteration: 282, loss 0.01932922936975956\n",
      "Iteration: 283, loss 0.018361927941441536\n",
      "Iteration: 284, loss 0.01745038665831089\n",
      "Iteration: 285, loss 0.016581248492002487\n",
      "Iteration: 286, loss 0.015749681740999222\n",
      "Iteration: 287, loss 0.014980156905949116\n",
      "Iteration: 288, loss 0.014232998713850975\n",
      "Iteration: 289, loss 0.013532746583223343\n",
      "Iteration: 290, loss 0.012848522514104843\n",
      "Iteration: 291, loss 0.012209848500788212\n",
      "Iteration: 292, loss 0.011613785289227962\n",
      "Iteration: 293, loss 0.01103891246020794\n",
      "Iteration: 294, loss 0.010497468523681164\n",
      "Iteration: 295, loss 0.00998886488378048\n",
      "Iteration: 296, loss 0.009496361017227173\n",
      "Iteration: 297, loss 0.009029250591993332\n",
      "Iteration: 298, loss 0.008590098470449448\n",
      "Iteration: 299, loss 0.008171314373612404\n",
      "Iteration: 300, loss 0.007771896198391914\n",
      "Iteration: 301, loss 0.007398365065455437\n",
      "Iteration: 302, loss 0.007039973046630621\n",
      "Iteration: 303, loss 0.0066976528614759445\n",
      "Iteration: 304, loss 0.006371180061250925\n",
      "Iteration: 305, loss 0.006069290451705456\n",
      "Iteration: 306, loss 0.005773938726633787\n",
      "Iteration: 307, loss 0.0055010090582072735\n",
      "Iteration: 308, loss 0.00524003803730011\n",
      "Iteration: 309, loss 0.004989710170775652\n",
      "Iteration: 310, loss 0.0047564757987856865\n",
      "Iteration: 311, loss 0.0045335302129387856\n",
      "Iteration: 312, loss 0.004326249938458204\n",
      "Iteration: 313, loss 0.004119476769119501\n",
      "Iteration: 314, loss 0.003926227334886789\n",
      "Iteration: 315, loss 0.003740706481039524\n",
      "Iteration: 316, loss 0.0035719177685678005\n",
      "Iteration: 317, loss 0.0034062485210597515\n",
      "Iteration: 318, loss 0.003253060858696699\n",
      "Iteration: 319, loss 0.003101796843111515\n",
      "Iteration: 320, loss 0.002963785082101822\n",
      "Iteration: 321, loss 0.0028319277334958315\n",
      "Iteration: 322, loss 0.0027042957954108715\n",
      "Iteration: 323, loss 0.0025792433880269527\n",
      "Iteration: 324, loss 0.002468166872859001\n",
      "Iteration: 325, loss 0.0023560835979878902\n",
      "Iteration: 326, loss 0.0022555235773324966\n",
      "Iteration: 327, loss 0.00215346273034811\n",
      "Iteration: 328, loss 0.002059642458334565\n",
      "Iteration: 329, loss 0.001972390338778496\n",
      "Iteration: 330, loss 0.0018867264734581113\n",
      "Iteration: 331, loss 0.0018058873247355223\n",
      "Iteration: 332, loss 0.001731236930936575\n",
      "Iteration: 333, loss 0.0016573745524510741\n",
      "Iteration: 334, loss 0.001587777165696025\n",
      "Iteration: 335, loss 0.0015208894619718194\n",
      "Iteration: 336, loss 0.0014598045963793993\n",
      "Iteration: 337, loss 0.0014002276584506035\n",
      "Iteration: 338, loss 0.0013408031081780791\n",
      "Iteration: 339, loss 0.001288769068196416\n",
      "Iteration: 340, loss 0.0012328876182436943\n",
      "Iteration: 341, loss 0.0011857862118631601\n",
      "Iteration: 342, loss 0.0011396619956940413\n",
      "Iteration: 343, loss 0.0010947637492790818\n",
      "Iteration: 344, loss 0.00105453806463629\n",
      "Iteration: 345, loss 0.0010109072318300605\n",
      "Iteration: 346, loss 0.0009730657329782844\n",
      "Iteration: 347, loss 0.0009369291947223246\n",
      "Iteration: 348, loss 0.0009020467987284064\n",
      "Iteration: 349, loss 0.0008679539896547794\n",
      "Iteration: 350, loss 0.000837477040477097\n",
      "Iteration: 351, loss 0.0008067351882345974\n",
      "Iteration: 352, loss 0.0007766574854031205\n",
      "Iteration: 353, loss 0.0007485401001758873\n",
      "Iteration: 354, loss 0.0007226077141240239\n",
      "Iteration: 355, loss 0.0006966419750824571\n",
      "Iteration: 356, loss 0.000671034213155508\n",
      "Iteration: 357, loss 0.0006497126305475831\n",
      "Iteration: 358, loss 0.0006268936558626592\n",
      "Iteration: 359, loss 0.0006050982046872377\n",
      "Iteration: 360, loss 0.0005823872634209692\n",
      "Iteration: 361, loss 0.000563842011615634\n",
      "Iteration: 362, loss 0.00054624502081424\n",
      "Iteration: 363, loss 0.0005273276474326849\n",
      "Iteration: 364, loss 0.0005095972446724772\n",
      "Iteration: 365, loss 0.0004928188864141703\n",
      "Iteration: 366, loss 0.0004764799086842686\n",
      "Iteration: 367, loss 0.00046154894516803324\n",
      "Iteration: 368, loss 0.00044681172585114837\n",
      "Iteration: 369, loss 0.0004332686075940728\n",
      "Iteration: 370, loss 0.0004201914998702705\n",
      "Iteration: 371, loss 0.0004074097378179431\n",
      "Iteration: 372, loss 0.0003951186372432858\n",
      "Iteration: 373, loss 0.0003828440676443279\n",
      "Iteration: 374, loss 0.0003711848403327167\n",
      "Iteration: 375, loss 0.0003594325971789658\n",
      "Iteration: 376, loss 0.0003489961673039943\n",
      "Iteration: 377, loss 0.00033949571661651134\n",
      "Iteration: 378, loss 0.00032947424915619195\n",
      "Iteration: 379, loss 0.0003199430357199162\n",
      "Iteration: 380, loss 0.0003102465416304767\n",
      "Iteration: 381, loss 0.00030175017309375107\n",
      "Iteration: 382, loss 0.0002930619812104851\n",
      "Iteration: 383, loss 0.00028510234551504254\n",
      "Iteration: 384, loss 0.00027701834915205836\n",
      "Iteration: 385, loss 0.00026913228794001043\n",
      "Iteration: 386, loss 0.00026094954228028655\n",
      "Iteration: 387, loss 0.00025464981445111334\n",
      "Iteration: 388, loss 0.0002482726995367557\n",
      "Iteration: 389, loss 0.00024091379600577056\n",
      "Iteration: 390, loss 0.00023450463777408004\n",
      "Iteration: 391, loss 0.00022949700360186398\n",
      "Iteration: 392, loss 0.00022344951867125928\n",
      "Iteration: 393, loss 0.00021767104044556618\n",
      "Iteration: 394, loss 0.0002119721903000027\n",
      "Iteration: 395, loss 0.00020683987531811\n",
      "Iteration: 396, loss 0.00020176067482680082\n",
      "Iteration: 397, loss 0.00019623612752184272\n",
      "Iteration: 398, loss 0.00019172272004652768\n",
      "Iteration: 399, loss 0.00018662326328922063\n",
      "Iteration: 400, loss 0.00018216563330497593\n",
      "Iteration: 401, loss 0.00017789563571568578\n",
      "Iteration: 402, loss 0.00017337326426059008\n",
      "Iteration: 403, loss 0.00016952038276940584\n",
      "Iteration: 404, loss 0.0001654092047829181\n",
      "Iteration: 405, loss 0.00016158408834598958\n",
      "Iteration: 406, loss 0.00015804760914761573\n",
      "Iteration: 407, loss 0.00015412658103741705\n",
      "Iteration: 408, loss 0.00015007818001322448\n",
      "Iteration: 409, loss 0.00014717834710609168\n",
      "Iteration: 410, loss 0.00014352932339534163\n",
      "Iteration: 411, loss 0.0001404747599735856\n",
      "Iteration: 412, loss 0.0001375539432046935\n",
      "Iteration: 413, loss 0.00013461496564559639\n",
      "Iteration: 414, loss 0.00013151444727554917\n",
      "Iteration: 415, loss 0.00012853914813604206\n",
      "Iteration: 416, loss 0.00012609022087417543\n",
      "Iteration: 417, loss 0.0001233679213328287\n",
      "Iteration: 418, loss 0.00012106727808713913\n",
      "Iteration: 419, loss 0.00011868866567965597\n",
      "Iteration: 420, loss 0.00011602445738390088\n",
      "Iteration: 421, loss 0.00011381164949852973\n",
      "Iteration: 422, loss 0.00011178540444234386\n",
      "Iteration: 423, loss 0.00010921259672613814\n",
      "Iteration: 424, loss 0.00010720444697653875\n",
      "Iteration: 425, loss 0.00010512444714549929\n",
      "Iteration: 426, loss 0.00010303015005774796\n",
      "Iteration: 427, loss 0.00010101731459144503\n",
      "Iteration: 428, loss 9.902779129333794e-05\n",
      "Iteration: 429, loss 9.708905417937785e-05\n",
      "Iteration: 430, loss 9.515944839222357e-05\n",
      "Iteration: 431, loss 9.31616232264787e-05\n",
      "Iteration: 432, loss 9.153874998446554e-05\n",
      "Iteration: 433, loss 8.99562073755078e-05\n",
      "Iteration: 434, loss 8.824725955491886e-05\n",
      "Iteration: 435, loss 8.684051863383502e-05\n",
      "Iteration: 436, loss 8.496447117067873e-05\n",
      "Iteration: 437, loss 8.367368718609214e-05\n",
      "Iteration: 438, loss 8.208030340028927e-05\n",
      "Iteration: 439, loss 8.085175068117678e-05\n",
      "Iteration: 440, loss 7.935795292723924e-05\n",
      "Iteration: 441, loss 7.81534836278297e-05\n",
      "Iteration: 442, loss 7.660258415853605e-05\n",
      "Iteration: 443, loss 7.530640868935734e-05\n",
      "Iteration: 444, loss 7.417720917146653e-05\n",
      "Iteration: 445, loss 7.295639807125553e-05\n",
      "Iteration: 446, loss 7.19230156391859e-05\n",
      "Iteration: 447, loss 7.045687380013987e-05\n",
      "Iteration: 448, loss 6.934711564099416e-05\n",
      "Iteration: 449, loss 6.810378545196727e-05\n",
      "Iteration: 450, loss 6.702836253680289e-05\n",
      "Iteration: 451, loss 6.566615775227547e-05\n",
      "Iteration: 452, loss 6.477994611486793e-05\n",
      "Iteration: 453, loss 6.375447264872491e-05\n",
      "Iteration: 454, loss 6.280939851421863e-05\n",
      "Iteration: 455, loss 6.194626621436328e-05\n",
      "Iteration: 456, loss 6.07718393439427e-05\n",
      "Iteration: 457, loss 6.002782902214676e-05\n",
      "Iteration: 458, loss 5.877345392946154e-05\n",
      "Iteration: 459, loss 5.797792982775718e-05\n",
      "Iteration: 460, loss 5.731653800467029e-05\n",
      "Iteration: 461, loss 5.627164864563383e-05\n",
      "Iteration: 462, loss 5.5446551414206624e-05\n",
      "Iteration: 463, loss 5.482881533680484e-05\n",
      "Iteration: 464, loss 5.368877464206889e-05\n",
      "Iteration: 465, loss 5.3175615903455764e-05\n",
      "Iteration: 466, loss 5.229920498095453e-05\n",
      "Iteration: 467, loss 5.1521161367418244e-05\n",
      "Iteration: 468, loss 5.07457552885171e-05\n",
      "Iteration: 469, loss 4.99978668813128e-05\n",
      "Iteration: 470, loss 4.9314559873891994e-05\n",
      "Iteration: 471, loss 4.867718234891072e-05\n",
      "Iteration: 472, loss 4.802067996934056e-05\n",
      "Iteration: 473, loss 4.7540343075525016e-05\n",
      "Iteration: 474, loss 4.668791370932013e-05\n",
      "Iteration: 475, loss 4.614976205630228e-05\n",
      "Iteration: 476, loss 4.5376811613095924e-05\n",
      "Iteration: 477, loss 4.471713327802718e-05\n",
      "Iteration: 478, loss 4.4222942960914224e-05\n",
      "Iteration: 479, loss 4.348422953626141e-05\n",
      "Iteration: 480, loss 4.269223063602112e-05\n",
      "Iteration: 481, loss 4.209642065688968e-05\n",
      "Iteration: 482, loss 4.16071925428696e-05\n",
      "Iteration: 483, loss 4.096805059816688e-05\n",
      "Iteration: 484, loss 4.049953349749558e-05\n",
      "Iteration: 485, loss 4.005550727015361e-05\n",
      "Iteration: 486, loss 3.9589154766872525e-05\n",
      "Iteration: 487, loss 3.9048140024533495e-05\n",
      "Iteration: 488, loss 3.871656736009754e-05\n",
      "Iteration: 489, loss 3.813947114394978e-05\n",
      "Iteration: 490, loss 3.754457429749891e-05\n",
      "Iteration: 491, loss 3.7054080166853964e-05\n",
      "Iteration: 492, loss 3.665282565634698e-05\n",
      "Iteration: 493, loss 3.6276695027481765e-05\n",
      "Iteration: 494, loss 3.5766941437032074e-05\n",
      "Iteration: 495, loss 3.5284279874758795e-05\n",
      "Iteration: 496, loss 3.488552101771347e-05\n",
      "Iteration: 497, loss 3.461315645836294e-05\n",
      "Iteration: 498, loss 3.4164109820267186e-05\n",
      "Iteration: 499, loss 3.398109402041882e-05\n"
     ]
    }
   ],
   "source": [
    "for i in range(500):\n",
    "    # forward pass\n",
    "    Z = X.mm(W1)\n",
    "    phi = Z.clamp(min=0)\n",
    "    pred = phi.mm(W2)\n",
    "    # compute/print loss\n",
    "    loss = (Y - pred).pow(2).sum()\n",
    "    print('Iteration: {}, loss {}'.format(i, loss))\n",
    "    # backward pass\n",
    "    delta_l2 = 2 * (pred - Y)\n",
    "    grad_l2 = phi.t().mm(delta_l2)\n",
    "    delta_l1 = delta_l2.mm(W2.t())\n",
    "    delta_l1[phi == 0] = 0\n",
    "    grad_l1 = X.t().mm(delta_l1)\n",
    "    # updates \n",
    "    W1 -= gamma * grad_l1\n",
    "    W2 -= gamma * grad_l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using PyTorch Autograd package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.randn(D1, H, device=device, dtype=dtype, requires_grad=True)\n",
    "W2 = torch.randn(H, D2, device=device, dtype=dtype, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, loss 31276690.0\n",
      "Iteration: 1, loss 29065940.0\n",
      "Iteration: 2, loss 29749312.0\n",
      "Iteration: 3, loss 28901852.0\n",
      "Iteration: 4, loss 24143488.0\n",
      "Iteration: 5, loss 16741321.0\n",
      "Iteration: 6, loss 9938664.0\n",
      "Iteration: 7, loss 5472923.0\n",
      "Iteration: 8, loss 3072420.75\n",
      "Iteration: 9, loss 1878972.375\n",
      "Iteration: 10, loss 1277651.25\n",
      "Iteration: 11, loss 950733.125\n",
      "Iteration: 12, loss 752579.0\n",
      "Iteration: 13, loss 619021.0\n",
      "Iteration: 14, loss 520922.6875\n",
      "Iteration: 15, loss 444675.53125\n",
      "Iteration: 16, loss 383197.59375\n",
      "Iteration: 17, loss 332514.40625\n",
      "Iteration: 18, loss 290149.5\n",
      "Iteration: 19, loss 254345.78125\n",
      "Iteration: 20, loss 223846.953125\n",
      "Iteration: 21, loss 197738.203125\n",
      "Iteration: 22, loss 175247.03125\n",
      "Iteration: 23, loss 155801.375\n",
      "Iteration: 24, loss 138910.53125\n",
      "Iteration: 25, loss 124193.390625\n",
      "Iteration: 26, loss 111320.515625\n",
      "Iteration: 27, loss 100015.9140625\n",
      "Iteration: 28, loss 90070.2734375\n",
      "Iteration: 29, loss 81291.2265625\n",
      "Iteration: 30, loss 73563.140625\n",
      "Iteration: 31, loss 66703.8984375\n",
      "Iteration: 32, loss 60593.75390625\n",
      "Iteration: 33, loss 55137.359375\n",
      "Iteration: 34, loss 50256.9921875\n",
      "Iteration: 35, loss 45876.6640625\n",
      "Iteration: 36, loss 41940.91015625\n",
      "Iteration: 37, loss 38404.8828125\n",
      "Iteration: 38, loss 35211.6484375\n",
      "Iteration: 39, loss 32324.064453125\n",
      "Iteration: 40, loss 29708.462890625\n",
      "Iteration: 41, loss 27337.71484375\n",
      "Iteration: 42, loss 25184.68359375\n",
      "Iteration: 43, loss 23223.724609375\n",
      "Iteration: 44, loss 21438.349609375\n",
      "Iteration: 45, loss 19812.3828125\n",
      "Iteration: 46, loss 18328.26171875\n",
      "Iteration: 47, loss 16970.08203125\n",
      "Iteration: 48, loss 15725.5244140625\n",
      "Iteration: 49, loss 14584.4921875\n",
      "Iteration: 50, loss 13536.78515625\n",
      "Iteration: 51, loss 12572.8642578125\n",
      "Iteration: 52, loss 11686.2724609375\n",
      "Iteration: 53, loss 10869.6396484375\n",
      "Iteration: 54, loss 10116.822265625\n",
      "Iteration: 55, loss 9421.9521484375\n",
      "Iteration: 56, loss 8780.15234375\n",
      "Iteration: 57, loss 8186.78857421875\n",
      "Iteration: 58, loss 7638.0029296875\n",
      "Iteration: 59, loss 7130.1982421875\n",
      "Iteration: 60, loss 6659.291015625\n",
      "Iteration: 61, loss 6222.6552734375\n",
      "Iteration: 62, loss 5817.57373046875\n",
      "Iteration: 63, loss 5441.2626953125\n",
      "Iteration: 64, loss 5091.75244140625\n",
      "Iteration: 65, loss 4766.75390625\n",
      "Iteration: 66, loss 4464.33349609375\n",
      "Iteration: 67, loss 4182.7294921875\n",
      "Iteration: 68, loss 3920.4326171875\n",
      "Iteration: 69, loss 3676.15576171875\n",
      "Iteration: 70, loss 3448.087890625\n",
      "Iteration: 71, loss 3235.55029296875\n",
      "Iteration: 72, loss 3037.251953125\n",
      "Iteration: 73, loss 2852.1064453125\n",
      "Iteration: 74, loss 2679.0966796875\n",
      "Iteration: 75, loss 2517.260009765625\n",
      "Iteration: 76, loss 2365.92138671875\n",
      "Iteration: 77, loss 2224.330810546875\n",
      "Iteration: 78, loss 2091.783935546875\n",
      "Iteration: 79, loss 1967.6995849609375\n",
      "Iteration: 80, loss 1851.497314453125\n",
      "Iteration: 81, loss 1742.523681640625\n",
      "Iteration: 82, loss 1640.3956298828125\n",
      "Iteration: 83, loss 1544.671875\n",
      "Iteration: 84, loss 1454.9154052734375\n",
      "Iteration: 85, loss 1370.78515625\n",
      "Iteration: 86, loss 1291.76513671875\n",
      "Iteration: 87, loss 1217.489501953125\n",
      "Iteration: 88, loss 1147.7371826171875\n",
      "Iteration: 89, loss 1082.220703125\n",
      "Iteration: 90, loss 1020.6529541015625\n",
      "Iteration: 91, loss 962.8208618164062\n",
      "Iteration: 92, loss 908.410400390625\n",
      "Iteration: 93, loss 857.2409057617188\n",
      "Iteration: 94, loss 809.1100463867188\n",
      "Iteration: 95, loss 763.8140258789062\n",
      "Iteration: 96, loss 721.2172241210938\n",
      "Iteration: 97, loss 681.1019897460938\n",
      "Iteration: 98, loss 643.2974853515625\n",
      "Iteration: 99, loss 607.6949462890625\n",
      "Iteration: 100, loss 574.1680297851562\n",
      "Iteration: 101, loss 542.5860595703125\n",
      "Iteration: 102, loss 512.8079833984375\n",
      "Iteration: 103, loss 484.7544860839844\n",
      "Iteration: 104, loss 458.3116455078125\n",
      "Iteration: 105, loss 433.38775634765625\n",
      "Iteration: 106, loss 409.90814208984375\n",
      "Iteration: 107, loss 387.76104736328125\n",
      "Iteration: 108, loss 366.8555603027344\n",
      "Iteration: 109, loss 347.138427734375\n",
      "Iteration: 110, loss 328.5050354003906\n",
      "Iteration: 111, loss 310.9192199707031\n",
      "Iteration: 112, loss 294.3138427734375\n",
      "Iteration: 113, loss 278.62982177734375\n",
      "Iteration: 114, loss 263.8238525390625\n",
      "Iteration: 115, loss 249.8354034423828\n",
      "Iteration: 116, loss 236.6200408935547\n",
      "Iteration: 117, loss 224.13107299804688\n",
      "Iteration: 118, loss 212.3201904296875\n",
      "Iteration: 119, loss 201.15655517578125\n",
      "Iteration: 120, loss 190.60145568847656\n",
      "Iteration: 121, loss 180.62399291992188\n",
      "Iteration: 122, loss 171.18109130859375\n",
      "Iteration: 123, loss 162.25099182128906\n",
      "Iteration: 124, loss 153.80682373046875\n",
      "Iteration: 125, loss 145.81446838378906\n",
      "Iteration: 126, loss 138.2598876953125\n",
      "Iteration: 127, loss 131.10855102539062\n",
      "Iteration: 128, loss 124.34300994873047\n",
      "Iteration: 129, loss 117.93730163574219\n",
      "Iteration: 130, loss 111.8722915649414\n",
      "Iteration: 131, loss 106.12921142578125\n",
      "Iteration: 132, loss 100.695556640625\n",
      "Iteration: 133, loss 95.5466079711914\n",
      "Iteration: 134, loss 90.66988372802734\n",
      "Iteration: 135, loss 86.05128479003906\n",
      "Iteration: 136, loss 81.67649841308594\n",
      "Iteration: 137, loss 77.52633666992188\n",
      "Iteration: 138, loss 73.59529876708984\n",
      "Iteration: 139, loss 69.87065124511719\n",
      "Iteration: 140, loss 66.33952331542969\n",
      "Iteration: 141, loss 62.99336242675781\n",
      "Iteration: 142, loss 59.81818389892578\n",
      "Iteration: 143, loss 56.80919647216797\n",
      "Iteration: 144, loss 53.957244873046875\n",
      "Iteration: 145, loss 51.255218505859375\n",
      "Iteration: 146, loss 48.68815612792969\n",
      "Iteration: 147, loss 46.25346755981445\n",
      "Iteration: 148, loss 43.94491195678711\n",
      "Iteration: 149, loss 41.754600524902344\n",
      "Iteration: 150, loss 39.67671203613281\n",
      "Iteration: 151, loss 37.704246520996094\n",
      "Iteration: 152, loss 35.8315315246582\n",
      "Iteration: 153, loss 34.056968688964844\n",
      "Iteration: 154, loss 32.372642517089844\n",
      "Iteration: 155, loss 30.773805618286133\n",
      "Iteration: 156, loss 29.25434112548828\n",
      "Iteration: 157, loss 27.812835693359375\n",
      "Iteration: 158, loss 26.443679809570312\n",
      "Iteration: 159, loss 25.143836975097656\n",
      "Iteration: 160, loss 23.909276962280273\n",
      "Iteration: 161, loss 22.73663330078125\n",
      "Iteration: 162, loss 21.623741149902344\n",
      "Iteration: 163, loss 20.567590713500977\n",
      "Iteration: 164, loss 19.562971115112305\n",
      "Iteration: 165, loss 18.608776092529297\n",
      "Iteration: 166, loss 17.702285766601562\n",
      "Iteration: 167, loss 16.84085464477539\n",
      "Iteration: 168, loss 16.021865844726562\n",
      "Iteration: 169, loss 15.244160652160645\n",
      "Iteration: 170, loss 14.505508422851562\n",
      "Iteration: 171, loss 13.803646087646484\n",
      "Iteration: 172, loss 13.135977745056152\n",
      "Iteration: 173, loss 12.50145435333252\n",
      "Iteration: 174, loss 11.897867202758789\n",
      "Iteration: 175, loss 11.324670791625977\n",
      "Iteration: 176, loss 10.77910041809082\n",
      "Iteration: 177, loss 10.261027336120605\n",
      "Iteration: 178, loss 9.768145561218262\n",
      "Iteration: 179, loss 9.299349784851074\n",
      "Iteration: 180, loss 8.854069709777832\n",
      "Iteration: 181, loss 8.430227279663086\n",
      "Iteration: 182, loss 8.026884078979492\n",
      "Iteration: 183, loss 7.643174171447754\n",
      "Iteration: 184, loss 7.278566837310791\n",
      "Iteration: 185, loss 6.931499481201172\n",
      "Iteration: 186, loss 6.601247787475586\n",
      "Iteration: 187, loss 6.2873125076293945\n",
      "Iteration: 188, loss 5.988650321960449\n",
      "Iteration: 189, loss 5.704367160797119\n",
      "Iteration: 190, loss 5.43392276763916\n",
      "Iteration: 191, loss 5.176357746124268\n",
      "Iteration: 192, loss 4.931161403656006\n",
      "Iteration: 193, loss 4.697934150695801\n",
      "Iteration: 194, loss 4.476089000701904\n",
      "Iteration: 195, loss 4.264693737030029\n",
      "Iteration: 196, loss 4.063670635223389\n",
      "Iteration: 197, loss 3.8723325729370117\n",
      "Iteration: 198, loss 3.689962863922119\n",
      "Iteration: 199, loss 3.5165047645568848\n",
      "Iteration: 200, loss 3.3513004779815674\n",
      "Iteration: 201, loss 3.1937453746795654\n",
      "Iteration: 202, loss 3.044185161590576\n",
      "Iteration: 203, loss 2.9014759063720703\n",
      "Iteration: 204, loss 2.7656869888305664\n",
      "Iteration: 205, loss 2.636516571044922\n",
      "Iteration: 206, loss 2.5133249759674072\n",
      "Iteration: 207, loss 2.3958687782287598\n",
      "Iteration: 208, loss 2.2841312885284424\n",
      "Iteration: 209, loss 2.1775732040405273\n",
      "Iteration: 210, loss 2.07611083984375\n",
      "Iteration: 211, loss 1.9794764518737793\n",
      "Iteration: 212, loss 1.8874976634979248\n",
      "Iteration: 213, loss 1.8000402450561523\n",
      "Iteration: 214, loss 1.7163816690444946\n",
      "Iteration: 215, loss 1.6367204189300537\n",
      "Iteration: 216, loss 1.5610910654067993\n",
      "Iteration: 217, loss 1.488773226737976\n",
      "Iteration: 218, loss 1.4198315143585205\n",
      "Iteration: 219, loss 1.3541367053985596\n",
      "Iteration: 220, loss 1.2915538549423218\n",
      "Iteration: 221, loss 1.2321131229400635\n",
      "Iteration: 222, loss 1.1753448247909546\n",
      "Iteration: 223, loss 1.1210801601409912\n",
      "Iteration: 224, loss 1.069374918937683\n",
      "Iteration: 225, loss 1.020235538482666\n",
      "Iteration: 226, loss 0.9734558463096619\n",
      "Iteration: 227, loss 0.9286924600601196\n",
      "Iteration: 228, loss 0.886001706123352\n",
      "Iteration: 229, loss 0.8454471230506897\n",
      "Iteration: 230, loss 0.8067166805267334\n",
      "Iteration: 231, loss 0.7697762250900269\n",
      "Iteration: 232, loss 0.7344529032707214\n",
      "Iteration: 233, loss 0.7008607387542725\n",
      "Iteration: 234, loss 0.6688448190689087\n",
      "Iteration: 235, loss 0.63820481300354\n",
      "Iteration: 236, loss 0.6090637445449829\n",
      "Iteration: 237, loss 0.581325113773346\n",
      "Iteration: 238, loss 0.5548309683799744\n",
      "Iteration: 239, loss 0.5296136140823364\n",
      "Iteration: 240, loss 0.5055327415466309\n",
      "Iteration: 241, loss 0.48253244161605835\n",
      "Iteration: 242, loss 0.460490882396698\n",
      "Iteration: 243, loss 0.43962982296943665\n",
      "Iteration: 244, loss 0.4196728765964508\n",
      "Iteration: 245, loss 0.400697261095047\n",
      "Iteration: 246, loss 0.3824952244758606\n",
      "Iteration: 247, loss 0.36510488390922546\n",
      "Iteration: 248, loss 0.3486224412918091\n",
      "Iteration: 249, loss 0.33288896083831787\n",
      "Iteration: 250, loss 0.31782764196395874\n",
      "Iteration: 251, loss 0.3034490644931793\n",
      "Iteration: 252, loss 0.2897423207759857\n",
      "Iteration: 253, loss 0.27671411633491516\n",
      "Iteration: 254, loss 0.264221727848053\n",
      "Iteration: 255, loss 0.2523282766342163\n",
      "Iteration: 256, loss 0.24097180366516113\n",
      "Iteration: 257, loss 0.23013509809970856\n",
      "Iteration: 258, loss 0.21981066465377808\n",
      "Iteration: 259, loss 0.20985905826091766\n",
      "Iteration: 260, loss 0.20044904947280884\n",
      "Iteration: 261, loss 0.19148099422454834\n",
      "Iteration: 262, loss 0.18286184966564178\n",
      "Iteration: 263, loss 0.17469564080238342\n",
      "Iteration: 264, loss 0.16684311628341675\n",
      "Iteration: 265, loss 0.1593906134366989\n",
      "Iteration: 266, loss 0.15222585201263428\n",
      "Iteration: 267, loss 0.1454125940799713\n",
      "Iteration: 268, loss 0.138918936252594\n",
      "Iteration: 269, loss 0.13269972801208496\n",
      "Iteration: 270, loss 0.1267952173948288\n",
      "Iteration: 271, loss 0.12118138372898102\n",
      "Iteration: 272, loss 0.11573571711778641\n",
      "Iteration: 273, loss 0.11056219041347504\n",
      "Iteration: 274, loss 0.10562455654144287\n",
      "Iteration: 275, loss 0.10095471888780594\n",
      "Iteration: 276, loss 0.0964173674583435\n",
      "Iteration: 277, loss 0.09215956926345825\n",
      "Iteration: 278, loss 0.08807379007339478\n",
      "Iteration: 279, loss 0.08414212614297867\n",
      "Iteration: 280, loss 0.08042152971029282\n",
      "Iteration: 281, loss 0.07683070003986359\n",
      "Iteration: 282, loss 0.07343708723783493\n",
      "Iteration: 283, loss 0.070164255797863\n",
      "Iteration: 284, loss 0.06707427650690079\n",
      "Iteration: 285, loss 0.06411950290203094\n",
      "Iteration: 286, loss 0.06128436699509621\n",
      "Iteration: 287, loss 0.05857468396425247\n",
      "Iteration: 288, loss 0.05598992481827736\n",
      "Iteration: 289, loss 0.05350269749760628\n",
      "Iteration: 290, loss 0.051139116287231445\n",
      "Iteration: 291, loss 0.048886146396398544\n",
      "Iteration: 292, loss 0.046730849891901016\n",
      "Iteration: 293, loss 0.04468819871544838\n",
      "Iteration: 294, loss 0.0427328497171402\n",
      "Iteration: 295, loss 0.04084639623761177\n",
      "Iteration: 296, loss 0.03906531259417534\n",
      "Iteration: 297, loss 0.03734535351395607\n",
      "Iteration: 298, loss 0.03570793196558952\n",
      "Iteration: 299, loss 0.034146036952733994\n",
      "Iteration: 300, loss 0.032655637711286545\n",
      "Iteration: 301, loss 0.03122393973171711\n",
      "Iteration: 302, loss 0.029870443046092987\n",
      "Iteration: 303, loss 0.028561268001794815\n",
      "Iteration: 304, loss 0.02731919474899769\n",
      "Iteration: 305, loss 0.026109518483281136\n",
      "Iteration: 306, loss 0.024982532486319542\n",
      "Iteration: 307, loss 0.023892836645245552\n",
      "Iteration: 308, loss 0.022858288139104843\n",
      "Iteration: 309, loss 0.02187129482626915\n",
      "Iteration: 310, loss 0.020921222865581512\n",
      "Iteration: 311, loss 0.020015086978673935\n",
      "Iteration: 312, loss 0.01914919540286064\n",
      "Iteration: 313, loss 0.018320178613066673\n",
      "Iteration: 314, loss 0.017523162066936493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 315, loss 0.016767950728535652\n",
      "Iteration: 316, loss 0.01603885553777218\n",
      "Iteration: 317, loss 0.015351833775639534\n",
      "Iteration: 318, loss 0.014693260192871094\n",
      "Iteration: 319, loss 0.014064528048038483\n",
      "Iteration: 320, loss 0.013465954922139645\n",
      "Iteration: 321, loss 0.012892507016658783\n",
      "Iteration: 322, loss 0.012342375703155994\n",
      "Iteration: 323, loss 0.011808909475803375\n",
      "Iteration: 324, loss 0.011302454397082329\n",
      "Iteration: 325, loss 0.010830039158463478\n",
      "Iteration: 326, loss 0.010377568192780018\n",
      "Iteration: 327, loss 0.00992952473461628\n",
      "Iteration: 328, loss 0.009507437236607075\n",
      "Iteration: 329, loss 0.009107199497520924\n",
      "Iteration: 330, loss 0.00872884877026081\n",
      "Iteration: 331, loss 0.008364317007362843\n",
      "Iteration: 332, loss 0.008008880540728569\n",
      "Iteration: 333, loss 0.00767037458717823\n",
      "Iteration: 334, loss 0.0073513793759047985\n",
      "Iteration: 335, loss 0.00704905204474926\n",
      "Iteration: 336, loss 0.006754837464541197\n",
      "Iteration: 337, loss 0.006480515003204346\n",
      "Iteration: 338, loss 0.006214209366589785\n",
      "Iteration: 339, loss 0.005960861220955849\n",
      "Iteration: 340, loss 0.005718932952731848\n",
      "Iteration: 341, loss 0.005485295318067074\n",
      "Iteration: 342, loss 0.005264109931886196\n",
      "Iteration: 343, loss 0.005046002101153135\n",
      "Iteration: 344, loss 0.004845189396291971\n",
      "Iteration: 345, loss 0.004646962974220514\n",
      "Iteration: 346, loss 0.004460561089217663\n",
      "Iteration: 347, loss 0.004283939953893423\n",
      "Iteration: 348, loss 0.004113988485187292\n",
      "Iteration: 349, loss 0.003949691541492939\n",
      "Iteration: 350, loss 0.003792892210185528\n",
      "Iteration: 351, loss 0.0036471481435000896\n",
      "Iteration: 352, loss 0.0035055887419730425\n",
      "Iteration: 353, loss 0.0033685958478599787\n",
      "Iteration: 354, loss 0.0032369049731642008\n",
      "Iteration: 355, loss 0.0031166374683380127\n",
      "Iteration: 356, loss 0.002995007671415806\n",
      "Iteration: 357, loss 0.0028801753651350737\n",
      "Iteration: 358, loss 0.002772221341729164\n",
      "Iteration: 359, loss 0.0026667495258152485\n",
      "Iteration: 360, loss 0.00256859022192657\n",
      "Iteration: 361, loss 0.0024728935677558184\n",
      "Iteration: 362, loss 0.0023815373424440622\n",
      "Iteration: 363, loss 0.00229180371388793\n",
      "Iteration: 364, loss 0.0022114692255854607\n",
      "Iteration: 365, loss 0.0021296737249940634\n",
      "Iteration: 366, loss 0.0020508586894720793\n",
      "Iteration: 367, loss 0.001978491432964802\n",
      "Iteration: 368, loss 0.0019081848440691829\n",
      "Iteration: 369, loss 0.0018404391594231129\n",
      "Iteration: 370, loss 0.0017736861482262611\n",
      "Iteration: 371, loss 0.0017116335220634937\n",
      "Iteration: 372, loss 0.0016528732376173139\n",
      "Iteration: 373, loss 0.0015956555726006627\n",
      "Iteration: 374, loss 0.0015387614257633686\n",
      "Iteration: 375, loss 0.0014872039901092649\n",
      "Iteration: 376, loss 0.0014350448036566377\n",
      "Iteration: 377, loss 0.0013863086933270097\n",
      "Iteration: 378, loss 0.0013403119519352913\n",
      "Iteration: 379, loss 0.0012949504889547825\n",
      "Iteration: 380, loss 0.0012518562143668532\n",
      "Iteration: 381, loss 0.0012110284296795726\n",
      "Iteration: 382, loss 0.001172419753856957\n",
      "Iteration: 383, loss 0.0011337175965309143\n",
      "Iteration: 384, loss 0.001098746550269425\n",
      "Iteration: 385, loss 0.0010619687382131815\n",
      "Iteration: 386, loss 0.0010295113315805793\n",
      "Iteration: 387, loss 0.0009982893243432045\n",
      "Iteration: 388, loss 0.0009651011787354946\n",
      "Iteration: 389, loss 0.0009351681219413877\n",
      "Iteration: 390, loss 0.0009056210401467979\n",
      "Iteration: 391, loss 0.0008775524329394102\n",
      "Iteration: 392, loss 0.0008518793038092554\n",
      "Iteration: 393, loss 0.0008265333017334342\n",
      "Iteration: 394, loss 0.0008011979516595602\n",
      "Iteration: 395, loss 0.0007768983487039804\n",
      "Iteration: 396, loss 0.0007543150568380952\n",
      "Iteration: 397, loss 0.0007325285114347935\n",
      "Iteration: 398, loss 0.0007118702633306384\n",
      "Iteration: 399, loss 0.0006914879777468741\n",
      "Iteration: 400, loss 0.0006707657594233751\n",
      "Iteration: 401, loss 0.0006513402331620455\n",
      "Iteration: 402, loss 0.0006324657006189227\n",
      "Iteration: 403, loss 0.000615578843280673\n",
      "Iteration: 404, loss 0.000598423182964325\n",
      "Iteration: 405, loss 0.000582511886022985\n",
      "Iteration: 406, loss 0.0005674432613886893\n",
      "Iteration: 407, loss 0.00055085145868361\n",
      "Iteration: 408, loss 0.0005357866757549345\n",
      "Iteration: 409, loss 0.000522840884514153\n",
      "Iteration: 410, loss 0.0005079902475699782\n",
      "Iteration: 411, loss 0.0004950900329276919\n",
      "Iteration: 412, loss 0.00048175326082855463\n",
      "Iteration: 413, loss 0.0004692527581937611\n",
      "Iteration: 414, loss 0.0004569684970192611\n",
      "Iteration: 415, loss 0.0004464881494641304\n",
      "Iteration: 416, loss 0.0004346717323642224\n",
      "Iteration: 417, loss 0.0004241109418217093\n",
      "Iteration: 418, loss 0.00041475926991552114\n",
      "Iteration: 419, loss 0.00040391075890511274\n",
      "Iteration: 420, loss 0.0003941553586628288\n",
      "Iteration: 421, loss 0.00038504565600305796\n",
      "Iteration: 422, loss 0.00037514703581109643\n",
      "Iteration: 423, loss 0.00036650060792453587\n",
      "Iteration: 424, loss 0.0003577619499992579\n",
      "Iteration: 425, loss 0.00034853871329687536\n",
      "Iteration: 426, loss 0.00034054077696055174\n",
      "Iteration: 427, loss 0.0003327132435515523\n",
      "Iteration: 428, loss 0.00032470509177073836\n",
      "Iteration: 429, loss 0.00031784881139174104\n",
      "Iteration: 430, loss 0.0003107293741777539\n",
      "Iteration: 431, loss 0.0003035532427020371\n",
      "Iteration: 432, loss 0.00029691224335692823\n",
      "Iteration: 433, loss 0.0002901736879721284\n",
      "Iteration: 434, loss 0.0002836882194969803\n",
      "Iteration: 435, loss 0.00027793622575700283\n",
      "Iteration: 436, loss 0.00027140567544847727\n",
      "Iteration: 437, loss 0.0002661182952579111\n",
      "Iteration: 438, loss 0.00026050268206745386\n",
      "Iteration: 439, loss 0.00025542048388160765\n",
      "Iteration: 440, loss 0.00024996549473144114\n",
      "Iteration: 441, loss 0.00024438000400550663\n",
      "Iteration: 442, loss 0.00023915221390780061\n",
      "Iteration: 443, loss 0.00023418797354679555\n",
      "Iteration: 444, loss 0.00022924909717403352\n",
      "Iteration: 445, loss 0.0002241632028017193\n",
      "Iteration: 446, loss 0.00021992111578583717\n",
      "Iteration: 447, loss 0.00021572565310634673\n",
      "Iteration: 448, loss 0.0002109796041622758\n",
      "Iteration: 449, loss 0.00020676162966992706\n",
      "Iteration: 450, loss 0.00020283200137782842\n",
      "Iteration: 451, loss 0.00019916567543987185\n",
      "Iteration: 452, loss 0.00019556283950805664\n",
      "Iteration: 453, loss 0.00019121244258712977\n",
      "Iteration: 454, loss 0.00018782129336614162\n",
      "Iteration: 455, loss 0.00018466004985384643\n",
      "Iteration: 456, loss 0.00018139976600650698\n",
      "Iteration: 457, loss 0.0001782426261343062\n",
      "Iteration: 458, loss 0.00017504103016108274\n",
      "Iteration: 459, loss 0.0001712213852442801\n",
      "Iteration: 460, loss 0.00016763611347414553\n",
      "Iteration: 461, loss 0.00016494514420628548\n",
      "Iteration: 462, loss 0.00016160598897840828\n",
      "Iteration: 463, loss 0.0001584901037858799\n",
      "Iteration: 464, loss 0.00015560915926471353\n",
      "Iteration: 465, loss 0.0001531599264126271\n",
      "Iteration: 466, loss 0.0001502778468420729\n",
      "Iteration: 467, loss 0.0001477640907978639\n",
      "Iteration: 468, loss 0.0001454176672268659\n",
      "Iteration: 469, loss 0.00014279306924436241\n",
      "Iteration: 470, loss 0.00014069995086174458\n",
      "Iteration: 471, loss 0.00013771212252322584\n",
      "Iteration: 472, loss 0.00013548965216614306\n",
      "Iteration: 473, loss 0.00013373605906963348\n",
      "Iteration: 474, loss 0.00013158189540263265\n",
      "Iteration: 475, loss 0.00012920003791805357\n",
      "Iteration: 476, loss 0.00012734619667753577\n",
      "Iteration: 477, loss 0.00012488567153923213\n",
      "Iteration: 478, loss 0.00012281410454306751\n",
      "Iteration: 479, loss 0.000120762393635232\n",
      "Iteration: 480, loss 0.00011885525600519031\n",
      "Iteration: 481, loss 0.0001172720585600473\n",
      "Iteration: 482, loss 0.00011547523899935186\n",
      "Iteration: 483, loss 0.00011355186143191531\n",
      "Iteration: 484, loss 0.00011168070341227576\n",
      "Iteration: 485, loss 0.00010983927495544776\n",
      "Iteration: 486, loss 0.00010835479770321399\n",
      "Iteration: 487, loss 0.0001065756005118601\n",
      "Iteration: 488, loss 0.00010501890938030556\n",
      "Iteration: 489, loss 0.00010372942051617429\n",
      "Iteration: 490, loss 0.00010233241482637823\n",
      "Iteration: 491, loss 0.00010034724255092442\n",
      "Iteration: 492, loss 9.875082469079643e-05\n",
      "Iteration: 493, loss 9.759296517586336e-05\n",
      "Iteration: 494, loss 9.610211418475956e-05\n",
      "Iteration: 495, loss 9.473217505728826e-05\n",
      "Iteration: 496, loss 9.314654744230211e-05\n",
      "Iteration: 497, loss 9.194147423841059e-05\n",
      "Iteration: 498, loss 9.076571586774662e-05\n",
      "Iteration: 499, loss 8.922373672248796e-05\n"
     ]
    }
   ],
   "source": [
    "for i in range(500):\n",
    "    # forward pass\n",
    "    pred = X.mm(W1).clamp(min=0).mm(W2)\n",
    "    # compute/print loss\n",
    "    loss = (Y - pred).pow(2).sum()\n",
    "    print('Iteration: {}, loss {}'.format(i, loss.item()))\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    # updates \n",
    "    with torch.no_grad(): #temporarily set requires_grad flags to False\n",
    "        W1 -= gamma * W1.grad\n",
    "        W2 -= gamma * W2.grad\n",
    "        W1.grad.zero_()\n",
    "        W2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining and using a custom Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyReLu(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.randn(D1, H, device=device, dtype=dtype, requires_grad=True)\n",
    "W2 = torch.randn(H, D2, device=device, dtype=dtype, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, loss 31672936.0\n",
      "Iteration: 1, loss 26050776.0\n",
      "Iteration: 2, loss 23715592.0\n",
      "Iteration: 3, loss 21074248.0\n",
      "Iteration: 4, loss 17157604.0\n",
      "Iteration: 5, loss 12431002.0\n",
      "Iteration: 6, loss 8225142.5\n",
      "Iteration: 7, loss 5148345.0\n",
      "Iteration: 8, loss 3213393.25\n",
      "Iteration: 9, loss 2074633.0\n",
      "Iteration: 10, loss 1418945.75\n",
      "Iteration: 11, loss 1031025.9375\n",
      "Iteration: 12, loss 790009.5625\n",
      "Iteration: 13, loss 630517.875\n",
      "Iteration: 14, loss 518066.1875\n",
      "Iteration: 15, loss 434290.75\n",
      "Iteration: 16, loss 369253.75\n",
      "Iteration: 17, loss 317299.78125\n",
      "Iteration: 18, loss 274748.75\n",
      "Iteration: 19, loss 239268.375\n",
      "Iteration: 20, loss 209326.5625\n",
      "Iteration: 21, loss 183902.8125\n",
      "Iteration: 22, loss 162099.984375\n",
      "Iteration: 23, loss 143333.1875\n",
      "Iteration: 24, loss 127103.703125\n",
      "Iteration: 25, loss 113002.03125\n",
      "Iteration: 26, loss 100703.4140625\n",
      "Iteration: 27, loss 89949.546875\n",
      "Iteration: 28, loss 80511.046875\n",
      "Iteration: 29, loss 72205.9453125\n",
      "Iteration: 30, loss 64878.6328125\n",
      "Iteration: 31, loss 58394.10546875\n",
      "Iteration: 32, loss 52641.921875\n",
      "Iteration: 33, loss 47527.203125\n",
      "Iteration: 34, loss 42975.3359375\n",
      "Iteration: 35, loss 38916.71484375\n",
      "Iteration: 36, loss 35283.875\n",
      "Iteration: 37, loss 32028.689453125\n",
      "Iteration: 38, loss 29108.91796875\n",
      "Iteration: 39, loss 26484.3046875\n",
      "Iteration: 40, loss 24122.3984375\n",
      "Iteration: 41, loss 21993.970703125\n",
      "Iteration: 42, loss 20072.994140625\n",
      "Iteration: 43, loss 18335.701171875\n",
      "Iteration: 44, loss 16763.77734375\n",
      "Iteration: 45, loss 15339.2607421875\n",
      "Iteration: 46, loss 14047.7333984375\n",
      "Iteration: 47, loss 12875.7421875\n",
      "Iteration: 48, loss 11810.169921875\n",
      "Iteration: 49, loss 10840.478515625\n",
      "Iteration: 50, loss 9957.501953125\n",
      "Iteration: 51, loss 9152.318359375\n",
      "Iteration: 52, loss 8417.6005859375\n",
      "Iteration: 53, loss 7747.1474609375\n",
      "Iteration: 54, loss 7134.146484375\n",
      "Iteration: 55, loss 6573.59228515625\n",
      "Iteration: 56, loss 6060.85400390625\n",
      "Iteration: 57, loss 5590.96923828125\n",
      "Iteration: 58, loss 5160.32861328125\n",
      "Iteration: 59, loss 4765.3779296875\n",
      "Iteration: 60, loss 4402.80078125\n",
      "Iteration: 61, loss 4069.52587890625\n",
      "Iteration: 62, loss 3763.18212890625\n",
      "Iteration: 63, loss 3481.538818359375\n",
      "Iteration: 64, loss 3222.355224609375\n",
      "Iteration: 65, loss 2983.614501953125\n",
      "Iteration: 66, loss 2763.705322265625\n",
      "Iteration: 67, loss 2561.0546875\n",
      "Iteration: 68, loss 2374.24267578125\n",
      "Iteration: 69, loss 2201.84912109375\n",
      "Iteration: 70, loss 2042.5770263671875\n",
      "Iteration: 71, loss 1895.4677734375\n",
      "Iteration: 72, loss 1759.5904541015625\n",
      "Iteration: 73, loss 1634.029052734375\n",
      "Iteration: 74, loss 1517.861572265625\n",
      "Iteration: 75, loss 1410.43701171875\n",
      "Iteration: 76, loss 1310.9932861328125\n",
      "Iteration: 77, loss 1218.973876953125\n",
      "Iteration: 78, loss 1133.677490234375\n",
      "Iteration: 79, loss 1054.624755859375\n",
      "Iteration: 80, loss 981.3659057617188\n",
      "Iteration: 81, loss 913.4755249023438\n",
      "Iteration: 82, loss 850.5162353515625\n",
      "Iteration: 83, loss 792.039306640625\n",
      "Iteration: 84, loss 737.8251953125\n",
      "Iteration: 85, loss 687.5257568359375\n",
      "Iteration: 86, loss 640.803466796875\n",
      "Iteration: 87, loss 597.3916625976562\n",
      "Iteration: 88, loss 557.0698852539062\n",
      "Iteration: 89, loss 519.57373046875\n",
      "Iteration: 90, loss 484.69952392578125\n",
      "Iteration: 91, loss 452.2450256347656\n",
      "Iteration: 92, loss 422.060302734375\n",
      "Iteration: 93, loss 393.97857666015625\n",
      "Iteration: 94, loss 367.8431396484375\n",
      "Iteration: 95, loss 343.500244140625\n",
      "Iteration: 96, loss 320.82489013671875\n",
      "Iteration: 97, loss 299.6983642578125\n",
      "Iteration: 98, loss 280.0041198730469\n",
      "Iteration: 99, loss 261.6585693359375\n",
      "Iteration: 100, loss 244.56968688964844\n",
      "Iteration: 101, loss 228.63430786132812\n",
      "Iteration: 102, loss 213.77536010742188\n",
      "Iteration: 103, loss 199.920654296875\n",
      "Iteration: 104, loss 186.9822998046875\n",
      "Iteration: 105, loss 174.90896606445312\n",
      "Iteration: 106, loss 163.6474151611328\n",
      "Iteration: 107, loss 153.1388397216797\n",
      "Iteration: 108, loss 143.31692504882812\n",
      "Iteration: 109, loss 134.1475830078125\n",
      "Iteration: 110, loss 125.58031463623047\n",
      "Iteration: 111, loss 117.5827865600586\n",
      "Iteration: 112, loss 110.10887908935547\n",
      "Iteration: 113, loss 103.11939239501953\n",
      "Iteration: 114, loss 96.5864028930664\n",
      "Iteration: 115, loss 90.48135375976562\n",
      "Iteration: 116, loss 84.77201843261719\n",
      "Iteration: 117, loss 79.43473815917969\n",
      "Iteration: 118, loss 74.44178009033203\n",
      "Iteration: 119, loss 69.77201843261719\n",
      "Iteration: 120, loss 65.4000015258789\n",
      "Iteration: 121, loss 61.3110237121582\n",
      "Iteration: 122, loss 57.485992431640625\n",
      "Iteration: 123, loss 53.903526306152344\n",
      "Iteration: 124, loss 50.550025939941406\n",
      "Iteration: 125, loss 47.41240692138672\n",
      "Iteration: 126, loss 44.47283935546875\n",
      "Iteration: 127, loss 41.71991729736328\n",
      "Iteration: 128, loss 39.14195251464844\n",
      "Iteration: 129, loss 36.7266731262207\n",
      "Iteration: 130, loss 34.463897705078125\n",
      "Iteration: 131, loss 32.34421157836914\n",
      "Iteration: 132, loss 30.357749938964844\n",
      "Iteration: 133, loss 28.495445251464844\n",
      "Iteration: 134, loss 26.750171661376953\n",
      "Iteration: 135, loss 25.11534881591797\n",
      "Iteration: 136, loss 23.58084487915039\n",
      "Iteration: 137, loss 22.14322280883789\n",
      "Iteration: 138, loss 20.794536590576172\n",
      "Iteration: 139, loss 19.530410766601562\n",
      "Iteration: 140, loss 18.34461212158203\n",
      "Iteration: 141, loss 17.231555938720703\n",
      "Iteration: 142, loss 16.18777084350586\n",
      "Iteration: 143, loss 15.208222389221191\n",
      "Iteration: 144, loss 14.289281845092773\n",
      "Iteration: 145, loss 13.42727279663086\n",
      "Iteration: 146, loss 12.617801666259766\n",
      "Iteration: 147, loss 11.858499526977539\n",
      "Iteration: 148, loss 11.145814895629883\n",
      "Iteration: 149, loss 10.476040840148926\n",
      "Iteration: 150, loss 9.848307609558105\n",
      "Iteration: 151, loss 9.257884979248047\n",
      "Iteration: 152, loss 8.703585624694824\n",
      "Iteration: 153, loss 8.183445930480957\n",
      "Iteration: 154, loss 7.694948196411133\n",
      "Iteration: 155, loss 7.23606538772583\n",
      "Iteration: 156, loss 6.804910659790039\n",
      "Iteration: 157, loss 6.399781227111816\n",
      "Iteration: 158, loss 6.019511699676514\n",
      "Iteration: 159, loss 5.66215181350708\n",
      "Iteration: 160, loss 5.32585334777832\n",
      "Iteration: 161, loss 5.010293006896973\n",
      "Iteration: 162, loss 4.713712692260742\n",
      "Iteration: 163, loss 4.434889316558838\n",
      "Iteration: 164, loss 4.172804832458496\n",
      "Iteration: 165, loss 3.9262919425964355\n",
      "Iteration: 166, loss 3.6947989463806152\n",
      "Iteration: 167, loss 3.4772510528564453\n",
      "Iteration: 168, loss 3.2724289894104004\n",
      "Iteration: 169, loss 3.0799965858459473\n",
      "Iteration: 170, loss 2.8988900184631348\n",
      "Iteration: 171, loss 2.7290656566619873\n",
      "Iteration: 172, loss 2.5691401958465576\n",
      "Iteration: 173, loss 2.4187769889831543\n",
      "Iteration: 174, loss 2.277188301086426\n",
      "Iteration: 175, loss 2.144441843032837\n",
      "Iteration: 176, loss 2.01914381980896\n",
      "Iteration: 177, loss 1.9012844562530518\n",
      "Iteration: 178, loss 1.7906180620193481\n",
      "Iteration: 179, loss 1.686488151550293\n",
      "Iteration: 180, loss 1.5882176160812378\n",
      "Iteration: 181, loss 1.4960174560546875\n",
      "Iteration: 182, loss 1.4090867042541504\n",
      "Iteration: 183, loss 1.32733154296875\n",
      "Iteration: 184, loss 1.2504329681396484\n",
      "Iteration: 185, loss 1.1779495477676392\n",
      "Iteration: 186, loss 1.1097744703292847\n",
      "Iteration: 187, loss 1.045581579208374\n",
      "Iteration: 188, loss 0.9851059317588806\n",
      "Iteration: 189, loss 0.9282604455947876\n",
      "Iteration: 190, loss 0.8746417760848999\n",
      "Iteration: 191, loss 0.8241732716560364\n",
      "Iteration: 192, loss 0.776695728302002\n",
      "Iteration: 193, loss 0.7319887280464172\n",
      "Iteration: 194, loss 0.6898354291915894\n",
      "Iteration: 195, loss 0.6501855254173279\n",
      "Iteration: 196, loss 0.6127774715423584\n",
      "Iteration: 197, loss 0.5775872468948364\n",
      "Iteration: 198, loss 0.5444790124893188\n",
      "Iteration: 199, loss 0.5131707191467285\n",
      "Iteration: 200, loss 0.4836864769458771\n",
      "Iteration: 201, loss 0.4560709595680237\n",
      "Iteration: 202, loss 0.42993488907814026\n",
      "Iteration: 203, loss 0.40533149242401123\n",
      "Iteration: 204, loss 0.382144570350647\n",
      "Iteration: 205, loss 0.3602506220340729\n",
      "Iteration: 206, loss 0.3397287428379059\n",
      "Iteration: 207, loss 0.3203171491622925\n",
      "Iteration: 208, loss 0.30202993750572205\n",
      "Iteration: 209, loss 0.28488367795944214\n",
      "Iteration: 210, loss 0.26861700415611267\n",
      "Iteration: 211, loss 0.25335317850112915\n",
      "Iteration: 212, loss 0.23893378674983978\n",
      "Iteration: 213, loss 0.22535178065299988\n",
      "Iteration: 214, loss 0.21252721548080444\n",
      "Iteration: 215, loss 0.20043718814849854\n",
      "Iteration: 216, loss 0.18904933333396912\n",
      "Iteration: 217, loss 0.17832797765731812\n",
      "Iteration: 218, loss 0.16821974515914917\n",
      "Iteration: 219, loss 0.1586841642856598\n",
      "Iteration: 220, loss 0.1496887505054474\n",
      "Iteration: 221, loss 0.1411951631307602\n",
      "Iteration: 222, loss 0.1331881284713745\n",
      "Iteration: 223, loss 0.12563733756542206\n",
      "Iteration: 224, loss 0.11852626502513885\n",
      "Iteration: 225, loss 0.11184626817703247\n",
      "Iteration: 226, loss 0.1055397316813469\n",
      "Iteration: 227, loss 0.09955637156963348\n",
      "Iteration: 228, loss 0.09395737200975418\n",
      "Iteration: 229, loss 0.08863312005996704\n",
      "Iteration: 230, loss 0.08365306258201599\n",
      "Iteration: 231, loss 0.0789199247956276\n",
      "Iteration: 232, loss 0.0744667649269104\n",
      "Iteration: 233, loss 0.07029499113559723\n",
      "Iteration: 234, loss 0.06633159518241882\n",
      "Iteration: 235, loss 0.06260162591934204\n",
      "Iteration: 236, loss 0.059069592505693436\n",
      "Iteration: 237, loss 0.055762916803359985\n",
      "Iteration: 238, loss 0.05263213813304901\n",
      "Iteration: 239, loss 0.04969819635152817\n",
      "Iteration: 240, loss 0.046894654631614685\n",
      "Iteration: 241, loss 0.04427509754896164\n",
      "Iteration: 242, loss 0.04178735241293907\n",
      "Iteration: 243, loss 0.039441004395484924\n",
      "Iteration: 244, loss 0.03723042085766792\n",
      "Iteration: 245, loss 0.03514372929930687\n",
      "Iteration: 246, loss 0.033178117126226425\n",
      "Iteration: 247, loss 0.0313122384250164\n",
      "Iteration: 248, loss 0.0295647494494915\n",
      "Iteration: 249, loss 0.02791878953576088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 250, loss 0.02635190263390541\n",
      "Iteration: 251, loss 0.02489776723086834\n",
      "Iteration: 252, loss 0.023511137813329697\n",
      "Iteration: 253, loss 0.02219097688794136\n",
      "Iteration: 254, loss 0.020958226174116135\n",
      "Iteration: 255, loss 0.01980147510766983\n",
      "Iteration: 256, loss 0.018708545714616776\n",
      "Iteration: 257, loss 0.0176742784678936\n",
      "Iteration: 258, loss 0.01669210195541382\n",
      "Iteration: 259, loss 0.015768706798553467\n",
      "Iteration: 260, loss 0.014898039400577545\n",
      "Iteration: 261, loss 0.01406997349113226\n",
      "Iteration: 262, loss 0.013298267498612404\n",
      "Iteration: 263, loss 0.012568983249366283\n",
      "Iteration: 264, loss 0.01188431866466999\n",
      "Iteration: 265, loss 0.01122964732348919\n",
      "Iteration: 266, loss 0.010617049410939217\n",
      "Iteration: 267, loss 0.010043379850685596\n",
      "Iteration: 268, loss 0.009492401033639908\n",
      "Iteration: 269, loss 0.00897474866360426\n",
      "Iteration: 270, loss 0.00848957896232605\n",
      "Iteration: 271, loss 0.008027318865060806\n",
      "Iteration: 272, loss 0.007595446892082691\n",
      "Iteration: 273, loss 0.00718553364276886\n",
      "Iteration: 274, loss 0.006800581235438585\n",
      "Iteration: 275, loss 0.006437056697905064\n",
      "Iteration: 276, loss 0.006092357449233532\n",
      "Iteration: 277, loss 0.005762708373367786\n",
      "Iteration: 278, loss 0.005460054613649845\n",
      "Iteration: 279, loss 0.005179232452064753\n",
      "Iteration: 280, loss 0.004900667816400528\n",
      "Iteration: 281, loss 0.004641913343220949\n",
      "Iteration: 282, loss 0.004401719197630882\n",
      "Iteration: 283, loss 0.004170536994934082\n",
      "Iteration: 284, loss 0.003957639914005995\n",
      "Iteration: 285, loss 0.0037541347555816174\n",
      "Iteration: 286, loss 0.0035568438470363617\n",
      "Iteration: 287, loss 0.0033755977638065815\n",
      "Iteration: 288, loss 0.0032044255640357733\n",
      "Iteration: 289, loss 0.003040533047169447\n",
      "Iteration: 290, loss 0.0028878790326416492\n",
      "Iteration: 291, loss 0.0027389656752347946\n",
      "Iteration: 292, loss 0.002601730404421687\n",
      "Iteration: 293, loss 0.002472651656717062\n",
      "Iteration: 294, loss 0.002350554335862398\n",
      "Iteration: 295, loss 0.00223721656948328\n",
      "Iteration: 296, loss 0.0021264594979584217\n",
      "Iteration: 297, loss 0.002024472923949361\n",
      "Iteration: 298, loss 0.0019276499515399337\n",
      "Iteration: 299, loss 0.0018359135137870908\n",
      "Iteration: 300, loss 0.001748297130689025\n",
      "Iteration: 301, loss 0.0016637536464259028\n",
      "Iteration: 302, loss 0.0015850239433348179\n",
      "Iteration: 303, loss 0.0015120027819648385\n",
      "Iteration: 304, loss 0.0014423939865082502\n",
      "Iteration: 305, loss 0.0013754457468166947\n",
      "Iteration: 306, loss 0.0013117245398461819\n",
      "Iteration: 307, loss 0.0012525058118626475\n",
      "Iteration: 308, loss 0.001196481054648757\n",
      "Iteration: 309, loss 0.0011422405950725079\n",
      "Iteration: 310, loss 0.0010913170408457518\n",
      "Iteration: 311, loss 0.0010432952549308538\n",
      "Iteration: 312, loss 0.0009989391546696424\n",
      "Iteration: 313, loss 0.0009547246736474335\n",
      "Iteration: 314, loss 0.0009139490430243313\n",
      "Iteration: 315, loss 0.0008747563697397709\n",
      "Iteration: 316, loss 0.000838080421090126\n",
      "Iteration: 317, loss 0.0008015474304556847\n",
      "Iteration: 318, loss 0.0007689200574532151\n",
      "Iteration: 319, loss 0.0007380757015198469\n",
      "Iteration: 320, loss 0.0007084721000865102\n",
      "Iteration: 321, loss 0.000680523575283587\n",
      "Iteration: 322, loss 0.0006534463609568775\n",
      "Iteration: 323, loss 0.0006266055279411376\n",
      "Iteration: 324, loss 0.0006014225073158741\n",
      "Iteration: 325, loss 0.0005790064460597932\n",
      "Iteration: 326, loss 0.000557512859813869\n",
      "Iteration: 327, loss 0.0005360060604289174\n",
      "Iteration: 328, loss 0.0005156683619134128\n",
      "Iteration: 329, loss 0.0004964465042576194\n",
      "Iteration: 330, loss 0.0004765890771523118\n",
      "Iteration: 331, loss 0.0004595126665662974\n",
      "Iteration: 332, loss 0.0004436354502104223\n",
      "Iteration: 333, loss 0.00042820090311579406\n",
      "Iteration: 334, loss 0.0004139253869652748\n",
      "Iteration: 335, loss 0.0003984951472375542\n",
      "Iteration: 336, loss 0.0003843685844913125\n",
      "Iteration: 337, loss 0.00037177331978455186\n",
      "Iteration: 338, loss 0.00035802455386146903\n",
      "Iteration: 339, loss 0.00034626544220373034\n",
      "Iteration: 340, loss 0.0003336240188218653\n",
      "Iteration: 341, loss 0.0003236419288441539\n",
      "Iteration: 342, loss 0.00031293678330257535\n",
      "Iteration: 343, loss 0.00030214438447728753\n",
      "Iteration: 344, loss 0.00029202731093391776\n",
      "Iteration: 345, loss 0.0002825409756042063\n",
      "Iteration: 346, loss 0.0002735387533903122\n",
      "Iteration: 347, loss 0.0002642054751049727\n",
      "Iteration: 348, loss 0.0002564863534644246\n",
      "Iteration: 349, loss 0.0002492991625331342\n",
      "Iteration: 350, loss 0.00024159584427252412\n",
      "Iteration: 351, loss 0.00023433504975400865\n",
      "Iteration: 352, loss 0.00022791486117057502\n",
      "Iteration: 353, loss 0.0002201902389060706\n",
      "Iteration: 354, loss 0.00021429758635349572\n",
      "Iteration: 355, loss 0.0002080501290038228\n",
      "Iteration: 356, loss 0.00020179458078928292\n",
      "Iteration: 357, loss 0.00019672377675306052\n",
      "Iteration: 358, loss 0.00019090366549789906\n",
      "Iteration: 359, loss 0.00018603025819174945\n",
      "Iteration: 360, loss 0.000180411443579942\n",
      "Iteration: 361, loss 0.0001755325502017513\n",
      "Iteration: 362, loss 0.00017086602747440338\n",
      "Iteration: 363, loss 0.00016611101455055177\n",
      "Iteration: 364, loss 0.00016151959425769746\n",
      "Iteration: 365, loss 0.000157463044160977\n",
      "Iteration: 366, loss 0.0001527441927464679\n",
      "Iteration: 367, loss 0.00014864877448417246\n",
      "Iteration: 368, loss 0.00014450433081947267\n",
      "Iteration: 369, loss 0.00014083915448281914\n",
      "Iteration: 370, loss 0.00013793000834994018\n",
      "Iteration: 371, loss 0.00013462758215609938\n",
      "Iteration: 372, loss 0.00013119986397214234\n",
      "Iteration: 373, loss 0.00012722198152914643\n",
      "Iteration: 374, loss 0.00012448873894754797\n",
      "Iteration: 375, loss 0.00012097701983293518\n",
      "Iteration: 376, loss 0.00011837675992865115\n",
      "Iteration: 377, loss 0.00011549324699444696\n",
      "Iteration: 378, loss 0.0001130121381720528\n",
      "Iteration: 379, loss 0.00011025334242731333\n",
      "Iteration: 380, loss 0.00010805181227624416\n",
      "Iteration: 381, loss 0.00010504475358175114\n",
      "Iteration: 382, loss 0.00010250208288198337\n",
      "Iteration: 383, loss 0.00010064942762255669\n",
      "Iteration: 384, loss 9.82428464340046e-05\n",
      "Iteration: 385, loss 9.597503958502784e-05\n",
      "Iteration: 386, loss 9.383766155224293e-05\n",
      "Iteration: 387, loss 9.173569560516626e-05\n",
      "Iteration: 388, loss 9.019736899062991e-05\n",
      "Iteration: 389, loss 8.794252062216401e-05\n",
      "Iteration: 390, loss 8.578720007790253e-05\n",
      "Iteration: 391, loss 8.394598989980295e-05\n",
      "Iteration: 392, loss 8.260025060735643e-05\n",
      "Iteration: 393, loss 8.065818110480905e-05\n",
      "Iteration: 394, loss 7.90478297858499e-05\n",
      "Iteration: 395, loss 7.75403605075553e-05\n",
      "Iteration: 396, loss 7.576560892630368e-05\n",
      "Iteration: 397, loss 7.434474537149072e-05\n",
      "Iteration: 398, loss 7.263221777975559e-05\n",
      "Iteration: 399, loss 7.137853390304372e-05\n",
      "Iteration: 400, loss 7.009765977272764e-05\n",
      "Iteration: 401, loss 6.878504791529849e-05\n",
      "Iteration: 402, loss 6.730313180014491e-05\n",
      "Iteration: 403, loss 6.592775753233582e-05\n",
      "Iteration: 404, loss 6.481448508566245e-05\n",
      "Iteration: 405, loss 6.36008262517862e-05\n",
      "Iteration: 406, loss 6.278601358644664e-05\n",
      "Iteration: 407, loss 6.138458411442116e-05\n",
      "Iteration: 408, loss 6.0438105720095336e-05\n",
      "Iteration: 409, loss 5.917941962252371e-05\n",
      "Iteration: 410, loss 5.818787758471444e-05\n",
      "Iteration: 411, loss 5.7388610002817586e-05\n",
      "Iteration: 412, loss 5.610739390249364e-05\n",
      "Iteration: 413, loss 5.4914318752707914e-05\n",
      "Iteration: 414, loss 5.4231088142842054e-05\n",
      "Iteration: 415, loss 5.345564568415284e-05\n",
      "Iteration: 416, loss 5.229637827142142e-05\n",
      "Iteration: 417, loss 5.135529499966651e-05\n",
      "Iteration: 418, loss 5.0636350351851434e-05\n",
      "Iteration: 419, loss 4.9734509957488626e-05\n",
      "Iteration: 420, loss 4.896851896774024e-05\n",
      "Iteration: 421, loss 4.816517321160063e-05\n",
      "Iteration: 422, loss 4.743882891489193e-05\n",
      "Iteration: 423, loss 4.697853364632465e-05\n",
      "Iteration: 424, loss 4.619765968527645e-05\n",
      "Iteration: 425, loss 4.515887485467829e-05\n",
      "Iteration: 426, loss 4.4586642616195604e-05\n",
      "Iteration: 427, loss 4.367323708720505e-05\n",
      "Iteration: 428, loss 4.319298022892326e-05\n",
      "Iteration: 429, loss 4.2611831304384395e-05\n",
      "Iteration: 430, loss 4.1876501200022176e-05\n",
      "Iteration: 431, loss 4.1303963371319696e-05\n",
      "Iteration: 432, loss 4.079376958543435e-05\n",
      "Iteration: 433, loss 4.015400554635562e-05\n",
      "Iteration: 434, loss 3.956314321840182e-05\n",
      "Iteration: 435, loss 3.8978439988568425e-05\n",
      "Iteration: 436, loss 3.842600199277513e-05\n",
      "Iteration: 437, loss 3.7947196688037366e-05\n",
      "Iteration: 438, loss 3.768670285353437e-05\n",
      "Iteration: 439, loss 3.717249273904599e-05\n",
      "Iteration: 440, loss 3.6531851947074756e-05\n",
      "Iteration: 441, loss 3.6210411053616554e-05\n",
      "Iteration: 442, loss 3.562496567610651e-05\n",
      "Iteration: 443, loss 3.516846118145622e-05\n",
      "Iteration: 444, loss 3.4634773328434676e-05\n",
      "Iteration: 445, loss 3.4064676583511755e-05\n",
      "Iteration: 446, loss 3.385351737961173e-05\n",
      "Iteration: 447, loss 3.3294658351223916e-05\n",
      "Iteration: 448, loss 3.294363705208525e-05\n",
      "Iteration: 449, loss 3.2522966648684815e-05\n",
      "Iteration: 450, loss 3.207581539754756e-05\n",
      "Iteration: 451, loss 3.151948476443067e-05\n",
      "Iteration: 452, loss 3.0979164876043797e-05\n",
      "Iteration: 453, loss 3.06750152958557e-05\n",
      "Iteration: 454, loss 3.0328046705108136e-05\n",
      "Iteration: 455, loss 2.999965363414958e-05\n",
      "Iteration: 456, loss 2.95770114462357e-05\n",
      "Iteration: 457, loss 2.939766454801429e-05\n",
      "Iteration: 458, loss 2.905914152506739e-05\n",
      "Iteration: 459, loss 2.873824632843025e-05\n",
      "Iteration: 460, loss 2.8391585146891885e-05\n",
      "Iteration: 461, loss 2.7998215955449268e-05\n",
      "Iteration: 462, loss 2.763720476650633e-05\n",
      "Iteration: 463, loss 2.725699778238777e-05\n",
      "Iteration: 464, loss 2.704680446186103e-05\n",
      "Iteration: 465, loss 2.6812667783815414e-05\n",
      "Iteration: 466, loss 2.6493771656532772e-05\n",
      "Iteration: 467, loss 2.621399107738398e-05\n",
      "Iteration: 468, loss 2.602302447485272e-05\n",
      "Iteration: 469, loss 2.5615707272663713e-05\n",
      "Iteration: 470, loss 2.5374876713613048e-05\n",
      "Iteration: 471, loss 2.5129895220743492e-05\n",
      "Iteration: 472, loss 2.4783084882074036e-05\n",
      "Iteration: 473, loss 2.4607712475699373e-05\n",
      "Iteration: 474, loss 2.4456467144773342e-05\n",
      "Iteration: 475, loss 2.4081098672468215e-05\n",
      "Iteration: 476, loss 2.3825848984415643e-05\n",
      "Iteration: 477, loss 2.3554417566629127e-05\n",
      "Iteration: 478, loss 2.320971543667838e-05\n",
      "Iteration: 479, loss 2.2989752324065194e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 480, loss 2.272463643748779e-05\n",
      "Iteration: 481, loss 2.251465048175305e-05\n",
      "Iteration: 482, loss 2.2362833988154307e-05\n",
      "Iteration: 483, loss 2.203566145908553e-05\n",
      "Iteration: 484, loss 2.184337063226849e-05\n",
      "Iteration: 485, loss 2.168760693166405e-05\n",
      "Iteration: 486, loss 2.145038524759002e-05\n",
      "Iteration: 487, loss 2.1298281353665516e-05\n",
      "Iteration: 488, loss 2.1144713173271157e-05\n",
      "Iteration: 489, loss 2.0923022020724602e-05\n",
      "Iteration: 490, loss 2.0737546947202645e-05\n",
      "Iteration: 491, loss 2.0456540369195864e-05\n",
      "Iteration: 492, loss 2.0280522221582942e-05\n",
      "Iteration: 493, loss 2.0098324966966175e-05\n",
      "Iteration: 494, loss 1.9868366507580504e-05\n",
      "Iteration: 495, loss 1.9653172785183415e-05\n",
      "Iteration: 496, loss 1.9660314137581736e-05\n",
      "Iteration: 497, loss 1.944763062056154e-05\n",
      "Iteration: 498, loss 1.9109635104541667e-05\n",
      "Iteration: 499, loss 1.8999880921910517e-05\n"
     ]
    }
   ],
   "source": [
    "for i in range(500):\n",
    "    relu = MyReLu.apply\n",
    "    # forward pass\n",
    "    pred = relu(X.mm(W1)).mm(W2)\n",
    "    # compute/print loss\n",
    "    loss = (Y - pred).pow(2).sum()\n",
    "    print('Iteration: {}, loss {}'.format(i, loss.item()))\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    # updates \n",
    "    with torch.no_grad(): #temporarily set requires_grad flags to False\n",
    "        W1 -= gamma * W1.grad\n",
    "        W2 -= gamma * W2.grad\n",
    "        W1.grad.zero_()\n",
    "        W2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using static computational graphs via TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(N, D1))\n",
    "Y = tf.placeholder(tf.float32, shape=(N, D2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.Variable(tf.random_normal((D1, H)))\n",
    "W2 = tf.Variable(tf.random_normal((H, D2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = tf.matmul(X, W1)\n",
    "phi = tf.maximum(Z, tf.zeros(1))\n",
    "pred = tf.matmul(phi, W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_sum((Y - pred) ** 2.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the gradients\n",
    "grad_l1, grad_l2 = tf.gradients(loss, [W1, W2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define update rules\n",
    "W1_new = W1.assign(W1 - gamma * grad_l1)\n",
    "W2_new = W2.assign(W2 - gamma * grad_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, loss 31025410.0\n",
      "Iteration: 1, loss 29603118.0\n",
      "Iteration: 2, loss 33416816.0\n",
      "Iteration: 3, loss 38626540.0\n",
      "Iteration: 4, loss 38374956.0\n",
      "Iteration: 5, loss 29441512.0\n",
      "Iteration: 6, loss 17120376.0\n",
      "Iteration: 7, loss 8508116.0\n",
      "Iteration: 8, loss 4687544.0\n",
      "Iteration: 9, loss 2653951.75\n",
      "Iteration: 10, loss 1641374.75\n",
      "Iteration: 11, loss 1127887.75\n",
      "Iteration: 12, loss 849153.0\n",
      "Iteration: 13, loss 681059.0\n",
      "Iteration: 14, loss 568065.6875\n",
      "Iteration: 15, loss 485065.3125\n",
      "Iteration: 16, loss 420107.625\n",
      "Iteration: 17, loss 367178.375\n",
      "Iteration: 18, loss 322931.625\n",
      "Iteration: 19, loss 285438.25\n",
      "Iteration: 20, loss 253331.125\n",
      "Iteration: 21, loss 225630.359375\n",
      "Iteration: 22, loss 201605.75\n",
      "Iteration: 23, loss 180672.59375\n",
      "Iteration: 24, loss 162334.53125\n",
      "Iteration: 25, loss 146223.03125\n",
      "Iteration: 26, loss 132008.421875\n",
      "Iteration: 27, loss 119431.484375\n",
      "Iteration: 28, loss 108281.265625\n",
      "Iteration: 29, loss 98368.640625\n",
      "Iteration: 30, loss 89542.5625\n",
      "Iteration: 31, loss 81649.625\n",
      "Iteration: 32, loss 74573.4140625\n",
      "Iteration: 33, loss 68218.8828125\n",
      "Iteration: 34, loss 62517.17578125\n",
      "Iteration: 35, loss 57374.546875\n",
      "Iteration: 36, loss 52727.5703125\n",
      "Iteration: 37, loss 48518.734375\n",
      "Iteration: 38, loss 44708.078125\n",
      "Iteration: 39, loss 41245.5703125\n",
      "Iteration: 40, loss 38094.0546875\n",
      "Iteration: 41, loss 35224.71875\n",
      "Iteration: 42, loss 32604.08984375\n",
      "Iteration: 43, loss 30211.5625\n",
      "Iteration: 44, loss 28020.3828125\n",
      "Iteration: 45, loss 26012.40234375\n",
      "Iteration: 46, loss 24169.80078125\n",
      "Iteration: 47, loss 22476.12109375\n",
      "Iteration: 48, loss 20917.322265625\n",
      "Iteration: 49, loss 19484.830078125\n",
      "Iteration: 50, loss 18164.859375\n",
      "Iteration: 51, loss 16946.3125\n",
      "Iteration: 52, loss 15819.8515625\n",
      "Iteration: 53, loss 14777.623046875\n",
      "Iteration: 54, loss 13813.05078125\n",
      "Iteration: 55, loss 12919.423828125\n",
      "Iteration: 56, loss 12090.3271484375\n",
      "Iteration: 57, loss 11320.6396484375\n",
      "Iteration: 58, loss 10605.556640625\n",
      "Iteration: 59, loss 9941.1494140625\n",
      "Iteration: 60, loss 9323.0419921875\n",
      "Iteration: 61, loss 8747.544921875\n",
      "Iteration: 62, loss 8211.181640625\n",
      "Iteration: 63, loss 7710.794921875\n",
      "Iteration: 64, loss 7244.24609375\n",
      "Iteration: 65, loss 6809.13671875\n",
      "Iteration: 66, loss 6402.896484375\n",
      "Iteration: 67, loss 6023.3720703125\n",
      "Iteration: 68, loss 5668.3271484375\n",
      "Iteration: 69, loss 5336.08544921875\n",
      "Iteration: 70, loss 5025.1474609375\n",
      "Iteration: 71, loss 4733.8818359375\n",
      "Iteration: 72, loss 4460.9423828125\n",
      "Iteration: 73, loss 4205.1357421875\n",
      "Iteration: 74, loss 3965.135009765625\n",
      "Iteration: 75, loss 3740.0078125\n",
      "Iteration: 76, loss 3528.7080078125\n",
      "Iteration: 77, loss 3330.20263671875\n",
      "Iteration: 78, loss 3143.75244140625\n",
      "Iteration: 79, loss 2968.53759765625\n",
      "Iteration: 80, loss 2803.79833984375\n",
      "Iteration: 81, loss 2648.8388671875\n",
      "Iteration: 82, loss 2503.07763671875\n",
      "Iteration: 83, loss 2365.9248046875\n",
      "Iteration: 84, loss 2236.76708984375\n",
      "Iteration: 85, loss 2115.105712890625\n",
      "Iteration: 86, loss 2000.509765625\n",
      "Iteration: 87, loss 1892.53759765625\n",
      "Iteration: 88, loss 1790.7626953125\n",
      "Iteration: 89, loss 1694.767822265625\n",
      "Iteration: 90, loss 1604.2152099609375\n",
      "Iteration: 91, loss 1518.7921142578125\n",
      "Iteration: 92, loss 1438.2176513671875\n",
      "Iteration: 93, loss 1362.12158203125\n",
      "Iteration: 94, loss 1290.2962646484375\n",
      "Iteration: 95, loss 1222.4781494140625\n",
      "Iteration: 96, loss 1158.431640625\n",
      "Iteration: 97, loss 1097.896728515625\n",
      "Iteration: 98, loss 1040.6611328125\n",
      "Iteration: 99, loss 986.5789184570312\n",
      "Iteration: 100, loss 935.5123901367188\n",
      "Iteration: 101, loss 887.2089233398438\n",
      "Iteration: 102, loss 841.5089111328125\n",
      "Iteration: 103, loss 798.3275146484375\n",
      "Iteration: 104, loss 757.475830078125\n",
      "Iteration: 105, loss 718.798583984375\n",
      "Iteration: 106, loss 682.1962890625\n",
      "Iteration: 107, loss 647.5390014648438\n",
      "Iteration: 108, loss 614.7239990234375\n",
      "Iteration: 109, loss 583.6285400390625\n",
      "Iteration: 110, loss 554.170166015625\n",
      "Iteration: 111, loss 526.267578125\n",
      "Iteration: 112, loss 499.84320068359375\n",
      "Iteration: 113, loss 474.791015625\n",
      "Iteration: 114, loss 451.02813720703125\n",
      "Iteration: 115, loss 428.50506591796875\n",
      "Iteration: 116, loss 407.1516418457031\n",
      "Iteration: 117, loss 386.90447998046875\n",
      "Iteration: 118, loss 367.70013427734375\n",
      "Iteration: 119, loss 349.478271484375\n",
      "Iteration: 120, loss 332.2012939453125\n",
      "Iteration: 121, loss 315.8013916015625\n",
      "Iteration: 122, loss 300.2421875\n",
      "Iteration: 123, loss 285.4696044921875\n",
      "Iteration: 124, loss 271.4578857421875\n",
      "Iteration: 125, loss 258.15679931640625\n",
      "Iteration: 126, loss 245.52413940429688\n",
      "Iteration: 127, loss 233.52806091308594\n",
      "Iteration: 128, loss 222.14085388183594\n",
      "Iteration: 129, loss 211.32705688476562\n",
      "Iteration: 130, loss 201.0511016845703\n",
      "Iteration: 131, loss 191.2919464111328\n",
      "Iteration: 132, loss 182.02642822265625\n",
      "Iteration: 133, loss 173.22158813476562\n",
      "Iteration: 134, loss 164.85406494140625\n",
      "Iteration: 135, loss 156.8975372314453\n",
      "Iteration: 136, loss 149.337646484375\n",
      "Iteration: 137, loss 142.15615844726562\n",
      "Iteration: 138, loss 135.3271484375\n",
      "Iteration: 139, loss 128.8380584716797\n",
      "Iteration: 140, loss 122.66677856445312\n",
      "Iteration: 141, loss 116.80023193359375\n",
      "Iteration: 142, loss 111.22180938720703\n",
      "Iteration: 143, loss 105.91532135009766\n",
      "Iteration: 144, loss 100.8703842163086\n",
      "Iteration: 145, loss 96.07254028320312\n",
      "Iteration: 146, loss 91.50543212890625\n",
      "Iteration: 147, loss 87.16315460205078\n",
      "Iteration: 148, loss 83.03253936767578\n",
      "Iteration: 149, loss 79.10272216796875\n",
      "Iteration: 150, loss 75.36325073242188\n",
      "Iteration: 151, loss 71.80488586425781\n",
      "Iteration: 152, loss 68.4195556640625\n",
      "Iteration: 153, loss 65.19718933105469\n",
      "Iteration: 154, loss 62.13113784790039\n",
      "Iteration: 155, loss 59.211185455322266\n",
      "Iteration: 156, loss 56.431392669677734\n",
      "Iteration: 157, loss 53.78727722167969\n",
      "Iteration: 158, loss 51.26802062988281\n",
      "Iteration: 159, loss 48.87029266357422\n",
      "Iteration: 160, loss 46.58562469482422\n",
      "Iteration: 161, loss 44.413368225097656\n",
      "Iteration: 162, loss 42.342681884765625\n",
      "Iteration: 163, loss 40.37034606933594\n",
      "Iteration: 164, loss 38.492984771728516\n",
      "Iteration: 165, loss 36.70388412475586\n",
      "Iteration: 166, loss 35.000152587890625\n",
      "Iteration: 167, loss 33.377620697021484\n",
      "Iteration: 168, loss 31.83242416381836\n",
      "Iteration: 169, loss 30.359516143798828\n",
      "Iteration: 170, loss 28.956207275390625\n",
      "Iteration: 171, loss 27.619037628173828\n",
      "Iteration: 172, loss 26.3447322845459\n",
      "Iteration: 173, loss 25.130800247192383\n",
      "Iteration: 174, loss 23.974111557006836\n",
      "Iteration: 175, loss 22.871273040771484\n",
      "Iteration: 176, loss 21.820446014404297\n",
      "Iteration: 177, loss 20.820205688476562\n",
      "Iteration: 178, loss 19.865158081054688\n",
      "Iteration: 179, loss 18.955223083496094\n",
      "Iteration: 180, loss 18.087831497192383\n",
      "Iteration: 181, loss 17.26062774658203\n",
      "Iteration: 182, loss 16.472515106201172\n",
      "Iteration: 183, loss 15.721031188964844\n",
      "Iteration: 184, loss 15.004274368286133\n",
      "Iteration: 185, loss 14.320806503295898\n",
      "Iteration: 186, loss 13.669163703918457\n",
      "Iteration: 187, loss 13.047647476196289\n",
      "Iteration: 188, loss 12.454580307006836\n",
      "Iteration: 189, loss 11.889266967773438\n",
      "Iteration: 190, loss 11.350354194641113\n",
      "Iteration: 191, loss 10.836484909057617\n",
      "Iteration: 192, loss 10.346396446228027\n",
      "Iteration: 193, loss 9.87792682647705\n",
      "Iteration: 194, loss 9.431700706481934\n",
      "Iteration: 195, loss 9.00584888458252\n",
      "Iteration: 196, loss 8.599868774414062\n",
      "Iteration: 197, loss 8.21221923828125\n",
      "Iteration: 198, loss 7.842883110046387\n",
      "Iteration: 199, loss 7.4900031089782715\n",
      "Iteration: 200, loss 7.15322732925415\n",
      "Iteration: 201, loss 6.8321614265441895\n",
      "Iteration: 202, loss 6.52567195892334\n",
      "Iteration: 203, loss 6.233329772949219\n",
      "Iteration: 204, loss 5.953869819641113\n",
      "Iteration: 205, loss 5.687394142150879\n",
      "Iteration: 206, loss 5.433165550231934\n",
      "Iteration: 207, loss 5.190718173980713\n",
      "Iteration: 208, loss 4.958808898925781\n",
      "Iteration: 209, loss 4.737738609313965\n",
      "Iteration: 210, loss 4.526550769805908\n",
      "Iteration: 211, loss 4.3249006271362305\n",
      "Iteration: 212, loss 4.13253116607666\n",
      "Iteration: 213, loss 3.9488396644592285\n",
      "Iteration: 214, loss 3.77337646484375\n",
      "Iteration: 215, loss 3.605790138244629\n",
      "Iteration: 216, loss 3.4457976818084717\n",
      "Iteration: 217, loss 3.2931759357452393\n",
      "Iteration: 218, loss 3.1476387977600098\n",
      "Iteration: 219, loss 3.008410930633545\n",
      "Iteration: 220, loss 2.8752970695495605\n",
      "Iteration: 221, loss 2.748326301574707\n",
      "Iteration: 222, loss 2.6270532608032227\n",
      "Iteration: 223, loss 2.511038303375244\n",
      "Iteration: 224, loss 2.400566816329956\n",
      "Iteration: 225, loss 2.2947654724121094\n",
      "Iteration: 226, loss 2.1938283443450928\n",
      "Iteration: 227, loss 2.097393035888672\n",
      "Iteration: 228, loss 2.0050768852233887\n",
      "Iteration: 229, loss 1.917135238647461\n",
      "Iteration: 230, loss 1.8331905603408813\n",
      "Iteration: 231, loss 1.7528460025787354\n",
      "Iteration: 232, loss 1.676053524017334\n",
      "Iteration: 233, loss 1.60256028175354\n",
      "Iteration: 234, loss 1.5326966047286987\n",
      "Iteration: 235, loss 1.4656165838241577\n",
      "Iteration: 236, loss 1.401665449142456\n",
      "Iteration: 237, loss 1.340468406677246\n",
      "Iteration: 238, loss 1.2819994688034058\n",
      "Iteration: 239, loss 1.2262566089630127\n",
      "Iteration: 240, loss 1.1728848218917847\n",
      "Iteration: 241, loss 1.1217985153198242\n",
      "Iteration: 242, loss 1.0730476379394531\n",
      "Iteration: 243, loss 1.0263471603393555\n",
      "Iteration: 244, loss 0.9818898439407349\n",
      "Iteration: 245, loss 0.9392946362495422\n",
      "Iteration: 246, loss 0.8986151814460754\n",
      "Iteration: 247, loss 0.859779953956604\n",
      "Iteration: 248, loss 0.8225637674331665\n",
      "Iteration: 249, loss 0.787034273147583\n",
      "Iteration: 250, loss 0.7529237270355225\n",
      "Iteration: 251, loss 0.7204816341400146\n",
      "Iteration: 252, loss 0.6894344091415405\n",
      "Iteration: 253, loss 0.6597374677658081\n",
      "Iteration: 254, loss 0.6312551498413086\n",
      "Iteration: 255, loss 0.6040723323822021\n",
      "Iteration: 256, loss 0.5781411528587341\n",
      "Iteration: 257, loss 0.5532180070877075\n",
      "Iteration: 258, loss 0.5294804573059082\n",
      "Iteration: 259, loss 0.5067352056503296\n",
      "Iteration: 260, loss 0.4850790202617645\n",
      "Iteration: 261, loss 0.4643099308013916\n",
      "Iteration: 262, loss 0.4443642199039459\n",
      "Iteration: 263, loss 0.42533087730407715\n",
      "Iteration: 264, loss 0.40727221965789795\n",
      "Iteration: 265, loss 0.389833927154541\n",
      "Iteration: 266, loss 0.3731248378753662\n",
      "Iteration: 267, loss 0.35724586248397827\n",
      "Iteration: 268, loss 0.3420065641403198\n",
      "Iteration: 269, loss 0.3274714946746826\n",
      "Iteration: 270, loss 0.3134748339653015\n",
      "Iteration: 271, loss 0.30013832449913025\n",
      "Iteration: 272, loss 0.287392258644104\n",
      "Iteration: 273, loss 0.2752029299736023\n",
      "Iteration: 274, loss 0.2635452151298523\n",
      "Iteration: 275, loss 0.25235071778297424\n",
      "Iteration: 276, loss 0.24163784086704254\n",
      "Iteration: 277, loss 0.23138168454170227\n",
      "Iteration: 278, loss 0.22161096334457397\n",
      "Iteration: 279, loss 0.21223469078540802\n",
      "Iteration: 280, loss 0.20326769351959229\n",
      "Iteration: 281, loss 0.19465532898902893\n",
      "Iteration: 282, loss 0.18644925951957703\n",
      "Iteration: 283, loss 0.1785622239112854\n",
      "Iteration: 284, loss 0.1710902452468872\n",
      "Iteration: 285, loss 0.16387498378753662\n",
      "Iteration: 286, loss 0.15698020160198212\n",
      "Iteration: 287, loss 0.15038655698299408\n",
      "Iteration: 288, loss 0.14405030012130737\n",
      "Iteration: 289, loss 0.137971431016922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 290, loss 0.1321861296892166\n",
      "Iteration: 291, loss 0.1266450583934784\n",
      "Iteration: 292, loss 0.12134191393852234\n",
      "Iteration: 293, loss 0.11625947058200836\n",
      "Iteration: 294, loss 0.11140196025371552\n",
      "Iteration: 295, loss 0.10674446821212769\n",
      "Iteration: 296, loss 0.10227891057729721\n",
      "Iteration: 297, loss 0.09799011051654816\n",
      "Iteration: 298, loss 0.09389135241508484\n",
      "Iteration: 299, loss 0.08999259024858475\n",
      "Iteration: 300, loss 0.0862569734454155\n",
      "Iteration: 301, loss 0.08264756202697754\n",
      "Iteration: 302, loss 0.07923401892185211\n",
      "Iteration: 303, loss 0.07592567801475525\n",
      "Iteration: 304, loss 0.07278864085674286\n",
      "Iteration: 305, loss 0.06975817680358887\n",
      "Iteration: 306, loss 0.06686851382255554\n",
      "Iteration: 307, loss 0.06408379226922989\n",
      "Iteration: 308, loss 0.06143498793244362\n",
      "Iteration: 309, loss 0.05890418961644173\n",
      "Iteration: 310, loss 0.05645819753408432\n",
      "Iteration: 311, loss 0.05414268374443054\n",
      "Iteration: 312, loss 0.05191560089588165\n",
      "Iteration: 313, loss 0.04977763071656227\n",
      "Iteration: 314, loss 0.04772619158029556\n",
      "Iteration: 315, loss 0.04575616121292114\n",
      "Iteration: 316, loss 0.04387248307466507\n",
      "Iteration: 317, loss 0.04206471145153046\n",
      "Iteration: 318, loss 0.040335237979888916\n",
      "Iteration: 319, loss 0.038665175437927246\n",
      "Iteration: 320, loss 0.03708899766206741\n",
      "Iteration: 321, loss 0.035586319863796234\n",
      "Iteration: 322, loss 0.03411511331796646\n",
      "Iteration: 323, loss 0.03271263465285301\n",
      "Iteration: 324, loss 0.0313798189163208\n",
      "Iteration: 325, loss 0.03009854070842266\n",
      "Iteration: 326, loss 0.02888811007142067\n",
      "Iteration: 327, loss 0.027704846113920212\n",
      "Iteration: 328, loss 0.02657904103398323\n",
      "Iteration: 329, loss 0.02551305666565895\n",
      "Iteration: 330, loss 0.024465931579470634\n",
      "Iteration: 331, loss 0.02348707988858223\n",
      "Iteration: 332, loss 0.022536423057317734\n",
      "Iteration: 333, loss 0.021628446877002716\n",
      "Iteration: 334, loss 0.020757194608449936\n",
      "Iteration: 335, loss 0.019928250461816788\n",
      "Iteration: 336, loss 0.01912284642457962\n",
      "Iteration: 337, loss 0.01836698316037655\n",
      "Iteration: 338, loss 0.017624057829380035\n",
      "Iteration: 339, loss 0.01691538095474243\n",
      "Iteration: 340, loss 0.016238220036029816\n",
      "Iteration: 341, loss 0.015585397370159626\n",
      "Iteration: 342, loss 0.01497078500688076\n",
      "Iteration: 343, loss 0.014374767430126667\n",
      "Iteration: 344, loss 0.013805762864649296\n",
      "Iteration: 345, loss 0.013253491371870041\n",
      "Iteration: 346, loss 0.01272940170019865\n",
      "Iteration: 347, loss 0.012230340391397476\n",
      "Iteration: 348, loss 0.011740786023437977\n",
      "Iteration: 349, loss 0.011279833503067493\n",
      "Iteration: 350, loss 0.010834649205207825\n",
      "Iteration: 351, loss 0.01040763407945633\n",
      "Iteration: 352, loss 0.009996913373470306\n",
      "Iteration: 353, loss 0.009603949263691902\n",
      "Iteration: 354, loss 0.009233120828866959\n",
      "Iteration: 355, loss 0.008877436630427837\n",
      "Iteration: 356, loss 0.008533750660717487\n",
      "Iteration: 357, loss 0.008205239661037922\n",
      "Iteration: 358, loss 0.007885869592428207\n",
      "Iteration: 359, loss 0.007579971570521593\n",
      "Iteration: 360, loss 0.00729142501950264\n",
      "Iteration: 361, loss 0.007011867128312588\n",
      "Iteration: 362, loss 0.00674601923674345\n",
      "Iteration: 363, loss 0.006489747669547796\n",
      "Iteration: 364, loss 0.006241918541491032\n",
      "Iteration: 365, loss 0.006006824783980846\n",
      "Iteration: 366, loss 0.005778172984719276\n",
      "Iteration: 367, loss 0.005563132464885712\n",
      "Iteration: 368, loss 0.005353453103452921\n",
      "Iteration: 369, loss 0.005156121216714382\n",
      "Iteration: 370, loss 0.004966788925230503\n",
      "Iteration: 371, loss 0.004785575903952122\n",
      "Iteration: 372, loss 0.004604206886142492\n",
      "Iteration: 373, loss 0.004435637034475803\n",
      "Iteration: 374, loss 0.00427249725908041\n",
      "Iteration: 375, loss 0.004119053483009338\n",
      "Iteration: 376, loss 0.003967992961406708\n",
      "Iteration: 377, loss 0.0038245278410613537\n",
      "Iteration: 378, loss 0.0036887547466903925\n",
      "Iteration: 379, loss 0.00355293950997293\n",
      "Iteration: 380, loss 0.0034280731342732906\n",
      "Iteration: 381, loss 0.003308608429506421\n",
      "Iteration: 382, loss 0.0031908208038657904\n",
      "Iteration: 383, loss 0.003075913991779089\n",
      "Iteration: 384, loss 0.002969364169985056\n",
      "Iteration: 385, loss 0.0028686930891126394\n",
      "Iteration: 386, loss 0.0027698413468897343\n",
      "Iteration: 387, loss 0.0026712012477219105\n",
      "Iteration: 388, loss 0.002580811735242605\n",
      "Iteration: 389, loss 0.002493451116606593\n",
      "Iteration: 390, loss 0.002409367123618722\n",
      "Iteration: 391, loss 0.0023259539157152176\n",
      "Iteration: 392, loss 0.0022485703229904175\n",
      "Iteration: 393, loss 0.0021720128133893013\n",
      "Iteration: 394, loss 0.002100232522934675\n",
      "Iteration: 395, loss 0.0020313416607677937\n",
      "Iteration: 396, loss 0.0019642747938632965\n",
      "Iteration: 397, loss 0.0018995332065969706\n",
      "Iteration: 398, loss 0.0018380467081442475\n",
      "Iteration: 399, loss 0.0017792729195207357\n",
      "Iteration: 400, loss 0.0017213777173310518\n",
      "Iteration: 401, loss 0.001667250762693584\n",
      "Iteration: 402, loss 0.0016129899304360151\n",
      "Iteration: 403, loss 0.001563073368743062\n",
      "Iteration: 404, loss 0.0015138315502554178\n",
      "Iteration: 405, loss 0.0014681201428174973\n",
      "Iteration: 406, loss 0.0014221493620425463\n",
      "Iteration: 407, loss 0.0013801218010485172\n",
      "Iteration: 408, loss 0.0013370485976338387\n",
      "Iteration: 409, loss 0.001298733172006905\n",
      "Iteration: 410, loss 0.0012589592952281237\n",
      "Iteration: 411, loss 0.001221825834363699\n",
      "Iteration: 412, loss 0.0011863430263474584\n",
      "Iteration: 413, loss 0.0011519353138282895\n",
      "Iteration: 414, loss 0.0011187730124220252\n",
      "Iteration: 415, loss 0.0010840629693120718\n",
      "Iteration: 416, loss 0.0010530983563512564\n",
      "Iteration: 417, loss 0.001023377408273518\n",
      "Iteration: 418, loss 0.0009944033809006214\n",
      "Iteration: 419, loss 0.0009675548644736409\n",
      "Iteration: 420, loss 0.0009413096704520285\n",
      "Iteration: 421, loss 0.0009144075447693467\n",
      "Iteration: 422, loss 0.0008893685881048441\n",
      "Iteration: 423, loss 0.0008645831258036196\n",
      "Iteration: 424, loss 0.0008393090683966875\n",
      "Iteration: 425, loss 0.0008176072151400149\n",
      "Iteration: 426, loss 0.0007951730513013899\n",
      "Iteration: 427, loss 0.0007737179403193295\n",
      "Iteration: 428, loss 0.0007536286138929427\n",
      "Iteration: 429, loss 0.0007324031321331859\n",
      "Iteration: 430, loss 0.0007143543916754425\n",
      "Iteration: 431, loss 0.0006955850403755903\n",
      "Iteration: 432, loss 0.0006776361260563135\n",
      "Iteration: 433, loss 0.0006593557773157954\n",
      "Iteration: 434, loss 0.000643438717816025\n",
      "Iteration: 435, loss 0.0006271753227338195\n",
      "Iteration: 436, loss 0.0006095137796364725\n",
      "Iteration: 437, loss 0.0005955261876806617\n",
      "Iteration: 438, loss 0.0005803534295409918\n",
      "Iteration: 439, loss 0.0005668886005878448\n",
      "Iteration: 440, loss 0.0005523428553715348\n",
      "Iteration: 441, loss 0.0005384555552154779\n",
      "Iteration: 442, loss 0.000525409821420908\n",
      "Iteration: 443, loss 0.0005129599012434483\n",
      "Iteration: 444, loss 0.0005005963030271232\n",
      "Iteration: 445, loss 0.00048827059799805284\n",
      "Iteration: 446, loss 0.0004766296478919685\n",
      "Iteration: 447, loss 0.0004652347997762263\n",
      "Iteration: 448, loss 0.0004549058503471315\n",
      "Iteration: 449, loss 0.000444095756392926\n",
      "Iteration: 450, loss 0.00043356380774639547\n",
      "Iteration: 451, loss 0.0004234418156556785\n",
      "Iteration: 452, loss 0.00041385291842743754\n",
      "Iteration: 453, loss 0.00040580047061666846\n",
      "Iteration: 454, loss 0.0003964955685660243\n",
      "Iteration: 455, loss 0.00038734733243472874\n",
      "Iteration: 456, loss 0.00037968024844303727\n",
      "Iteration: 457, loss 0.0003703865804709494\n",
      "Iteration: 458, loss 0.0003627188561949879\n",
      "Iteration: 459, loss 0.0003543477214407176\n",
      "Iteration: 460, loss 0.0003468099457677454\n",
      "Iteration: 461, loss 0.0003395267412997782\n",
      "Iteration: 462, loss 0.0003327008744236082\n",
      "Iteration: 463, loss 0.0003252435417380184\n",
      "Iteration: 464, loss 0.00031808356288820505\n",
      "Iteration: 465, loss 0.0003113553684670478\n",
      "Iteration: 466, loss 0.00030470924684777856\n",
      "Iteration: 467, loss 0.0002983947051689029\n",
      "Iteration: 468, loss 0.00029181380523368716\n",
      "Iteration: 469, loss 0.00028537464095279574\n",
      "Iteration: 470, loss 0.00028038094751536846\n",
      "Iteration: 471, loss 0.0002743439399637282\n",
      "Iteration: 472, loss 0.00026990071637555957\n",
      "Iteration: 473, loss 0.00026387927937321365\n",
      "Iteration: 474, loss 0.0002587593626230955\n",
      "Iteration: 475, loss 0.00025407929206267\n",
      "Iteration: 476, loss 0.0002496154047548771\n",
      "Iteration: 477, loss 0.0002438694646116346\n",
      "Iteration: 478, loss 0.00023980799596756697\n",
      "Iteration: 479, loss 0.00023565783340018243\n",
      "Iteration: 480, loss 0.0002309778647031635\n",
      "Iteration: 481, loss 0.00022636752692051232\n",
      "Iteration: 482, loss 0.00022199827071744949\n",
      "Iteration: 483, loss 0.00021788572485093027\n",
      "Iteration: 484, loss 0.0002145480248145759\n",
      "Iteration: 485, loss 0.00021058796846773475\n",
      "Iteration: 486, loss 0.0002063970168819651\n",
      "Iteration: 487, loss 0.0002022640546783805\n",
      "Iteration: 488, loss 0.0001986608112929389\n",
      "Iteration: 489, loss 0.0001948187709785998\n",
      "Iteration: 490, loss 0.0001909290294861421\n",
      "Iteration: 491, loss 0.00018743786495178938\n",
      "Iteration: 492, loss 0.00018458589329384267\n",
      "Iteration: 493, loss 0.00018141420150641352\n",
      "Iteration: 494, loss 0.00017838363419286907\n",
      "Iteration: 495, loss 0.00017555721569806337\n",
      "Iteration: 496, loss 0.00017249562370125204\n",
      "Iteration: 497, loss 0.00016973399033304304\n",
      "Iteration: 498, loss 0.00016688273171894252\n",
      "Iteration: 499, loss 0.00016358858556486666\n"
     ]
    }
   ],
   "source": [
    "# run the computational graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    X_np = np.random.randn(N, D1)\n",
    "    Y_np = np.random.randn(N, D2)\n",
    "    for i in range(500):\n",
    "        loss_value, _, _ = sess.run([loss, W1_new, W2_new],\n",
    "                             feed_dict={X: X_np, Y: Y_np})\n",
    "        print('Iteration: {}, loss {}'.format(i, loss_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using nn package from PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(N, D1)\n",
    "Y = torch.randn(N, D2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the neural network\n",
    "model = torch.nn.Sequential(torch.nn.Linear(D1, H),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.Linear(H, D2))\n",
    "loss_function = torch.nn.MSELoss(reduction='sum')\n",
    "gamma = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, loss 714.2217407226562\n",
      "Iteration: 1, loss 661.5736083984375\n",
      "Iteration: 2, loss 615.9337158203125\n",
      "Iteration: 3, loss 576.0433959960938\n",
      "Iteration: 4, loss 540.3504638671875\n",
      "Iteration: 5, loss 508.2291259765625\n",
      "Iteration: 6, loss 478.7377624511719\n",
      "Iteration: 7, loss 451.75323486328125\n",
      "Iteration: 8, loss 427.0273742675781\n",
      "Iteration: 9, loss 404.0857849121094\n",
      "Iteration: 10, loss 382.3077697753906\n",
      "Iteration: 11, loss 361.88568115234375\n",
      "Iteration: 12, loss 342.5139465332031\n",
      "Iteration: 13, loss 324.1628723144531\n",
      "Iteration: 14, loss 306.7515563964844\n",
      "Iteration: 15, loss 290.18792724609375\n",
      "Iteration: 16, loss 274.5047302246094\n",
      "Iteration: 17, loss 259.52587890625\n",
      "Iteration: 18, loss 245.2332763671875\n",
      "Iteration: 19, loss 231.57936096191406\n",
      "Iteration: 20, loss 218.574462890625\n",
      "Iteration: 21, loss 206.22418212890625\n",
      "Iteration: 22, loss 194.47549438476562\n",
      "Iteration: 23, loss 183.2827606201172\n",
      "Iteration: 24, loss 172.63693237304688\n",
      "Iteration: 25, loss 162.53482055664062\n",
      "Iteration: 26, loss 152.9459991455078\n",
      "Iteration: 27, loss 143.87721252441406\n",
      "Iteration: 28, loss 135.3007354736328\n",
      "Iteration: 29, loss 127.21208953857422\n",
      "Iteration: 30, loss 119.51367950439453\n",
      "Iteration: 31, loss 112.25802612304688\n",
      "Iteration: 32, loss 105.4205551147461\n",
      "Iteration: 33, loss 98.98966979980469\n",
      "Iteration: 34, loss 92.93529510498047\n",
      "Iteration: 35, loss 87.25299835205078\n",
      "Iteration: 36, loss 81.91515350341797\n",
      "Iteration: 37, loss 76.88388061523438\n",
      "Iteration: 38, loss 72.15841674804688\n",
      "Iteration: 39, loss 67.70702362060547\n",
      "Iteration: 40, loss 63.548423767089844\n",
      "Iteration: 41, loss 59.650596618652344\n",
      "Iteration: 42, loss 55.9999885559082\n",
      "Iteration: 43, loss 52.588069915771484\n",
      "Iteration: 44, loss 49.391910552978516\n",
      "Iteration: 45, loss 46.397369384765625\n",
      "Iteration: 46, loss 43.60319519042969\n",
      "Iteration: 47, loss 40.98363494873047\n",
      "Iteration: 48, loss 38.53343200683594\n",
      "Iteration: 49, loss 36.23121643066406\n",
      "Iteration: 50, loss 34.076324462890625\n",
      "Iteration: 51, loss 32.062557220458984\n",
      "Iteration: 52, loss 30.175016403198242\n",
      "Iteration: 53, loss 28.409997940063477\n",
      "Iteration: 54, loss 26.760906219482422\n",
      "Iteration: 55, loss 25.212173461914062\n",
      "Iteration: 56, loss 23.76433753967285\n",
      "Iteration: 57, loss 22.407001495361328\n",
      "Iteration: 58, loss 21.134376525878906\n",
      "Iteration: 59, loss 19.940263748168945\n",
      "Iteration: 60, loss 18.818729400634766\n",
      "Iteration: 61, loss 17.768922805786133\n",
      "Iteration: 62, loss 16.78508949279785\n",
      "Iteration: 63, loss 15.860677719116211\n",
      "Iteration: 64, loss 14.991887092590332\n",
      "Iteration: 65, loss 14.177350044250488\n",
      "Iteration: 66, loss 13.411005973815918\n",
      "Iteration: 67, loss 12.688220977783203\n",
      "Iteration: 68, loss 12.00858211517334\n",
      "Iteration: 69, loss 11.36973762512207\n",
      "Iteration: 70, loss 10.769157409667969\n",
      "Iteration: 71, loss 10.204039573669434\n",
      "Iteration: 72, loss 9.67123794555664\n",
      "Iteration: 73, loss 9.169719696044922\n",
      "Iteration: 74, loss 8.696861267089844\n",
      "Iteration: 75, loss 8.25108528137207\n",
      "Iteration: 76, loss 7.830402851104736\n",
      "Iteration: 77, loss 7.432682514190674\n",
      "Iteration: 78, loss 7.057202339172363\n",
      "Iteration: 79, loss 6.702846050262451\n",
      "Iteration: 80, loss 6.368093013763428\n",
      "Iteration: 81, loss 6.0516862869262695\n",
      "Iteration: 82, loss 5.752678394317627\n",
      "Iteration: 83, loss 5.469809055328369\n",
      "Iteration: 84, loss 5.202335834503174\n",
      "Iteration: 85, loss 4.9493794441223145\n",
      "Iteration: 86, loss 4.710068225860596\n",
      "Iteration: 87, loss 4.484004974365234\n",
      "Iteration: 88, loss 4.269312381744385\n",
      "Iteration: 89, loss 4.065879821777344\n",
      "Iteration: 90, loss 3.873363971710205\n",
      "Iteration: 91, loss 3.691033363342285\n",
      "Iteration: 92, loss 3.517902374267578\n",
      "Iteration: 93, loss 3.353630542755127\n",
      "Iteration: 94, loss 3.198568105697632\n",
      "Iteration: 95, loss 3.05159068107605\n",
      "Iteration: 96, loss 2.911923885345459\n",
      "Iteration: 97, loss 2.7792110443115234\n",
      "Iteration: 98, loss 2.6534948348999023\n",
      "Iteration: 99, loss 2.533829927444458\n",
      "Iteration: 100, loss 2.4201533794403076\n",
      "Iteration: 101, loss 2.3120176792144775\n",
      "Iteration: 102, loss 2.2088794708251953\n",
      "Iteration: 103, loss 2.1108479499816895\n",
      "Iteration: 104, loss 2.0175118446350098\n",
      "Iteration: 105, loss 1.9286012649536133\n",
      "Iteration: 106, loss 1.843692421913147\n",
      "Iteration: 107, loss 1.7628018856048584\n",
      "Iteration: 108, loss 1.685782790184021\n",
      "Iteration: 109, loss 1.6124613285064697\n",
      "Iteration: 110, loss 1.5425083637237549\n",
      "Iteration: 111, loss 1.4757531881332397\n",
      "Iteration: 112, loss 1.4120852947235107\n",
      "Iteration: 113, loss 1.3512870073318481\n",
      "Iteration: 114, loss 1.2933145761489868\n",
      "Iteration: 115, loss 1.2378789186477661\n",
      "Iteration: 116, loss 1.185138463973999\n",
      "Iteration: 117, loss 1.1348612308502197\n",
      "Iteration: 118, loss 1.0867701768875122\n",
      "Iteration: 119, loss 1.0407707691192627\n",
      "Iteration: 120, loss 0.9968587756156921\n",
      "Iteration: 121, loss 0.9549400210380554\n",
      "Iteration: 122, loss 0.9149460792541504\n",
      "Iteration: 123, loss 0.8767488598823547\n",
      "Iteration: 124, loss 0.8402050733566284\n",
      "Iteration: 125, loss 0.8050755858421326\n",
      "Iteration: 126, loss 0.7715240120887756\n",
      "Iteration: 127, loss 0.7394089698791504\n",
      "Iteration: 128, loss 0.7087283730506897\n",
      "Iteration: 129, loss 0.6793555617332458\n",
      "Iteration: 130, loss 0.6513084769248962\n",
      "Iteration: 131, loss 0.6245432496070862\n",
      "Iteration: 132, loss 0.5989577174186707\n",
      "Iteration: 133, loss 0.5745158195495605\n",
      "Iteration: 134, loss 0.5511067509651184\n",
      "Iteration: 135, loss 0.5286757349967957\n",
      "Iteration: 136, loss 0.5071997046470642\n",
      "Iteration: 137, loss 0.48667532205581665\n",
      "Iteration: 138, loss 0.4669985771179199\n",
      "Iteration: 139, loss 0.4481419324874878\n",
      "Iteration: 140, loss 0.43009310960769653\n",
      "Iteration: 141, loss 0.4128505289554596\n",
      "Iteration: 142, loss 0.3963128626346588\n",
      "Iteration: 143, loss 0.380470871925354\n",
      "Iteration: 144, loss 0.3652784526348114\n",
      "Iteration: 145, loss 0.35071441531181335\n",
      "Iteration: 146, loss 0.3367590010166168\n",
      "Iteration: 147, loss 0.3233890235424042\n",
      "Iteration: 148, loss 0.31057190895080566\n",
      "Iteration: 149, loss 0.29831209778785706\n",
      "Iteration: 150, loss 0.2865423560142517\n",
      "Iteration: 151, loss 0.27524980902671814\n",
      "Iteration: 152, loss 0.2644136846065521\n",
      "Iteration: 153, loss 0.25402310490608215\n",
      "Iteration: 154, loss 0.24404901266098022\n",
      "Iteration: 155, loss 0.23449504375457764\n",
      "Iteration: 156, loss 0.22531846165657043\n",
      "Iteration: 157, loss 0.21651175618171692\n",
      "Iteration: 158, loss 0.20806942880153656\n",
      "Iteration: 159, loss 0.19996602833271027\n",
      "Iteration: 160, loss 0.19218900799751282\n",
      "Iteration: 161, loss 0.18472664058208466\n",
      "Iteration: 162, loss 0.17756327986717224\n",
      "Iteration: 163, loss 0.17069284617900848\n",
      "Iteration: 164, loss 0.16409626603126526\n",
      "Iteration: 165, loss 0.15775874257087708\n",
      "Iteration: 166, loss 0.1516823023557663\n",
      "Iteration: 167, loss 0.14584580063819885\n",
      "Iteration: 168, loss 0.14023835957050323\n",
      "Iteration: 169, loss 0.13485214114189148\n",
      "Iteration: 170, loss 0.12968359887599945\n",
      "Iteration: 171, loss 0.1247117891907692\n",
      "Iteration: 172, loss 0.11993607133626938\n",
      "Iteration: 173, loss 0.11535558104515076\n",
      "Iteration: 174, loss 0.11094988882541656\n",
      "Iteration: 175, loss 0.10671883821487427\n",
      "Iteration: 176, loss 0.10265910625457764\n",
      "Iteration: 177, loss 0.09875891357660294\n",
      "Iteration: 178, loss 0.09500763565301895\n",
      "Iteration: 179, loss 0.09140457212924957\n",
      "Iteration: 180, loss 0.0879455953836441\n",
      "Iteration: 181, loss 0.08462049812078476\n",
      "Iteration: 182, loss 0.08142218738794327\n",
      "Iteration: 183, loss 0.07834753394126892\n",
      "Iteration: 184, loss 0.07539229094982147\n",
      "Iteration: 185, loss 0.07255411893129349\n",
      "Iteration: 186, loss 0.06982220709323883\n",
      "Iteration: 187, loss 0.06719646602869034\n",
      "Iteration: 188, loss 0.06467601656913757\n",
      "Iteration: 189, loss 0.06225168704986572\n",
      "Iteration: 190, loss 0.05992243066430092\n",
      "Iteration: 191, loss 0.05767996236681938\n",
      "Iteration: 192, loss 0.05552290752530098\n",
      "Iteration: 193, loss 0.05345037579536438\n",
      "Iteration: 194, loss 0.05145686864852905\n",
      "Iteration: 195, loss 0.04953806474804878\n",
      "Iteration: 196, loss 0.04769328981637955\n",
      "Iteration: 197, loss 0.04592094570398331\n",
      "Iteration: 198, loss 0.04421598091721535\n",
      "Iteration: 199, loss 0.04257826879620552\n",
      "Iteration: 200, loss 0.041001446545124054\n",
      "Iteration: 201, loss 0.03948456794023514\n",
      "Iteration: 202, loss 0.038024287670850754\n",
      "Iteration: 203, loss 0.036620307713747025\n",
      "Iteration: 204, loss 0.035269446671009064\n",
      "Iteration: 205, loss 0.033969320356845856\n",
      "Iteration: 206, loss 0.0327182412147522\n",
      "Iteration: 207, loss 0.03151332959532738\n",
      "Iteration: 208, loss 0.03035496175289154\n",
      "Iteration: 209, loss 0.029241202399134636\n",
      "Iteration: 210, loss 0.028170255944132805\n",
      "Iteration: 211, loss 0.027139803394675255\n",
      "Iteration: 212, loss 0.026148280128836632\n",
      "Iteration: 213, loss 0.02519330009818077\n",
      "Iteration: 214, loss 0.024273643270134926\n",
      "Iteration: 215, loss 0.02338823862373829\n",
      "Iteration: 216, loss 0.02253659814596176\n",
      "Iteration: 217, loss 0.0217171348631382\n",
      "Iteration: 218, loss 0.020928528159856796\n",
      "Iteration: 219, loss 0.02016867697238922\n",
      "Iteration: 220, loss 0.01943640038371086\n",
      "Iteration: 221, loss 0.018731409683823586\n",
      "Iteration: 222, loss 0.018052909523248672\n",
      "Iteration: 223, loss 0.01739996112883091\n",
      "Iteration: 224, loss 0.01677057519555092\n",
      "Iteration: 225, loss 0.0161641463637352\n",
      "Iteration: 226, loss 0.015580486506223679\n",
      "Iteration: 227, loss 0.015019063837826252\n",
      "Iteration: 228, loss 0.014477838762104511\n",
      "Iteration: 229, loss 0.013956614769995213\n",
      "Iteration: 230, loss 0.013454643078148365\n",
      "Iteration: 231, loss 0.012971178628504276\n",
      "Iteration: 232, loss 0.012505431659519672\n",
      "Iteration: 233, loss 0.012056680396199226\n",
      "Iteration: 234, loss 0.011624612845480442\n",
      "Iteration: 235, loss 0.011208347976207733\n",
      "Iteration: 236, loss 0.010807118378579617\n",
      "Iteration: 237, loss 0.010420450009405613\n",
      "Iteration: 238, loss 0.010048100724816322\n",
      "Iteration: 239, loss 0.009689214639365673\n",
      "Iteration: 240, loss 0.009343575686216354\n",
      "Iteration: 241, loss 0.009010842069983482\n",
      "Iteration: 242, loss 0.008689644746482372\n",
      "Iteration: 243, loss 0.008380024693906307\n",
      "Iteration: 244, loss 0.008082162588834763\n",
      "Iteration: 245, loss 0.007795312441885471\n",
      "Iteration: 246, loss 0.007518591824918985\n",
      "Iteration: 247, loss 0.007251710630953312\n",
      "Iteration: 248, loss 0.006994444876909256\n",
      "Iteration: 249, loss 0.006746622733771801\n",
      "Iteration: 250, loss 0.006507746875286102\n",
      "Iteration: 251, loss 0.006277916021645069\n",
      "Iteration: 252, loss 0.0060561117716133595\n",
      "Iteration: 253, loss 0.0058420575223863125\n",
      "Iteration: 254, loss 0.005635831505060196\n",
      "Iteration: 255, loss 0.005436748266220093\n",
      "Iteration: 256, loss 0.005244947038590908\n",
      "Iteration: 257, loss 0.005060074385255575\n",
      "Iteration: 258, loss 0.004881681874394417\n",
      "Iteration: 259, loss 0.004709695931524038\n",
      "Iteration: 260, loss 0.004543909803032875\n",
      "Iteration: 261, loss 0.004384193569421768\n",
      "Iteration: 262, loss 0.004230190999805927\n",
      "Iteration: 263, loss 0.004081639461219311\n",
      "Iteration: 264, loss 0.0039385221898555756\n",
      "Iteration: 265, loss 0.003800487145781517\n",
      "Iteration: 266, loss 0.0036673119757324457\n",
      "Iteration: 267, loss 0.003539008554071188\n",
      "Iteration: 268, loss 0.0034153058659285307\n",
      "Iteration: 269, loss 0.0032958730589598417\n",
      "Iteration: 270, loss 0.0031807306222617626\n",
      "Iteration: 271, loss 0.0030696457251906395\n",
      "Iteration: 272, loss 0.002962562721222639\n",
      "Iteration: 273, loss 0.002859319094568491\n",
      "Iteration: 274, loss 0.002759716473519802\n",
      "Iteration: 275, loss 0.002663630060851574\n",
      "Iteration: 276, loss 0.002570933196693659\n",
      "Iteration: 277, loss 0.0024816012009978294\n",
      "Iteration: 278, loss 0.002395405201241374\n",
      "Iteration: 279, loss 0.0023122329730540514\n",
      "Iteration: 280, loss 0.0022319722920656204\n",
      "Iteration: 281, loss 0.0021545649506151676\n",
      "Iteration: 282, loss 0.0020799441263079643\n",
      "Iteration: 283, loss 0.0020079941023141146\n",
      "Iteration: 284, loss 0.0019385416526347399\n",
      "Iteration: 285, loss 0.0018715189071372151\n",
      "Iteration: 286, loss 0.001806784886866808\n",
      "Iteration: 287, loss 0.0017443817341700196\n",
      "Iteration: 288, loss 0.0016841614851728082\n",
      "Iteration: 289, loss 0.0016261531272903085\n",
      "Iteration: 290, loss 0.001570042222738266\n",
      "Iteration: 291, loss 0.0015159749891608953\n",
      "Iteration: 292, loss 0.0014638055581599474\n",
      "Iteration: 293, loss 0.001413492253050208\n",
      "Iteration: 294, loss 0.001364910276606679\n",
      "Iteration: 295, loss 0.0013181081740185618\n",
      "Iteration: 296, loss 0.001272823428735137\n",
      "Iteration: 297, loss 0.0012291757157072425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 298, loss 0.001186998444609344\n",
      "Iteration: 299, loss 0.001146293361671269\n",
      "Iteration: 300, loss 0.001107013551518321\n",
      "Iteration: 301, loss 0.0010691044153645635\n",
      "Iteration: 302, loss 0.0010325420880690217\n",
      "Iteration: 303, loss 0.0009972580010071397\n",
      "Iteration: 304, loss 0.000963167636655271\n",
      "Iteration: 305, loss 0.0009302593534812331\n",
      "Iteration: 306, loss 0.0008985032327473164\n",
      "Iteration: 307, loss 0.0008678358281031251\n",
      "Iteration: 308, loss 0.0008382269297726452\n",
      "Iteration: 309, loss 0.0008096633828245103\n",
      "Iteration: 310, loss 0.0007820841856300831\n",
      "Iteration: 311, loss 0.0007554820040240884\n",
      "Iteration: 312, loss 0.0007297936244867742\n",
      "Iteration: 313, loss 0.0007049776031635702\n",
      "Iteration: 314, loss 0.000681019970215857\n",
      "Iteration: 315, loss 0.0006579100154340267\n",
      "Iteration: 316, loss 0.0006355892401188612\n",
      "Iteration: 317, loss 0.0006140602054074407\n",
      "Iteration: 318, loss 0.0005932322819717228\n",
      "Iteration: 319, loss 0.000573124154470861\n",
      "Iteration: 320, loss 0.0005537171382457018\n",
      "Iteration: 321, loss 0.0005349881830625236\n",
      "Iteration: 322, loss 0.0005168905481696129\n",
      "Iteration: 323, loss 0.0004994236514903605\n",
      "Iteration: 324, loss 0.0004825363866984844\n",
      "Iteration: 325, loss 0.00046624470269307494\n",
      "Iteration: 326, loss 0.0004505084652919322\n",
      "Iteration: 327, loss 0.00043530535185709596\n",
      "Iteration: 328, loss 0.00042064394801855087\n",
      "Iteration: 329, loss 0.0004064879030920565\n",
      "Iteration: 330, loss 0.0003928113146685064\n",
      "Iteration: 331, loss 0.0003795818192884326\n",
      "Iteration: 332, loss 0.00036680593620985746\n",
      "Iteration: 333, loss 0.00035447184927761555\n",
      "Iteration: 334, loss 0.00034255944774486125\n",
      "Iteration: 335, loss 0.00033105190959759057\n",
      "Iteration: 336, loss 0.00031994582968764007\n",
      "Iteration: 337, loss 0.00030921006691642106\n",
      "Iteration: 338, loss 0.0002988418273162097\n",
      "Iteration: 339, loss 0.00028884902712889016\n",
      "Iteration: 340, loss 0.0002791847800835967\n",
      "Iteration: 341, loss 0.00026983744464814663\n",
      "Iteration: 342, loss 0.00026080390671268106\n",
      "Iteration: 343, loss 0.0002520860289223492\n",
      "Iteration: 344, loss 0.00024365809804294258\n",
      "Iteration: 345, loss 0.00023551593767479062\n",
      "Iteration: 346, loss 0.000227652708417736\n",
      "Iteration: 347, loss 0.00022005864593666047\n",
      "Iteration: 348, loss 0.00021271761215757579\n",
      "Iteration: 349, loss 0.00020562554709613323\n",
      "Iteration: 350, loss 0.00019877581507898867\n",
      "Iteration: 351, loss 0.00019214667554479092\n",
      "Iteration: 352, loss 0.00018575071590021253\n",
      "Iteration: 353, loss 0.00017957366071641445\n",
      "Iteration: 354, loss 0.00017359787307213992\n",
      "Iteration: 355, loss 0.0001678280532360077\n",
      "Iteration: 356, loss 0.0001622490381123498\n",
      "Iteration: 357, loss 0.00015686327242292464\n",
      "Iteration: 358, loss 0.0001516565534984693\n",
      "Iteration: 359, loss 0.00014663072943221778\n",
      "Iteration: 360, loss 0.000141761644044891\n",
      "Iteration: 361, loss 0.00013706569734495133\n",
      "Iteration: 362, loss 0.00013252836652100086\n",
      "Iteration: 363, loss 0.00012813974171876907\n",
      "Iteration: 364, loss 0.00012389595212880522\n",
      "Iteration: 365, loss 0.00011980141425738111\n",
      "Iteration: 366, loss 0.00011584409367060289\n",
      "Iteration: 367, loss 0.00011202139285160229\n",
      "Iteration: 368, loss 0.00010831861436599866\n",
      "Iteration: 369, loss 0.00010474018199602142\n",
      "Iteration: 370, loss 0.00010128832218470052\n",
      "Iteration: 371, loss 9.79481337708421e-05\n",
      "Iteration: 372, loss 9.471386874793097e-05\n",
      "Iteration: 373, loss 9.15915152290836e-05\n",
      "Iteration: 374, loss 8.85761110112071e-05\n",
      "Iteration: 375, loss 8.566890755901113e-05\n",
      "Iteration: 376, loss 8.284672367153689e-05\n",
      "Iteration: 377, loss 8.012020407477394e-05\n",
      "Iteration: 378, loss 7.748346979497e-05\n",
      "Iteration: 379, loss 7.49360624467954e-05\n",
      "Iteration: 380, loss 7.247012399602681e-05\n",
      "Iteration: 381, loss 7.009318505879492e-05\n",
      "Iteration: 382, loss 6.780114199500531e-05\n",
      "Iteration: 383, loss 6.557202141266316e-05\n",
      "Iteration: 384, loss 6.342084088828415e-05\n",
      "Iteration: 385, loss 6.134426075732335e-05\n",
      "Iteration: 386, loss 5.9335263358661905e-05\n",
      "Iteration: 387, loss 5.739343396271579e-05\n",
      "Iteration: 388, loss 5.551196954911575e-05\n",
      "Iteration: 389, loss 5.370023427531123e-05\n",
      "Iteration: 390, loss 5.19412278663367e-05\n",
      "Iteration: 391, loss 5.024392157793045e-05\n",
      "Iteration: 392, loss 4.8604048060951754e-05\n",
      "Iteration: 393, loss 4.701598663814366e-05\n",
      "Iteration: 394, loss 4.547990101855248e-05\n",
      "Iteration: 395, loss 4.399484532768838e-05\n",
      "Iteration: 396, loss 4.2562045564409345e-05\n",
      "Iteration: 397, loss 4.117457501706667e-05\n",
      "Iteration: 398, loss 3.983063652412966e-05\n",
      "Iteration: 399, loss 3.8535494240932167e-05\n",
      "Iteration: 400, loss 3.727895091287792e-05\n",
      "Iteration: 401, loss 3.606752216001041e-05\n",
      "Iteration: 402, loss 3.489252776489593e-05\n",
      "Iteration: 403, loss 3.375881351530552e-05\n",
      "Iteration: 404, loss 3.265880513936281e-05\n",
      "Iteration: 405, loss 3.159946572850458e-05\n",
      "Iteration: 406, loss 3.057379217352718e-05\n",
      "Iteration: 407, loss 2.957840115414001e-05\n",
      "Iteration: 408, loss 2.8620921511901543e-05\n",
      "Iteration: 409, loss 2.7695159587892704e-05\n",
      "Iteration: 410, loss 2.6795043595484458e-05\n",
      "Iteration: 411, loss 2.5927718525053933e-05\n",
      "Iteration: 412, loss 2.5087718313443474e-05\n",
      "Iteration: 413, loss 2.4276074327644892e-05\n",
      "Iteration: 414, loss 2.3489614250138402e-05\n",
      "Iteration: 415, loss 2.2729040210833773e-05\n",
      "Iteration: 416, loss 2.199313894379884e-05\n",
      "Iteration: 417, loss 2.1283820387907326e-05\n",
      "Iteration: 418, loss 2.0596993635990657e-05\n",
      "Iteration: 419, loss 1.9929962945752777e-05\n",
      "Iteration: 420, loss 1.928895289893262e-05\n",
      "Iteration: 421, loss 1.8664944946067408e-05\n",
      "Iteration: 422, loss 1.8062492017634213e-05\n",
      "Iteration: 423, loss 1.747888745740056e-05\n",
      "Iteration: 424, loss 1.6915155356400646e-05\n",
      "Iteration: 425, loss 1.6370226148865186e-05\n",
      "Iteration: 426, loss 1.5843985238461755e-05\n",
      "Iteration: 427, loss 1.5333094779634848e-05\n",
      "Iteration: 428, loss 1.4839626601315103e-05\n",
      "Iteration: 429, loss 1.436255934095243e-05\n",
      "Iteration: 430, loss 1.3898538782086689e-05\n",
      "Iteration: 431, loss 1.3451676750264596e-05\n",
      "Iteration: 432, loss 1.3020538972341456e-05\n",
      "Iteration: 433, loss 1.2601321941474453e-05\n",
      "Iteration: 434, loss 1.2197069736430421e-05\n",
      "Iteration: 435, loss 1.1805803296738304e-05\n",
      "Iteration: 436, loss 1.142731161962729e-05\n",
      "Iteration: 437, loss 1.1059250937250908e-05\n",
      "Iteration: 438, loss 1.0706014109018724e-05\n",
      "Iteration: 439, loss 1.0362653483753093e-05\n",
      "Iteration: 440, loss 1.0029841178038623e-05\n",
      "Iteration: 441, loss 9.709321602713317e-06\n",
      "Iteration: 442, loss 9.398012480232865e-06\n",
      "Iteration: 443, loss 9.09718255570624e-06\n",
      "Iteration: 444, loss 8.805332981864922e-06\n",
      "Iteration: 445, loss 8.523800715920515e-06\n",
      "Iteration: 446, loss 8.251612598542124e-06\n",
      "Iteration: 447, loss 7.987726348801516e-06\n",
      "Iteration: 448, loss 7.731719961157069e-06\n",
      "Iteration: 449, loss 7.4854492595477495e-06\n",
      "Iteration: 450, loss 7.246220775414258e-06\n",
      "Iteration: 451, loss 7.015804840193596e-06\n",
      "Iteration: 452, loss 6.790555744373705e-06\n",
      "Iteration: 453, loss 6.574359304067912e-06\n",
      "Iteration: 454, loss 6.364721684803953e-06\n",
      "Iteration: 455, loss 6.161949386296328e-06\n",
      "Iteration: 456, loss 5.9664685068128165e-06\n",
      "Iteration: 457, loss 5.776331363449572e-06\n",
      "Iteration: 458, loss 5.591315130004659e-06\n",
      "Iteration: 459, loss 5.414192855823785e-06\n",
      "Iteration: 460, loss 5.241817689238815e-06\n",
      "Iteration: 461, loss 5.075507033325266e-06\n",
      "Iteration: 462, loss 4.914092187391361e-06\n",
      "Iteration: 463, loss 4.757152964884881e-06\n",
      "Iteration: 464, loss 4.6063823901931755e-06\n",
      "Iteration: 465, loss 4.460231593839126e-06\n",
      "Iteration: 466, loss 4.318743322073715e-06\n",
      "Iteration: 467, loss 4.182114025752526e-06\n",
      "Iteration: 468, loss 4.048234131914796e-06\n",
      "Iteration: 469, loss 3.920344170182943e-06\n",
      "Iteration: 470, loss 3.7967065509292297e-06\n",
      "Iteration: 471, loss 3.6761578030564124e-06\n",
      "Iteration: 472, loss 3.559439619493787e-06\n",
      "Iteration: 473, loss 3.4466497709217947e-06\n",
      "Iteration: 474, loss 3.3371627523592906e-06\n",
      "Iteration: 475, loss 3.2317884688382037e-06\n",
      "Iteration: 476, loss 3.129159722448094e-06\n",
      "Iteration: 477, loss 3.030970447071013e-06\n",
      "Iteration: 478, loss 2.9348639145609923e-06\n",
      "Iteration: 479, loss 2.8420604394341353e-06\n",
      "Iteration: 480, loss 2.752361979219131e-06\n",
      "Iteration: 481, loss 2.665161673576222e-06\n",
      "Iteration: 482, loss 2.5802780783124035e-06\n",
      "Iteration: 483, loss 2.499383981557912e-06\n",
      "Iteration: 484, loss 2.4205523914133664e-06\n",
      "Iteration: 485, loss 2.344386530239717e-06\n",
      "Iteration: 486, loss 2.269883452754584e-06\n",
      "Iteration: 487, loss 2.198670017605764e-06\n",
      "Iteration: 488, loss 2.1288217340043047e-06\n",
      "Iteration: 489, loss 2.0618815597117646e-06\n",
      "Iteration: 490, loss 1.996859737118939e-06\n",
      "Iteration: 491, loss 1.9340084236318944e-06\n",
      "Iteration: 492, loss 1.8731403770289035e-06\n",
      "Iteration: 493, loss 1.8140158317692112e-06\n",
      "Iteration: 494, loss 1.7571246644365601e-06\n",
      "Iteration: 495, loss 1.7016686797433067e-06\n",
      "Iteration: 496, loss 1.6480911426697276e-06\n",
      "Iteration: 497, loss 1.596232664269337e-06\n",
      "Iteration: 498, loss 1.546538328511815e-06\n",
      "Iteration: 499, loss 1.4978487570260768e-06\n"
     ]
    }
   ],
   "source": [
    "for i in range(500):\n",
    "    pred = model(X)\n",
    "    loss = loss_function(pred, Y)\n",
    "    print('Iteration: {}, loss {}'.format(i, loss.item()))\n",
    "    model.zero_grad() # zero the gradients before the computation\n",
    "    loss.backward() # compute gradients for all learnable parameters in the model\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= gamma * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the optim package from PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-initialize nn\n",
    "model = torch.nn.Sequential(torch.nn.Linear(D1, H),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.Linear(H, D2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=gamma) # define the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, loss 730.1437377929688\n",
      "Iteration: 1, loss 711.9232177734375\n",
      "Iteration: 2, loss 694.1687622070312\n",
      "Iteration: 3, loss 676.9307861328125\n",
      "Iteration: 4, loss 660.1820068359375\n",
      "Iteration: 5, loss 643.9859619140625\n",
      "Iteration: 6, loss 628.30517578125\n",
      "Iteration: 7, loss 613.11083984375\n",
      "Iteration: 8, loss 598.380126953125\n",
      "Iteration: 9, loss 584.1588134765625\n",
      "Iteration: 10, loss 570.2985229492188\n",
      "Iteration: 11, loss 556.867431640625\n",
      "Iteration: 12, loss 543.822998046875\n",
      "Iteration: 13, loss 531.1405029296875\n",
      "Iteration: 14, loss 518.7693481445312\n",
      "Iteration: 15, loss 506.6921081542969\n",
      "Iteration: 16, loss 494.9648132324219\n",
      "Iteration: 17, loss 483.5303955078125\n",
      "Iteration: 18, loss 472.3996276855469\n",
      "Iteration: 19, loss 461.5086364746094\n",
      "Iteration: 20, loss 450.9468688964844\n",
      "Iteration: 21, loss 440.619873046875\n",
      "Iteration: 22, loss 430.5333557128906\n",
      "Iteration: 23, loss 420.7386474609375\n",
      "Iteration: 24, loss 411.2206115722656\n",
      "Iteration: 25, loss 401.9309387207031\n",
      "Iteration: 26, loss 392.8981018066406\n",
      "Iteration: 27, loss 384.09637451171875\n",
      "Iteration: 28, loss 375.4711608886719\n",
      "Iteration: 29, loss 367.0478820800781\n",
      "Iteration: 30, loss 358.8543395996094\n",
      "Iteration: 31, loss 350.87890625\n",
      "Iteration: 32, loss 343.0951843261719\n",
      "Iteration: 33, loss 335.47998046875\n",
      "Iteration: 34, loss 328.0250244140625\n",
      "Iteration: 35, loss 320.73370361328125\n",
      "Iteration: 36, loss 313.5702209472656\n",
      "Iteration: 37, loss 306.5610656738281\n",
      "Iteration: 38, loss 299.6988220214844\n",
      "Iteration: 39, loss 292.97662353515625\n",
      "Iteration: 40, loss 286.3863830566406\n",
      "Iteration: 41, loss 279.91864013671875\n",
      "Iteration: 42, loss 273.6012268066406\n",
      "Iteration: 43, loss 267.39605712890625\n",
      "Iteration: 44, loss 261.29632568359375\n",
      "Iteration: 45, loss 255.31375122070312\n",
      "Iteration: 46, loss 249.44277954101562\n",
      "Iteration: 47, loss 243.67031860351562\n",
      "Iteration: 48, loss 238.00611877441406\n",
      "Iteration: 49, loss 232.4495391845703\n",
      "Iteration: 50, loss 227.00106811523438\n",
      "Iteration: 51, loss 221.6801300048828\n",
      "Iteration: 52, loss 216.46141052246094\n",
      "Iteration: 53, loss 211.3378448486328\n",
      "Iteration: 54, loss 206.3330078125\n",
      "Iteration: 55, loss 201.4355010986328\n",
      "Iteration: 56, loss 196.6388702392578\n",
      "Iteration: 57, loss 191.9410400390625\n",
      "Iteration: 58, loss 187.3229522705078\n",
      "Iteration: 59, loss 182.7891082763672\n",
      "Iteration: 60, loss 178.32855224609375\n",
      "Iteration: 61, loss 173.9615020751953\n",
      "Iteration: 62, loss 169.6919403076172\n",
      "Iteration: 63, loss 165.51698303222656\n",
      "Iteration: 64, loss 161.4249725341797\n",
      "Iteration: 65, loss 157.41835021972656\n",
      "Iteration: 66, loss 153.48031616210938\n",
      "Iteration: 67, loss 149.62071228027344\n",
      "Iteration: 68, loss 145.84097290039062\n",
      "Iteration: 69, loss 142.13125610351562\n",
      "Iteration: 70, loss 138.48245239257812\n",
      "Iteration: 71, loss 134.90533447265625\n",
      "Iteration: 72, loss 131.3971405029297\n",
      "Iteration: 73, loss 127.95528411865234\n",
      "Iteration: 74, loss 124.58122253417969\n",
      "Iteration: 75, loss 121.26972198486328\n",
      "Iteration: 76, loss 118.01895141601562\n",
      "Iteration: 77, loss 114.84076690673828\n",
      "Iteration: 78, loss 111.7281494140625\n",
      "Iteration: 79, loss 108.67871856689453\n",
      "Iteration: 80, loss 105.69796752929688\n",
      "Iteration: 81, loss 102.7805404663086\n",
      "Iteration: 82, loss 99.92427062988281\n",
      "Iteration: 83, loss 97.12594604492188\n",
      "Iteration: 84, loss 94.38774871826172\n",
      "Iteration: 85, loss 91.7071762084961\n",
      "Iteration: 86, loss 89.08775329589844\n",
      "Iteration: 87, loss 86.5212631225586\n",
      "Iteration: 88, loss 84.01190948486328\n",
      "Iteration: 89, loss 81.5547103881836\n",
      "Iteration: 90, loss 79.15487670898438\n",
      "Iteration: 91, loss 76.80946350097656\n",
      "Iteration: 92, loss 74.5193862915039\n",
      "Iteration: 93, loss 72.28324127197266\n",
      "Iteration: 94, loss 70.1002197265625\n",
      "Iteration: 95, loss 67.96673583984375\n",
      "Iteration: 96, loss 65.88697814941406\n",
      "Iteration: 97, loss 63.855255126953125\n",
      "Iteration: 98, loss 61.87165069580078\n",
      "Iteration: 99, loss 59.935340881347656\n",
      "Iteration: 100, loss 58.04972839355469\n",
      "Iteration: 101, loss 56.207698822021484\n",
      "Iteration: 102, loss 54.40929412841797\n",
      "Iteration: 103, loss 52.65741729736328\n",
      "Iteration: 104, loss 50.95484924316406\n",
      "Iteration: 105, loss 49.29728698730469\n",
      "Iteration: 106, loss 47.68217468261719\n",
      "Iteration: 107, loss 46.108341217041016\n",
      "Iteration: 108, loss 44.5773811340332\n",
      "Iteration: 109, loss 43.08706283569336\n",
      "Iteration: 110, loss 41.63817596435547\n",
      "Iteration: 111, loss 40.22922897338867\n",
      "Iteration: 112, loss 38.859962463378906\n",
      "Iteration: 113, loss 37.52842712402344\n",
      "Iteration: 114, loss 36.23329544067383\n",
      "Iteration: 115, loss 34.9774169921875\n",
      "Iteration: 116, loss 33.75533676147461\n",
      "Iteration: 117, loss 32.56805419921875\n",
      "Iteration: 118, loss 31.419567108154297\n",
      "Iteration: 119, loss 30.304479598999023\n",
      "Iteration: 120, loss 29.22179412841797\n",
      "Iteration: 121, loss 28.172306060791016\n",
      "Iteration: 122, loss 27.155555725097656\n",
      "Iteration: 123, loss 26.168195724487305\n",
      "Iteration: 124, loss 25.21282196044922\n",
      "Iteration: 125, loss 24.286447525024414\n",
      "Iteration: 126, loss 23.39034652709961\n",
      "Iteration: 127, loss 22.52202796936035\n",
      "Iteration: 128, loss 21.68198013305664\n",
      "Iteration: 129, loss 20.867956161499023\n",
      "Iteration: 130, loss 20.08022689819336\n",
      "Iteration: 131, loss 19.31742286682129\n",
      "Iteration: 132, loss 18.578041076660156\n",
      "Iteration: 133, loss 17.86252212524414\n",
      "Iteration: 134, loss 17.170772552490234\n",
      "Iteration: 135, loss 16.501724243164062\n",
      "Iteration: 136, loss 15.855964660644531\n",
      "Iteration: 137, loss 15.2313814163208\n",
      "Iteration: 138, loss 14.628302574157715\n",
      "Iteration: 139, loss 14.04697036743164\n",
      "Iteration: 140, loss 13.484468460083008\n",
      "Iteration: 141, loss 12.942014694213867\n",
      "Iteration: 142, loss 12.418821334838867\n",
      "Iteration: 143, loss 11.914363861083984\n",
      "Iteration: 144, loss 11.428519248962402\n",
      "Iteration: 145, loss 10.960116386413574\n",
      "Iteration: 146, loss 10.508922576904297\n",
      "Iteration: 147, loss 10.074100494384766\n",
      "Iteration: 148, loss 9.65549087524414\n",
      "Iteration: 149, loss 9.252965927124023\n",
      "Iteration: 150, loss 8.866066932678223\n",
      "Iteration: 151, loss 8.493551254272461\n",
      "Iteration: 152, loss 8.135323524475098\n",
      "Iteration: 153, loss 7.790560245513916\n",
      "Iteration: 154, loss 7.458732604980469\n",
      "Iteration: 155, loss 7.1402716636657715\n",
      "Iteration: 156, loss 6.834196090698242\n",
      "Iteration: 157, loss 6.540217876434326\n",
      "Iteration: 158, loss 6.257382392883301\n",
      "Iteration: 159, loss 5.986150741577148\n",
      "Iteration: 160, loss 5.72550106048584\n",
      "Iteration: 161, loss 5.4753546714782715\n",
      "Iteration: 162, loss 5.235211372375488\n",
      "Iteration: 163, loss 5.004849433898926\n",
      "Iteration: 164, loss 4.783384799957275\n",
      "Iteration: 165, loss 4.570927619934082\n",
      "Iteration: 166, loss 4.367353916168213\n",
      "Iteration: 167, loss 4.1718902587890625\n",
      "Iteration: 168, loss 3.9845058917999268\n",
      "Iteration: 169, loss 3.8050992488861084\n",
      "Iteration: 170, loss 3.63293719291687\n",
      "Iteration: 171, loss 3.468245267868042\n",
      "Iteration: 172, loss 3.3102803230285645\n",
      "Iteration: 173, loss 3.1590583324432373\n",
      "Iteration: 174, loss 3.014292001724243\n",
      "Iteration: 175, loss 2.875823736190796\n",
      "Iteration: 176, loss 2.743428945541382\n",
      "Iteration: 177, loss 2.6166231632232666\n",
      "Iteration: 178, loss 2.4954581260681152\n",
      "Iteration: 179, loss 2.3793110847473145\n",
      "Iteration: 180, loss 2.268493175506592\n",
      "Iteration: 181, loss 2.1625757217407227\n",
      "Iteration: 182, loss 2.0614047050476074\n",
      "Iteration: 183, loss 1.9646681547164917\n",
      "Iteration: 184, loss 1.87245512008667\n",
      "Iteration: 185, loss 1.7844716310501099\n",
      "Iteration: 186, loss 1.7004210948944092\n",
      "Iteration: 187, loss 1.6202043294906616\n",
      "Iteration: 188, loss 1.5435304641723633\n",
      "Iteration: 189, loss 1.4704512357711792\n",
      "Iteration: 190, loss 1.400707483291626\n",
      "Iteration: 191, loss 1.3341983556747437\n",
      "Iteration: 192, loss 1.2707242965698242\n",
      "Iteration: 193, loss 1.2102292776107788\n",
      "Iteration: 194, loss 1.1525962352752686\n",
      "Iteration: 195, loss 1.0976572036743164\n",
      "Iteration: 196, loss 1.0452224016189575\n",
      "Iteration: 197, loss 0.9952238202095032\n",
      "Iteration: 198, loss 0.9480445384979248\n",
      "Iteration: 199, loss 0.9030489325523376\n",
      "Iteration: 200, loss 0.8601522445678711\n",
      "Iteration: 201, loss 0.8192628622055054\n",
      "Iteration: 202, loss 0.7803219556808472\n",
      "Iteration: 203, loss 0.7432255148887634\n",
      "Iteration: 204, loss 0.707900881767273\n",
      "Iteration: 205, loss 0.6742553114891052\n",
      "Iteration: 206, loss 0.6422101259231567\n",
      "Iteration: 207, loss 0.611662745475769\n",
      "Iteration: 208, loss 0.582568347454071\n",
      "Iteration: 209, loss 0.5548617839813232\n",
      "Iteration: 210, loss 0.5284594297409058\n",
      "Iteration: 211, loss 0.5033343434333801\n",
      "Iteration: 212, loss 0.4794057607650757\n",
      "Iteration: 213, loss 0.45663806796073914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 214, loss 0.43494558334350586\n",
      "Iteration: 215, loss 0.41430142521858215\n",
      "Iteration: 216, loss 0.39466017484664917\n",
      "Iteration: 217, loss 0.3759506344795227\n",
      "Iteration: 218, loss 0.35813891887664795\n",
      "Iteration: 219, loss 0.3411835730075836\n",
      "Iteration: 220, loss 0.325062096118927\n",
      "Iteration: 221, loss 0.30968374013900757\n",
      "Iteration: 222, loss 0.2950601279735565\n",
      "Iteration: 223, loss 0.2811462879180908\n",
      "Iteration: 224, loss 0.2678896486759186\n",
      "Iteration: 225, loss 0.2552884817123413\n",
      "Iteration: 226, loss 0.24328161776065826\n",
      "Iteration: 227, loss 0.2318524569272995\n",
      "Iteration: 228, loss 0.2209801822900772\n",
      "Iteration: 229, loss 0.21062764525413513\n",
      "Iteration: 230, loss 0.2007783055305481\n",
      "Iteration: 231, loss 0.1913951188325882\n",
      "Iteration: 232, loss 0.18246501684188843\n",
      "Iteration: 233, loss 0.17398026585578918\n",
      "Iteration: 234, loss 0.16590918600559235\n",
      "Iteration: 235, loss 0.15822042524814606\n",
      "Iteration: 236, loss 0.15090550482273102\n",
      "Iteration: 237, loss 0.14393408596515656\n",
      "Iteration: 238, loss 0.1372995227575302\n",
      "Iteration: 239, loss 0.13097965717315674\n",
      "Iteration: 240, loss 0.12496373057365417\n",
      "Iteration: 241, loss 0.1192382425069809\n",
      "Iteration: 242, loss 0.11378269642591476\n",
      "Iteration: 243, loss 0.1085832417011261\n",
      "Iteration: 244, loss 0.10363679379224777\n",
      "Iteration: 245, loss 0.09892778098583221\n",
      "Iteration: 246, loss 0.09443716704845428\n",
      "Iteration: 247, loss 0.09016421437263489\n",
      "Iteration: 248, loss 0.08608991652727127\n",
      "Iteration: 249, loss 0.0822090208530426\n",
      "Iteration: 250, loss 0.07851041108369827\n",
      "Iteration: 251, loss 0.07498679310083389\n",
      "Iteration: 252, loss 0.07162963598966599\n",
      "Iteration: 253, loss 0.06842996180057526\n",
      "Iteration: 254, loss 0.06537958979606628\n",
      "Iteration: 255, loss 0.06247043237090111\n",
      "Iteration: 256, loss 0.059696439653635025\n",
      "Iteration: 257, loss 0.05705104395747185\n",
      "Iteration: 258, loss 0.05452761799097061\n",
      "Iteration: 259, loss 0.052120305597782135\n",
      "Iteration: 260, loss 0.04982345551252365\n",
      "Iteration: 261, loss 0.04763149097561836\n",
      "Iteration: 262, loss 0.04554015025496483\n",
      "Iteration: 263, loss 0.043544359505176544\n",
      "Iteration: 264, loss 0.04163871333003044\n",
      "Iteration: 265, loss 0.03982013091444969\n",
      "Iteration: 266, loss 0.038084059953689575\n",
      "Iteration: 267, loss 0.03642632067203522\n",
      "Iteration: 268, loss 0.0348438136279583\n",
      "Iteration: 269, loss 0.033332716673612595\n",
      "Iteration: 270, loss 0.03188835084438324\n",
      "Iteration: 271, loss 0.030509265139698982\n",
      "Iteration: 272, loss 0.029191691428422928\n",
      "Iteration: 273, loss 0.02793281152844429\n",
      "Iteration: 274, loss 0.026729755103588104\n",
      "Iteration: 275, loss 0.025580430403351784\n",
      "Iteration: 276, loss 0.024481933563947678\n",
      "Iteration: 277, loss 0.023431116715073586\n",
      "Iteration: 278, loss 0.022427082061767578\n",
      "Iteration: 279, loss 0.021467193961143494\n",
      "Iteration: 280, loss 0.02054973877966404\n",
      "Iteration: 281, loss 0.01967262104153633\n",
      "Iteration: 282, loss 0.018833085894584656\n",
      "Iteration: 283, loss 0.018030483275651932\n",
      "Iteration: 284, loss 0.017262982204556465\n",
      "Iteration: 285, loss 0.01652871072292328\n",
      "Iteration: 286, loss 0.015826284885406494\n",
      "Iteration: 287, loss 0.015154949389398098\n",
      "Iteration: 288, loss 0.014511545188724995\n",
      "Iteration: 289, loss 0.01389648299664259\n",
      "Iteration: 290, loss 0.013307967223227024\n",
      "Iteration: 291, loss 0.01274455338716507\n",
      "Iteration: 292, loss 0.012205465696752071\n",
      "Iteration: 293, loss 0.011689475737512112\n",
      "Iteration: 294, loss 0.01119556650519371\n",
      "Iteration: 295, loss 0.010722861625254154\n",
      "Iteration: 296, loss 0.01027023233473301\n",
      "Iteration: 297, loss 0.009836913086473942\n",
      "Iteration: 298, loss 0.00942214671522379\n",
      "Iteration: 299, loss 0.009024866856634617\n",
      "Iteration: 300, loss 0.008644495159387589\n",
      "Iteration: 301, loss 0.008280276320874691\n",
      "Iteration: 302, loss 0.007931416854262352\n",
      "Iteration: 303, loss 0.007597350515425205\n",
      "Iteration: 304, loss 0.007277393713593483\n",
      "Iteration: 305, loss 0.0069710202515125275\n",
      "Iteration: 306, loss 0.006677479483187199\n",
      "Iteration: 307, loss 0.006396320182830095\n",
      "Iteration: 308, loss 0.006126988213509321\n",
      "Iteration: 309, loss 0.005869004875421524\n",
      "Iteration: 310, loss 0.005621906369924545\n",
      "Iteration: 311, loss 0.005385077092796564\n",
      "Iteration: 312, loss 0.00515819201245904\n",
      "Iteration: 313, loss 0.0049408599734306335\n",
      "Iteration: 314, loss 0.004732624161988497\n",
      "Iteration: 315, loss 0.00453310227021575\n",
      "Iteration: 316, loss 0.004341891501098871\n",
      "Iteration: 317, loss 0.004158683121204376\n",
      "Iteration: 318, loss 0.003983156755566597\n",
      "Iteration: 319, loss 0.0038149836473166943\n",
      "Iteration: 320, loss 0.003653760300949216\n",
      "Iteration: 321, loss 0.003499310929328203\n",
      "Iteration: 322, loss 0.003351279068738222\n",
      "Iteration: 323, loss 0.0032094568014144897\n",
      "Iteration: 324, loss 0.003073539584875107\n",
      "Iteration: 325, loss 0.002943272003903985\n",
      "Iteration: 326, loss 0.0028184421826153994\n",
      "Iteration: 327, loss 0.0026988245081156492\n",
      "Iteration: 328, loss 0.0025841756723821163\n",
      "Iteration: 329, loss 0.0024743189569562674\n",
      "Iteration: 330, loss 0.0023690536618232727\n",
      "Iteration: 331, loss 0.0022681760601699352\n",
      "Iteration: 332, loss 0.002171494299545884\n",
      "Iteration: 333, loss 0.0020788453985005617\n",
      "Iteration: 334, loss 0.001990069169551134\n",
      "Iteration: 335, loss 0.0019050107803195715\n",
      "Iteration: 336, loss 0.0018235122552141547\n",
      "Iteration: 337, loss 0.0017453889595344663\n",
      "Iteration: 338, loss 0.0016705341404303908\n",
      "Iteration: 339, loss 0.0015988200902938843\n",
      "Iteration: 340, loss 0.0015301026869565248\n",
      "Iteration: 341, loss 0.0014642545720562339\n",
      "Iteration: 342, loss 0.0014011785387992859\n",
      "Iteration: 343, loss 0.0013407346559688449\n",
      "Iteration: 344, loss 0.001282832701690495\n",
      "Iteration: 345, loss 0.0012273527681827545\n",
      "Iteration: 346, loss 0.0011742142960429192\n",
      "Iteration: 347, loss 0.0011233019176870584\n",
      "Iteration: 348, loss 0.001074539846740663\n",
      "Iteration: 349, loss 0.001027819118462503\n",
      "Iteration: 350, loss 0.000983074540272355\n",
      "Iteration: 351, loss 0.0009402176947332919\n",
      "Iteration: 352, loss 0.000899175473023206\n",
      "Iteration: 353, loss 0.0008598549175076187\n",
      "Iteration: 354, loss 0.0008222238393500447\n",
      "Iteration: 355, loss 0.0007861601188778877\n",
      "Iteration: 356, loss 0.0007516529294662178\n",
      "Iteration: 357, loss 0.0007185964495874941\n",
      "Iteration: 358, loss 0.0006869533099234104\n",
      "Iteration: 359, loss 0.0006566551746800542\n",
      "Iteration: 360, loss 0.0006276460480876267\n",
      "Iteration: 361, loss 0.0005998772103339434\n",
      "Iteration: 362, loss 0.0005732902209274471\n",
      "Iteration: 363, loss 0.0005478522507473826\n",
      "Iteration: 364, loss 0.0005235001444816589\n",
      "Iteration: 365, loss 0.0005001897807233036\n",
      "Iteration: 366, loss 0.0004778857692144811\n",
      "Iteration: 367, loss 0.0004565382841974497\n",
      "Iteration: 368, loss 0.00043611411820165813\n",
      "Iteration: 369, loss 0.00041656711255200207\n",
      "Iteration: 370, loss 0.0003978733147960156\n",
      "Iteration: 371, loss 0.00037998007610440254\n",
      "Iteration: 372, loss 0.00036286882823333144\n",
      "Iteration: 373, loss 0.00034649684675969183\n",
      "Iteration: 374, loss 0.0003308368904981762\n",
      "Iteration: 375, loss 0.000315858400426805\n",
      "Iteration: 376, loss 0.00030153439729474485\n",
      "Iteration: 377, loss 0.00028783141169697046\n",
      "Iteration: 378, loss 0.00027473244699649513\n",
      "Iteration: 379, loss 0.00026220668223686516\n",
      "Iteration: 380, loss 0.0002502375573385507\n",
      "Iteration: 381, loss 0.0002387890563113615\n",
      "Iteration: 382, loss 0.00022784450266044587\n",
      "Iteration: 383, loss 0.00021738595387432724\n",
      "Iteration: 384, loss 0.00020738820603583008\n",
      "Iteration: 385, loss 0.00019782932940870523\n",
      "Iteration: 386, loss 0.00018870241183321923\n",
      "Iteration: 387, loss 0.0001799784804461524\n",
      "Iteration: 388, loss 0.00017163953452836722\n",
      "Iteration: 389, loss 0.0001636756060179323\n",
      "Iteration: 390, loss 0.00015606635133735836\n",
      "Iteration: 391, loss 0.00014879550144542009\n",
      "Iteration: 392, loss 0.0001418537285644561\n",
      "Iteration: 393, loss 0.00013522783410735428\n",
      "Iteration: 394, loss 0.00012889296340290457\n",
      "Iteration: 395, loss 0.00012284367403481156\n",
      "Iteration: 396, loss 0.00011707256635418162\n",
      "Iteration: 397, loss 0.00011155821266584098\n",
      "Iteration: 398, loss 0.00010629338066792116\n",
      "Iteration: 399, loss 0.00010126776760444045\n",
      "Iteration: 400, loss 9.647280239732936e-05\n",
      "Iteration: 401, loss 9.189785487251356e-05\n",
      "Iteration: 402, loss 8.752814756007865e-05\n",
      "Iteration: 403, loss 8.335945312865078e-05\n",
      "Iteration: 404, loss 7.938320050016046e-05\n",
      "Iteration: 405, loss 7.559212826890871e-05\n",
      "Iteration: 406, loss 7.1971386205405e-05\n",
      "Iteration: 407, loss 6.85175255057402e-05\n",
      "Iteration: 408, loss 6.522418698295951e-05\n",
      "Iteration: 409, loss 6.208351260283962e-05\n",
      "Iteration: 410, loss 5.908588718739338e-05\n",
      "Iteration: 411, loss 5.6229902838822454e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 412, loss 5.3507927077589557e-05\n",
      "Iteration: 413, loss 5.091095590614714e-05\n",
      "Iteration: 414, loss 4.843486749450676e-05\n",
      "Iteration: 415, loss 4.607634400599636e-05\n",
      "Iteration: 416, loss 4.382687984616496e-05\n",
      "Iteration: 417, loss 4.1683175368234515e-05\n",
      "Iteration: 418, loss 3.964348434237763e-05\n",
      "Iteration: 419, loss 3.769612158066593e-05\n",
      "Iteration: 420, loss 3.584220394259319e-05\n",
      "Iteration: 421, loss 3.407773328945041e-05\n",
      "Iteration: 422, loss 3.239541911170818e-05\n",
      "Iteration: 423, loss 3.0793846235610545e-05\n",
      "Iteration: 424, loss 2.9267521313158795e-05\n",
      "Iteration: 425, loss 2.781357579806354e-05\n",
      "Iteration: 426, loss 2.6431531296111643e-05\n",
      "Iteration: 427, loss 2.511354432499502e-05\n",
      "Iteration: 428, loss 2.3858783606556244e-05\n",
      "Iteration: 429, loss 2.2665399228571914e-05\n",
      "Iteration: 430, loss 2.15300933632534e-05\n",
      "Iteration: 431, loss 2.0449449948500842e-05\n",
      "Iteration: 432, loss 1.9420604076003656e-05\n",
      "Iteration: 433, loss 1.8440916392137296e-05\n",
      "Iteration: 434, loss 1.7509544704807922e-05\n",
      "Iteration: 435, loss 1.6621766917523928e-05\n",
      "Iteration: 436, loss 1.5778496162965894e-05\n",
      "Iteration: 437, loss 1.4978589206293691e-05\n",
      "Iteration: 438, loss 1.4215814189810771e-05\n",
      "Iteration: 439, loss 1.3490403034666087e-05\n",
      "Iteration: 440, loss 1.2800707736460026e-05\n",
      "Iteration: 441, loss 1.2145484106440563e-05\n",
      "Iteration: 442, loss 1.152318964159349e-05\n",
      "Iteration: 443, loss 1.0930145435850136e-05\n",
      "Iteration: 444, loss 1.0367393770138733e-05\n",
      "Iteration: 445, loss 9.832438081502914e-06\n",
      "Iteration: 446, loss 9.324594429926947e-06\n",
      "Iteration: 447, loss 8.84193923411658e-06\n",
      "Iteration: 448, loss 8.381818588532042e-06\n",
      "Iteration: 449, loss 7.947374797367956e-06\n",
      "Iteration: 450, loss 7.5331849984650034e-06\n",
      "Iteration: 451, loss 7.140023171814391e-06\n",
      "Iteration: 452, loss 6.766254955437034e-06\n",
      "Iteration: 453, loss 6.411550202756189e-06\n",
      "Iteration: 454, loss 6.0749166550522204e-06\n",
      "Iteration: 455, loss 5.755845904786838e-06\n",
      "Iteration: 456, loss 5.4526381063624285e-06\n",
      "Iteration: 457, loss 5.165069978829706e-06\n",
      "Iteration: 458, loss 4.892121523880633e-06\n",
      "Iteration: 459, loss 4.632493528333725e-06\n",
      "Iteration: 460, loss 4.386037289805245e-06\n",
      "Iteration: 461, loss 4.152269411861198e-06\n",
      "Iteration: 462, loss 3.9312858461926226e-06\n",
      "Iteration: 463, loss 3.7217866974970093e-06\n",
      "Iteration: 464, loss 3.5220418794779107e-06\n",
      "Iteration: 465, loss 3.3335134048684267e-06\n",
      "Iteration: 466, loss 3.154705609631492e-06\n",
      "Iteration: 467, loss 2.9844773052900564e-06\n",
      "Iteration: 468, loss 2.8236324851604877e-06\n",
      "Iteration: 469, loss 2.671226411621319e-06\n",
      "Iteration: 470, loss 2.5262345388910035e-06\n",
      "Iteration: 471, loss 2.3891225282568485e-06\n",
      "Iteration: 472, loss 2.2591420929529704e-06\n",
      "Iteration: 473, loss 2.1365640350268222e-06\n",
      "Iteration: 474, loss 2.0198415313643636e-06\n",
      "Iteration: 475, loss 1.908957983687287e-06\n",
      "Iteration: 476, loss 1.8046076775135589e-06\n",
      "Iteration: 477, loss 1.705861791378993e-06\n",
      "Iteration: 478, loss 1.6117120367198368e-06\n",
      "Iteration: 479, loss 1.5232374153129058e-06\n",
      "Iteration: 480, loss 1.4392878711078083e-06\n",
      "Iteration: 481, loss 1.3589698255600524e-06\n",
      "Iteration: 482, loss 1.2842048136008088e-06\n",
      "Iteration: 483, loss 1.212630195368547e-06\n",
      "Iteration: 484, loss 1.1450637202869984e-06\n",
      "Iteration: 485, loss 1.0813633934958489e-06\n",
      "Iteration: 486, loss 1.0210661685050582e-06\n",
      "Iteration: 487, loss 9.638877145334845e-07\n",
      "Iteration: 488, loss 9.100127158490068e-07\n",
      "Iteration: 489, loss 8.588487503402575e-07\n",
      "Iteration: 490, loss 8.103302775452903e-07\n",
      "Iteration: 491, loss 7.648071687071933e-07\n",
      "Iteration: 492, loss 7.215056143650145e-07\n",
      "Iteration: 493, loss 6.805409498156223e-07\n",
      "Iteration: 494, loss 6.421749958462897e-07\n",
      "Iteration: 495, loss 6.057442192286544e-07\n",
      "Iteration: 496, loss 5.712822144232632e-07\n",
      "Iteration: 497, loss 5.385872441365791e-07\n",
      "Iteration: 498, loss 5.077194487057568e-07\n",
      "Iteration: 499, loss 4.787197553923761e-07\n"
     ]
    }
   ],
   "source": [
    "for i in range(500):\n",
    "    pred = model(X)\n",
    "    loss = loss_function(pred, Y)\n",
    "    print('Iteration: {}, loss {}'.format(i, loss.item()))\n",
    "    optimizer.zero_grad() # zero the gradients before the computation\n",
    "    loss.backward() # compute gradients for all learnable parameters in the model\n",
    "    optimizer.step() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining and using custom nn modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D1, H, D2):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D1, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D2)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        phi = self.linear1(X).clamp(min=0)\n",
    "        pred = self.linear2(phi)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoLayerNet(D1, H, D2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, loss 724.108154296875\n",
      "Iteration: 1, loss 673.79248046875\n",
      "Iteration: 2, loss 630.09619140625\n",
      "Iteration: 3, loss 591.4645385742188\n",
      "Iteration: 4, loss 556.9235229492188\n",
      "Iteration: 5, loss 525.6498413085938\n",
      "Iteration: 6, loss 497.08795166015625\n",
      "Iteration: 7, loss 470.97906494140625\n",
      "Iteration: 8, loss 446.9306945800781\n",
      "Iteration: 9, loss 424.6529541015625\n",
      "Iteration: 10, loss 403.61328125\n",
      "Iteration: 11, loss 383.6639709472656\n",
      "Iteration: 12, loss 364.6640319824219\n",
      "Iteration: 13, loss 346.5650634765625\n",
      "Iteration: 14, loss 329.2579345703125\n",
      "Iteration: 15, loss 312.80084228515625\n",
      "Iteration: 16, loss 297.0255432128906\n",
      "Iteration: 17, loss 281.9476623535156\n",
      "Iteration: 18, loss 267.5547790527344\n",
      "Iteration: 19, loss 253.77894592285156\n",
      "Iteration: 20, loss 240.56871032714844\n",
      "Iteration: 21, loss 227.87045288085938\n",
      "Iteration: 22, loss 215.7568817138672\n",
      "Iteration: 23, loss 204.234375\n",
      "Iteration: 24, loss 193.14144897460938\n",
      "Iteration: 25, loss 182.53933715820312\n",
      "Iteration: 26, loss 172.39833068847656\n",
      "Iteration: 27, loss 162.72360229492188\n",
      "Iteration: 28, loss 153.51492309570312\n",
      "Iteration: 29, loss 144.7538299560547\n",
      "Iteration: 30, loss 136.43443298339844\n",
      "Iteration: 31, loss 128.53482055664062\n",
      "Iteration: 32, loss 121.05633544921875\n",
      "Iteration: 33, loss 113.9795913696289\n",
      "Iteration: 34, loss 107.27658081054688\n",
      "Iteration: 35, loss 100.9240951538086\n",
      "Iteration: 36, loss 94.91535186767578\n",
      "Iteration: 37, loss 89.25031280517578\n",
      "Iteration: 38, loss 83.90081024169922\n",
      "Iteration: 39, loss 78.85646057128906\n",
      "Iteration: 40, loss 74.1028823852539\n",
      "Iteration: 41, loss 69.63607788085938\n",
      "Iteration: 42, loss 65.43524932861328\n",
      "Iteration: 43, loss 61.4937629699707\n",
      "Iteration: 44, loss 57.78969955444336\n",
      "Iteration: 45, loss 54.31031036376953\n",
      "Iteration: 46, loss 51.046627044677734\n",
      "Iteration: 47, loss 47.984195709228516\n",
      "Iteration: 48, loss 45.12031555175781\n",
      "Iteration: 49, loss 42.436771392822266\n",
      "Iteration: 50, loss 39.91487503051758\n",
      "Iteration: 51, loss 37.54291915893555\n",
      "Iteration: 52, loss 35.32346725463867\n",
      "Iteration: 53, loss 33.25082778930664\n",
      "Iteration: 54, loss 31.31096076965332\n",
      "Iteration: 55, loss 29.48920249938965\n",
      "Iteration: 56, loss 27.780536651611328\n",
      "Iteration: 57, loss 26.178693771362305\n",
      "Iteration: 58, loss 24.67607307434082\n",
      "Iteration: 59, loss 23.265056610107422\n",
      "Iteration: 60, loss 21.940622329711914\n",
      "Iteration: 61, loss 20.6989688873291\n",
      "Iteration: 62, loss 19.53483009338379\n",
      "Iteration: 63, loss 18.441783905029297\n",
      "Iteration: 64, loss 17.414819717407227\n",
      "Iteration: 65, loss 16.450353622436523\n",
      "Iteration: 66, loss 15.544107437133789\n",
      "Iteration: 67, loss 14.692998886108398\n",
      "Iteration: 68, loss 13.890288352966309\n",
      "Iteration: 69, loss 13.135512351989746\n",
      "Iteration: 70, loss 12.425666809082031\n",
      "Iteration: 71, loss 11.75696086883545\n",
      "Iteration: 72, loss 11.128393173217773\n",
      "Iteration: 73, loss 10.534082412719727\n",
      "Iteration: 74, loss 9.974616050720215\n",
      "Iteration: 75, loss 9.447610855102539\n",
      "Iteration: 76, loss 8.95174503326416\n",
      "Iteration: 77, loss 8.484044075012207\n",
      "Iteration: 78, loss 8.04312515258789\n",
      "Iteration: 79, loss 7.627720832824707\n",
      "Iteration: 80, loss 7.235525608062744\n",
      "Iteration: 81, loss 6.865250110626221\n",
      "Iteration: 82, loss 6.515482425689697\n",
      "Iteration: 83, loss 6.184631824493408\n",
      "Iteration: 84, loss 5.872091770172119\n",
      "Iteration: 85, loss 5.576485633850098\n",
      "Iteration: 86, loss 5.296834468841553\n",
      "Iteration: 87, loss 5.03276252746582\n",
      "Iteration: 88, loss 4.783206462860107\n",
      "Iteration: 89, loss 4.547000408172607\n",
      "Iteration: 90, loss 4.323647499084473\n",
      "Iteration: 91, loss 4.111731052398682\n",
      "Iteration: 92, loss 3.911402463912964\n",
      "Iteration: 93, loss 3.7214512825012207\n",
      "Iteration: 94, loss 3.5415122509002686\n",
      "Iteration: 95, loss 3.371072769165039\n",
      "Iteration: 96, loss 3.209317207336426\n",
      "Iteration: 97, loss 3.0558958053588867\n",
      "Iteration: 98, loss 2.9104273319244385\n",
      "Iteration: 99, loss 2.7724497318267822\n",
      "Iteration: 100, loss 2.6415281295776367\n",
      "Iteration: 101, loss 2.5172126293182373\n",
      "Iteration: 102, loss 2.3992977142333984\n",
      "Iteration: 103, loss 2.2872729301452637\n",
      "Iteration: 104, loss 2.180821180343628\n",
      "Iteration: 105, loss 2.0796165466308594\n",
      "Iteration: 106, loss 1.9834328889846802\n",
      "Iteration: 107, loss 1.8920236825942993\n",
      "Iteration: 108, loss 1.8051716089248657\n",
      "Iteration: 109, loss 1.722563624382019\n",
      "Iteration: 110, loss 1.6440318822860718\n",
      "Iteration: 111, loss 1.5690919160842896\n",
      "Iteration: 112, loss 1.4978444576263428\n",
      "Iteration: 113, loss 1.4300261735916138\n",
      "Iteration: 114, loss 1.3655014038085938\n",
      "Iteration: 115, loss 1.3041212558746338\n",
      "Iteration: 116, loss 1.2457093000411987\n",
      "Iteration: 117, loss 1.1899758577346802\n",
      "Iteration: 118, loss 1.1369785070419312\n",
      "Iteration: 119, loss 1.086477279663086\n",
      "Iteration: 120, loss 1.0384547710418701\n",
      "Iteration: 121, loss 0.9927917122840881\n",
      "Iteration: 122, loss 0.9493141770362854\n",
      "Iteration: 123, loss 0.9078332185745239\n",
      "Iteration: 124, loss 0.8683147430419922\n",
      "Iteration: 125, loss 0.8306062817573547\n",
      "Iteration: 126, loss 0.7946560382843018\n",
      "Iteration: 127, loss 0.7603790163993835\n",
      "Iteration: 128, loss 0.7276903390884399\n",
      "Iteration: 129, loss 0.6964843273162842\n",
      "Iteration: 130, loss 0.6667156219482422\n",
      "Iteration: 131, loss 0.6383101344108582\n",
      "Iteration: 132, loss 0.6111736297607422\n",
      "Iteration: 133, loss 0.5852624177932739\n",
      "Iteration: 134, loss 0.5605300664901733\n",
      "Iteration: 135, loss 0.5369229912757874\n",
      "Iteration: 136, loss 0.5143647193908691\n",
      "Iteration: 137, loss 0.4928429126739502\n",
      "Iteration: 138, loss 0.4722481369972229\n",
      "Iteration: 139, loss 0.45257264375686646\n",
      "Iteration: 140, loss 0.43379124999046326\n",
      "Iteration: 141, loss 0.4158201813697815\n",
      "Iteration: 142, loss 0.3986448645591736\n",
      "Iteration: 143, loss 0.38222262263298035\n",
      "Iteration: 144, loss 0.3665200173854828\n",
      "Iteration: 145, loss 0.3514988422393799\n",
      "Iteration: 146, loss 0.3371226489543915\n",
      "Iteration: 147, loss 0.3233824074268341\n",
      "Iteration: 148, loss 0.3102712035179138\n",
      "Iteration: 149, loss 0.2977118194103241\n",
      "Iteration: 150, loss 0.28569266200065613\n",
      "Iteration: 151, loss 0.27419427037239075\n",
      "Iteration: 152, loss 0.2631862461566925\n",
      "Iteration: 153, loss 0.25264468789100647\n",
      "Iteration: 154, loss 0.24254447221755981\n",
      "Iteration: 155, loss 0.2328755259513855\n",
      "Iteration: 156, loss 0.2236153781414032\n",
      "Iteration: 157, loss 0.2147432565689087\n",
      "Iteration: 158, loss 0.2062365561723709\n",
      "Iteration: 159, loss 0.19809503853321075\n",
      "Iteration: 160, loss 0.1902923285961151\n",
      "Iteration: 161, loss 0.182816281914711\n",
      "Iteration: 162, loss 0.1756502389907837\n",
      "Iteration: 163, loss 0.1687779277563095\n",
      "Iteration: 164, loss 0.1621866226196289\n",
      "Iteration: 165, loss 0.15586966276168823\n",
      "Iteration: 166, loss 0.14981532096862793\n",
      "Iteration: 167, loss 0.1439920961856842\n",
      "Iteration: 168, loss 0.1384047269821167\n",
      "Iteration: 169, loss 0.13304467499256134\n",
      "Iteration: 170, loss 0.1279054880142212\n",
      "Iteration: 171, loss 0.12297371029853821\n",
      "Iteration: 172, loss 0.11824547499418259\n",
      "Iteration: 173, loss 0.11370175331830978\n",
      "Iteration: 174, loss 0.10934664309024811\n",
      "Iteration: 175, loss 0.10516703128814697\n",
      "Iteration: 176, loss 0.10115199536085129\n",
      "Iteration: 177, loss 0.09730113297700882\n",
      "Iteration: 178, loss 0.09360117465257645\n",
      "Iteration: 179, loss 0.0900525450706482\n",
      "Iteration: 180, loss 0.08664100617170334\n",
      "Iteration: 181, loss 0.08336367458105087\n",
      "Iteration: 182, loss 0.08021576702594757\n",
      "Iteration: 183, loss 0.07719603180885315\n",
      "Iteration: 184, loss 0.07429246604442596\n",
      "Iteration: 185, loss 0.07152360677719116\n",
      "Iteration: 186, loss 0.06887858361005783\n",
      "Iteration: 187, loss 0.06633725762367249\n",
      "Iteration: 188, loss 0.06389500945806503\n",
      "Iteration: 189, loss 0.0615486279129982\n",
      "Iteration: 190, loss 0.05929044261574745\n",
      "Iteration: 191, loss 0.05711966007947922\n",
      "Iteration: 192, loss 0.05503178760409355\n",
      "Iteration: 193, loss 0.05302051827311516\n",
      "Iteration: 194, loss 0.05108717083930969\n",
      "Iteration: 195, loss 0.049228403717279434\n",
      "Iteration: 196, loss 0.04743865877389908\n",
      "Iteration: 197, loss 0.04571673274040222\n",
      "Iteration: 198, loss 0.04406062513589859\n",
      "Iteration: 199, loss 0.042467791587114334\n",
      "Iteration: 200, loss 0.040934737771749496\n",
      "Iteration: 201, loss 0.03945985436439514\n",
      "Iteration: 202, loss 0.0380403958261013\n",
      "Iteration: 203, loss 0.03667531907558441\n",
      "Iteration: 204, loss 0.03536113351583481\n",
      "Iteration: 205, loss 0.03409631550312042\n",
      "Iteration: 206, loss 0.03287869319319725\n",
      "Iteration: 207, loss 0.031707119196653366\n",
      "Iteration: 208, loss 0.03057842142879963\n",
      "Iteration: 209, loss 0.02949162945151329\n",
      "Iteration: 210, loss 0.028444964438676834\n",
      "Iteration: 211, loss 0.027436787262558937\n",
      "Iteration: 212, loss 0.02646542340517044\n",
      "Iteration: 213, loss 0.02552969940006733\n",
      "Iteration: 214, loss 0.02462899126112461\n",
      "Iteration: 215, loss 0.023761676624417305\n",
      "Iteration: 216, loss 0.02292696200311184\n",
      "Iteration: 217, loss 0.022122083231806755\n",
      "Iteration: 218, loss 0.021346310153603554\n",
      "Iteration: 219, loss 0.020598528906702995\n",
      "Iteration: 220, loss 0.019878659397363663\n",
      "Iteration: 221, loss 0.01918475516140461\n",
      "Iteration: 222, loss 0.018515296280384064\n",
      "Iteration: 223, loss 0.017870377749204636\n",
      "Iteration: 224, loss 0.01724856160581112\n",
      "Iteration: 225, loss 0.016649069264531136\n",
      "Iteration: 226, loss 0.016071049496531487\n",
      "Iteration: 227, loss 0.015513499267399311\n",
      "Iteration: 228, loss 0.014976311475038528\n",
      "Iteration: 229, loss 0.014458232559263706\n",
      "Iteration: 230, loss 0.01395826693624258\n",
      "Iteration: 231, loss 0.013476424850523472\n",
      "Iteration: 232, loss 0.013010961934924126\n",
      "Iteration: 233, loss 0.012561949901282787\n",
      "Iteration: 234, loss 0.012128678150475025\n",
      "Iteration: 235, loss 0.011710958555340767\n",
      "Iteration: 236, loss 0.01130816712975502\n",
      "Iteration: 237, loss 0.010919563472270966\n",
      "Iteration: 238, loss 0.010544710792601109\n",
      "Iteration: 239, loss 0.010182967409491539\n",
      "Iteration: 240, loss 0.009834188967943192\n",
      "Iteration: 241, loss 0.009497563354671001\n",
      "Iteration: 242, loss 0.00917266309261322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 243, loss 0.008868599310517311\n",
      "Iteration: 244, loss 0.00857487041503191\n",
      "Iteration: 245, loss 0.008291061967611313\n",
      "Iteration: 246, loss 0.008017173036932945\n",
      "Iteration: 247, loss 0.0077527230605483055\n",
      "Iteration: 248, loss 0.00749761750921607\n",
      "Iteration: 249, loss 0.007251030765473843\n",
      "Iteration: 250, loss 0.007012960501015186\n",
      "Iteration: 251, loss 0.006782988086342812\n",
      "Iteration: 252, loss 0.00656092306599021\n",
      "Iteration: 253, loss 0.006346302106976509\n",
      "Iteration: 254, loss 0.006139026954770088\n",
      "Iteration: 255, loss 0.005938739515841007\n",
      "Iteration: 256, loss 0.0057452660985291\n",
      "Iteration: 257, loss 0.005558326840400696\n",
      "Iteration: 258, loss 0.005377717781811953\n",
      "Iteration: 259, loss 0.005203227512538433\n",
      "Iteration: 260, loss 0.005034560803323984\n",
      "Iteration: 261, loss 0.004871495999395847\n",
      "Iteration: 262, loss 0.00471392460167408\n",
      "Iteration: 263, loss 0.004561764188110828\n",
      "Iteration: 264, loss 0.0044146329164505005\n",
      "Iteration: 265, loss 0.00427234498783946\n",
      "Iteration: 266, loss 0.004134729970246553\n",
      "Iteration: 267, loss 0.004001683555543423\n",
      "Iteration: 268, loss 0.003873162902891636\n",
      "Iteration: 269, loss 0.0037488825619220734\n",
      "Iteration: 270, loss 0.0036287643015384674\n",
      "Iteration: 271, loss 0.0035126011352986097\n",
      "Iteration: 272, loss 0.0034002589527517557\n",
      "Iteration: 273, loss 0.0032916537020355463\n",
      "Iteration: 274, loss 0.003186604008078575\n",
      "Iteration: 275, loss 0.0030849771574139595\n",
      "Iteration: 276, loss 0.0029866681434214115\n",
      "Iteration: 277, loss 0.0028916848823428154\n",
      "Iteration: 278, loss 0.002799856010824442\n",
      "Iteration: 279, loss 0.0027109701186418533\n",
      "Iteration: 280, loss 0.00262496923096478\n",
      "Iteration: 281, loss 0.0025417786091566086\n",
      "Iteration: 282, loss 0.0024612736888229847\n",
      "Iteration: 283, loss 0.0023833392187952995\n",
      "Iteration: 284, loss 0.002307987306267023\n",
      "Iteration: 285, loss 0.0022351089864969254\n",
      "Iteration: 286, loss 0.002164611127227545\n",
      "Iteration: 287, loss 0.002096382435411215\n",
      "Iteration: 288, loss 0.00203036074526608\n",
      "Iteration: 289, loss 0.0019665644504129887\n",
      "Iteration: 290, loss 0.001904738019220531\n",
      "Iteration: 291, loss 0.0018449235940352082\n",
      "Iteration: 292, loss 0.0017870133742690086\n",
      "Iteration: 293, loss 0.0017309393733739853\n",
      "Iteration: 294, loss 0.0016766920452937484\n",
      "Iteration: 295, loss 0.0016242204001173377\n",
      "Iteration: 296, loss 0.0015734056942164898\n",
      "Iteration: 297, loss 0.0015242021763697267\n",
      "Iteration: 298, loss 0.0014765964588150382\n",
      "Iteration: 299, loss 0.0014304841170087457\n",
      "Iteration: 300, loss 0.0013858770253136754\n",
      "Iteration: 301, loss 0.001342672505415976\n",
      "Iteration: 302, loss 0.0013008428504690528\n",
      "Iteration: 303, loss 0.0012603331124410033\n",
      "Iteration: 304, loss 0.0012211118591949344\n",
      "Iteration: 305, loss 0.0011831539450213313\n",
      "Iteration: 306, loss 0.0011464396957308054\n",
      "Iteration: 307, loss 0.001110826269723475\n",
      "Iteration: 308, loss 0.0010763707105070353\n",
      "Iteration: 309, loss 0.001043011900037527\n",
      "Iteration: 310, loss 0.0010106832487508655\n",
      "Iteration: 311, loss 0.0009794242214411497\n",
      "Iteration: 312, loss 0.0009491256205365062\n",
      "Iteration: 313, loss 0.0009197902982123196\n",
      "Iteration: 314, loss 0.0008913759374991059\n",
      "Iteration: 315, loss 0.0008638192084617913\n",
      "Iteration: 316, loss 0.000837180414237082\n",
      "Iteration: 317, loss 0.0008113864460028708\n",
      "Iteration: 318, loss 0.0007863629143685102\n",
      "Iteration: 319, loss 0.0007621566765010357\n",
      "Iteration: 320, loss 0.0007386979414150119\n",
      "Iteration: 321, loss 0.0007159538217820227\n",
      "Iteration: 322, loss 0.0006939487066119909\n",
      "Iteration: 323, loss 0.0006726361461915076\n",
      "Iteration: 324, loss 0.0006519819726236165\n",
      "Iteration: 325, loss 0.0006319843232631683\n",
      "Iteration: 326, loss 0.0006125960499048233\n",
      "Iteration: 327, loss 0.0005938245449215174\n",
      "Iteration: 328, loss 0.0005756256286986172\n",
      "Iteration: 329, loss 0.0005579943535849452\n",
      "Iteration: 330, loss 0.0005409190780483186\n",
      "Iteration: 331, loss 0.0005243694176897407\n",
      "Iteration: 332, loss 0.0005083371652290225\n",
      "Iteration: 333, loss 0.0004928103880956769\n",
      "Iteration: 334, loss 0.00047775867278687656\n",
      "Iteration: 335, loss 0.00046318001113831997\n",
      "Iteration: 336, loss 0.0004490475112106651\n",
      "Iteration: 337, loss 0.00043535101576708257\n",
      "Iteration: 338, loss 0.0004220804839860648\n",
      "Iteration: 339, loss 0.0004092288145329803\n",
      "Iteration: 340, loss 0.00039677383028902113\n",
      "Iteration: 341, loss 0.00038469856372103095\n",
      "Iteration: 342, loss 0.00037299812538549304\n",
      "Iteration: 343, loss 0.0003616692265495658\n",
      "Iteration: 344, loss 0.00035067895078100264\n",
      "Iteration: 345, loss 0.0003400322166271508\n",
      "Iteration: 346, loss 0.0003297098446637392\n",
      "Iteration: 347, loss 0.0003197051119059324\n",
      "Iteration: 348, loss 0.0003100142639596015\n",
      "Iteration: 349, loss 0.00030062271980568767\n",
      "Iteration: 350, loss 0.0002915169461630285\n",
      "Iteration: 351, loss 0.0002827018906828016\n",
      "Iteration: 352, loss 0.0002741443458944559\n",
      "Iteration: 353, loss 0.00026584332226775587\n",
      "Iteration: 354, loss 0.0002578148851171136\n",
      "Iteration: 355, loss 0.0002500226837582886\n",
      "Iteration: 356, loss 0.0002424657577648759\n",
      "Iteration: 357, loss 0.0002351490402361378\n",
      "Iteration: 358, loss 0.00022805242042522877\n",
      "Iteration: 359, loss 0.0002211704704677686\n",
      "Iteration: 360, loss 0.00021450234635267407\n",
      "Iteration: 361, loss 0.00020803534425795078\n",
      "Iteration: 362, loss 0.00020176832913421094\n",
      "Iteration: 363, loss 0.0001956932683242485\n",
      "Iteration: 364, loss 0.00018980215827468783\n",
      "Iteration: 365, loss 0.0001840913901105523\n",
      "Iteration: 366, loss 0.00017855828627943993\n",
      "Iteration: 367, loss 0.00017319299513474107\n",
      "Iteration: 368, loss 0.00016798822616692632\n",
      "Iteration: 369, loss 0.0001629470643820241\n",
      "Iteration: 370, loss 0.00015805351722519845\n",
      "Iteration: 371, loss 0.00015330786118283868\n",
      "Iteration: 372, loss 0.0001487054250901565\n",
      "Iteration: 373, loss 0.0001442480570403859\n",
      "Iteration: 374, loss 0.00013992494496051222\n",
      "Iteration: 375, loss 0.000135735041112639\n",
      "Iteration: 376, loss 0.00013166652934160084\n",
      "Iteration: 377, loss 0.00012772658374160528\n",
      "Iteration: 378, loss 0.00012390925257932395\n",
      "Iteration: 379, loss 0.00012020124995615333\n",
      "Iteration: 380, loss 0.00011660251038847491\n",
      "Iteration: 381, loss 0.00011311912385281175\n",
      "Iteration: 382, loss 0.00010973792086588219\n",
      "Iteration: 383, loss 0.00010645834845490754\n",
      "Iteration: 384, loss 0.0001032787113217637\n",
      "Iteration: 385, loss 0.0001001962591544725\n",
      "Iteration: 386, loss 9.72067064139992e-05\n",
      "Iteration: 387, loss 9.430770296603441e-05\n",
      "Iteration: 388, loss 9.149515244644135e-05\n",
      "Iteration: 389, loss 8.876983338268474e-05\n",
      "Iteration: 390, loss 8.612331293988973e-05\n",
      "Iteration: 391, loss 8.356172475032508e-05\n",
      "Iteration: 392, loss 8.106967288767919e-05\n",
      "Iteration: 393, loss 7.865780935389921e-05\n",
      "Iteration: 394, loss 7.631786138517782e-05\n",
      "Iteration: 395, loss 7.404883217532188e-05\n",
      "Iteration: 396, loss 7.184548303484917e-05\n",
      "Iteration: 397, loss 6.971099355723709e-05\n",
      "Iteration: 398, loss 6.7640830820892e-05\n",
      "Iteration: 399, loss 6.563158240169287e-05\n",
      "Iteration: 400, loss 6.368134927470237e-05\n",
      "Iteration: 401, loss 6.17933546891436e-05\n",
      "Iteration: 402, loss 5.995992250973359e-05\n",
      "Iteration: 403, loss 5.818400313728489e-05\n",
      "Iteration: 404, loss 5.645936471410096e-05\n",
      "Iteration: 405, loss 5.478306411532685e-05\n",
      "Iteration: 406, loss 5.315869566402398e-05\n",
      "Iteration: 407, loss 5.158632848178968e-05\n",
      "Iteration: 408, loss 5.005684943171218e-05\n",
      "Iteration: 409, loss 4.857476960751228e-05\n",
      "Iteration: 410, loss 4.713788803201169e-05\n",
      "Iteration: 411, loss 4.574461854645051e-05\n",
      "Iteration: 412, loss 4.439118129084818e-05\n",
      "Iteration: 413, loss 4.308065035729669e-05\n",
      "Iteration: 414, loss 4.180545874987729e-05\n",
      "Iteration: 415, loss 4.0570263081463054e-05\n",
      "Iteration: 416, loss 3.9370195736410096e-05\n",
      "Iteration: 417, loss 3.820876372628845e-05\n",
      "Iteration: 418, loss 3.7080473703099415e-05\n",
      "Iteration: 419, loss 3.598472540033981e-05\n",
      "Iteration: 420, loss 3.492597534204833e-05\n",
      "Iteration: 421, loss 3.389789708307944e-05\n",
      "Iteration: 422, loss 3.28963651554659e-05\n",
      "Iteration: 423, loss 3.1928211683407426e-05\n",
      "Iteration: 424, loss 3.0988947401056066e-05\n",
      "Iteration: 425, loss 3.0074310416239314e-05\n",
      "Iteration: 426, loss 2.9188229746068828e-05\n",
      "Iteration: 427, loss 2.832915197359398e-05\n",
      "Iteration: 428, loss 2.7495636459207162e-05\n",
      "Iteration: 429, loss 2.668639353942126e-05\n",
      "Iteration: 430, loss 2.5900486434693448e-05\n",
      "Iteration: 431, loss 2.5140690922853537e-05\n",
      "Iteration: 432, loss 2.4399912945227697e-05\n",
      "Iteration: 433, loss 2.3683920517214574e-05\n",
      "Iteration: 434, loss 2.2987085685599595e-05\n",
      "Iteration: 435, loss 2.231372673122678e-05\n",
      "Iteration: 436, loss 2.1657122488250025e-05\n",
      "Iteration: 437, loss 2.1022218788857572e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 438, loss 2.0404375391080976e-05\n",
      "Iteration: 439, loss 1.980578963411972e-05\n",
      "Iteration: 440, loss 1.922477713378612e-05\n",
      "Iteration: 441, loss 1.8663065930013545e-05\n",
      "Iteration: 442, loss 1.811381480365526e-05\n",
      "Iteration: 443, loss 1.758452708600089e-05\n",
      "Iteration: 444, loss 1.7068123270291835e-05\n",
      "Iteration: 445, loss 1.6567179045523517e-05\n",
      "Iteration: 446, loss 1.6081217836472206e-05\n",
      "Iteration: 447, loss 1.5611496564815752e-05\n",
      "Iteration: 448, loss 1.5154952052398585e-05\n",
      "Iteration: 449, loss 1.4710301911691204e-05\n",
      "Iteration: 450, loss 1.4278889466368128e-05\n",
      "Iteration: 451, loss 1.3862682862963993e-05\n",
      "Iteration: 452, loss 1.3457021850626916e-05\n",
      "Iteration: 453, loss 1.3064175618637819e-05\n",
      "Iteration: 454, loss 1.268195228476543e-05\n",
      "Iteration: 455, loss 1.230992711498402e-05\n",
      "Iteration: 456, loss 1.1950630323553924e-05\n",
      "Iteration: 457, loss 1.1602055565163027e-05\n",
      "Iteration: 458, loss 1.1264321074122563e-05\n",
      "Iteration: 459, loss 1.0935286809399258e-05\n",
      "Iteration: 460, loss 1.0615027349558659e-05\n",
      "Iteration: 461, loss 1.0304727766197175e-05\n",
      "Iteration: 462, loss 1.0004552677855827e-05\n",
      "Iteration: 463, loss 9.712473911349662e-06\n",
      "Iteration: 464, loss 9.429073543287814e-06\n",
      "Iteration: 465, loss 9.153715836873744e-06\n",
      "Iteration: 466, loss 8.886599061952438e-06\n",
      "Iteration: 467, loss 8.62732940731803e-06\n",
      "Iteration: 468, loss 8.377353879041038e-06\n",
      "Iteration: 469, loss 8.133409210131504e-06\n",
      "Iteration: 470, loss 7.89621572039323e-06\n",
      "Iteration: 471, loss 7.665630619158037e-06\n",
      "Iteration: 472, loss 7.4432255132705905e-06\n",
      "Iteration: 473, loss 7.226271918625571e-06\n",
      "Iteration: 474, loss 7.015476967353607e-06\n",
      "Iteration: 475, loss 6.812839728809195e-06\n",
      "Iteration: 476, loss 6.613965979340719e-06\n",
      "Iteration: 477, loss 6.422164005925879e-06\n",
      "Iteration: 478, loss 6.234611646505073e-06\n",
      "Iteration: 479, loss 6.053649940440664e-06\n",
      "Iteration: 480, loss 5.877901458006818e-06\n",
      "Iteration: 481, loss 5.707105628971476e-06\n",
      "Iteration: 482, loss 5.5413479458366055e-06\n",
      "Iteration: 483, loss 5.379734375310363e-06\n",
      "Iteration: 484, loss 5.2240789045754354e-06\n",
      "Iteration: 485, loss 5.071831765235402e-06\n",
      "Iteration: 486, loss 4.925841039948864e-06\n",
      "Iteration: 487, loss 4.782461019203765e-06\n",
      "Iteration: 488, loss 4.643321062758332e-06\n",
      "Iteration: 489, loss 4.508669917413499e-06\n",
      "Iteration: 490, loss 4.377914592623711e-06\n",
      "Iteration: 491, loss 4.251472091709729e-06\n",
      "Iteration: 492, loss 4.127475676796166e-06\n",
      "Iteration: 493, loss 4.0081563383864705e-06\n",
      "Iteration: 494, loss 3.892012955475366e-06\n",
      "Iteration: 495, loss 3.779221515287645e-06\n",
      "Iteration: 496, loss 3.6696803817903856e-06\n",
      "Iteration: 497, loss 3.563015525287483e-06\n",
      "Iteration: 498, loss 3.460785819697776e-06\n",
      "Iteration: 499, loss 3.359441507200245e-06\n"
     ]
    }
   ],
   "source": [
    "for i in range(500):\n",
    "    pred = model(X)\n",
    "    loss = loss_function(pred, Y)\n",
    "    print('Iteration: {}, loss {}'.format(i, loss.item()))\n",
    "    optimizer.zero_grad() # zero the gradients before the computation\n",
    "    loss.backward() # compute gradients for all learnable parameters in the model\n",
    "    optimizer.step() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example using Control Flow + Weight Sharing via PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicNet(torch.nn.Module): # 3 layer NN\n",
    "    def __init__(self, D1, H, D2):\n",
    "        super(DynamicNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D1, H)\n",
    "        self.linear2 = torch.nn.Linear(H, H)\n",
    "        self.linear3 = torch.nn.Linear(H, D2)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        phi = self.linear1(X).clamp(min=0)\n",
    "        for _ in range(np.random.randint(0, 3)): # randomly change the layer structure\n",
    "            phi = self.linear2(phi).clamp(min=0)\n",
    "        pred = self.linear3(phi)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DynamicNet(D1, H, D2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, loss 677.4867553710938\n",
      "Iteration: 1, loss 698.7186279296875\n",
      "Iteration: 2, loss 674.6714477539062\n",
      "Iteration: 3, loss 672.2929077148438\n",
      "Iteration: 4, loss 677.2782592773438\n",
      "Iteration: 5, loss 668.3834838867188\n",
      "Iteration: 6, loss 663.9821166992188\n",
      "Iteration: 7, loss 641.7876586914062\n",
      "Iteration: 8, loss 658.2225341796875\n",
      "Iteration: 9, loss 481.8358154296875\n",
      "Iteration: 10, loss 601.582275390625\n",
      "Iteration: 11, loss 649.9966430664062\n",
      "Iteration: 12, loss 398.4241638183594\n",
      "Iteration: 13, loss 561.011474609375\n",
      "Iteration: 14, loss 639.14111328125\n",
      "Iteration: 15, loss 298.14892578125\n",
      "Iteration: 16, loss 627.719482421875\n",
      "Iteration: 17, loss 226.00283813476562\n",
      "Iteration: 18, loss 611.1524658203125\n",
      "Iteration: 19, loss 156.58444213867188\n",
      "Iteration: 20, loss 446.5323181152344\n",
      "Iteration: 21, loss 573.2296142578125\n",
      "Iteration: 22, loss 552.8634643554688\n",
      "Iteration: 23, loss 365.3282470703125\n",
      "Iteration: 24, loss 494.82330322265625\n",
      "Iteration: 25, loss 455.7518005371094\n",
      "Iteration: 26, loss 130.2053985595703\n",
      "Iteration: 27, loss 129.50245666503906\n",
      "Iteration: 28, loss 342.17486572265625\n",
      "Iteration: 29, loss 310.3609313964844\n",
      "Iteration: 30, loss 218.43133544921875\n",
      "Iteration: 31, loss 244.1015167236328\n",
      "Iteration: 32, loss 212.99801635742188\n",
      "Iteration: 33, loss 179.59385681152344\n",
      "Iteration: 34, loss 170.502197265625\n",
      "Iteration: 35, loss 134.07810974121094\n",
      "Iteration: 36, loss 135.37208557128906\n",
      "Iteration: 37, loss 116.95181274414062\n",
      "Iteration: 38, loss 100.49382019042969\n",
      "Iteration: 39, loss 104.7008285522461\n",
      "Iteration: 40, loss 84.83441162109375\n",
      "Iteration: 41, loss 59.69314956665039\n",
      "Iteration: 42, loss 83.8707275390625\n",
      "Iteration: 43, loss 92.24076080322266\n",
      "Iteration: 44, loss 38.67863082885742\n",
      "Iteration: 45, loss 38.45375061035156\n",
      "Iteration: 46, loss 31.920337677001953\n",
      "Iteration: 47, loss 113.98774719238281\n",
      "Iteration: 48, loss 64.93788146972656\n",
      "Iteration: 49, loss 76.51250457763672\n",
      "Iteration: 50, loss 62.04633712768555\n",
      "Iteration: 51, loss 50.74771499633789\n",
      "Iteration: 52, loss 43.94889450073242\n",
      "Iteration: 53, loss 42.34413528442383\n",
      "Iteration: 54, loss 75.78499603271484\n",
      "Iteration: 55, loss 41.90818405151367\n",
      "Iteration: 56, loss 61.784461975097656\n",
      "Iteration: 57, loss 34.686363220214844\n",
      "Iteration: 58, loss 36.864532470703125\n",
      "Iteration: 59, loss 45.67074966430664\n",
      "Iteration: 60, loss 50.939170837402344\n",
      "Iteration: 61, loss 30.48660659790039\n",
      "Iteration: 62, loss 24.408946990966797\n",
      "Iteration: 63, loss 159.44198608398438\n",
      "Iteration: 64, loss 56.55107498168945\n",
      "Iteration: 65, loss 22.361520767211914\n",
      "Iteration: 66, loss 314.1127014160156\n",
      "Iteration: 67, loss 28.947874069213867\n",
      "Iteration: 68, loss 19.95779800415039\n",
      "Iteration: 69, loss 123.76192474365234\n",
      "Iteration: 70, loss 102.92573547363281\n",
      "Iteration: 71, loss 77.0912094116211\n",
      "Iteration: 72, loss 51.91767501831055\n",
      "Iteration: 73, loss 97.53665161132812\n",
      "Iteration: 74, loss 50.43962097167969\n",
      "Iteration: 75, loss 55.79093933105469\n",
      "Iteration: 76, loss 52.30821990966797\n",
      "Iteration: 77, loss 32.944698333740234\n",
      "Iteration: 78, loss 18.803874969482422\n",
      "Iteration: 79, loss 54.355934143066406\n",
      "Iteration: 80, loss 110.13998413085938\n",
      "Iteration: 81, loss 28.371212005615234\n",
      "Iteration: 82, loss 64.43748474121094\n",
      "Iteration: 83, loss 88.44733428955078\n",
      "Iteration: 84, loss 38.968597412109375\n",
      "Iteration: 85, loss 71.11776733398438\n",
      "Iteration: 86, loss 51.68003463745117\n",
      "Iteration: 87, loss 35.05738067626953\n",
      "Iteration: 88, loss 25.572690963745117\n",
      "Iteration: 89, loss 56.481563568115234\n",
      "Iteration: 90, loss 50.60197067260742\n",
      "Iteration: 91, loss 24.490032196044922\n",
      "Iteration: 92, loss 14.656547546386719\n",
      "Iteration: 93, loss 38.8614501953125\n",
      "Iteration: 94, loss 19.17043113708496\n",
      "Iteration: 95, loss 36.1490592956543\n",
      "Iteration: 96, loss 20.865253448486328\n",
      "Iteration: 97, loss 14.882375717163086\n",
      "Iteration: 98, loss 19.3078670501709\n",
      "Iteration: 99, loss 21.23397445678711\n",
      "Iteration: 100, loss 25.322017669677734\n",
      "Iteration: 101, loss 14.409136772155762\n",
      "Iteration: 102, loss 13.818056106567383\n",
      "Iteration: 103, loss 9.578070640563965\n",
      "Iteration: 104, loss 11.4760160446167\n",
      "Iteration: 105, loss 13.464326858520508\n",
      "Iteration: 106, loss 10.6031494140625\n",
      "Iteration: 107, loss 6.786138534545898\n",
      "Iteration: 108, loss 7.668867588043213\n",
      "Iteration: 109, loss 4.075527667999268\n",
      "Iteration: 110, loss 8.733244895935059\n",
      "Iteration: 111, loss 13.113149642944336\n",
      "Iteration: 112, loss 5.377344131469727\n",
      "Iteration: 113, loss 6.664988040924072\n",
      "Iteration: 114, loss 6.061556339263916\n",
      "Iteration: 115, loss 4.881047248840332\n",
      "Iteration: 116, loss 5.998787879943848\n",
      "Iteration: 117, loss 3.683840751647949\n",
      "Iteration: 118, loss 7.096861839294434\n",
      "Iteration: 119, loss 5.6741533279418945\n",
      "Iteration: 120, loss 2.701232433319092\n",
      "Iteration: 121, loss 4.79149866104126\n",
      "Iteration: 122, loss 4.138665676116943\n",
      "Iteration: 123, loss 4.438026428222656\n",
      "Iteration: 124, loss 1.8525099754333496\n",
      "Iteration: 125, loss 2.972809314727783\n",
      "Iteration: 126, loss 3.934462070465088\n",
      "Iteration: 127, loss 3.3494954109191895\n",
      "Iteration: 128, loss 2.0505330562591553\n",
      "Iteration: 129, loss 2.087932825088501\n",
      "Iteration: 130, loss 3.221701145172119\n",
      "Iteration: 131, loss 1.4147411584854126\n",
      "Iteration: 132, loss 1.105116605758667\n",
      "Iteration: 133, loss 0.9417572021484375\n",
      "Iteration: 134, loss 0.8911739587783813\n",
      "Iteration: 135, loss 0.8132205009460449\n",
      "Iteration: 136, loss 0.6411672234535217\n",
      "Iteration: 137, loss 2.948232650756836\n",
      "Iteration: 138, loss 2.3659121990203857\n",
      "Iteration: 139, loss 2.591174840927124\n",
      "Iteration: 140, loss 2.1446313858032227\n",
      "Iteration: 141, loss 2.26008939743042\n",
      "Iteration: 142, loss 2.1261000633239746\n",
      "Iteration: 143, loss 2.314992904663086\n",
      "Iteration: 144, loss 1.5964330434799194\n",
      "Iteration: 145, loss 1.8933361768722534\n",
      "Iteration: 146, loss 1.7882212400436401\n",
      "Iteration: 147, loss 0.47017666697502136\n",
      "Iteration: 148, loss 1.8861790895462036\n",
      "Iteration: 149, loss 0.6990886330604553\n",
      "Iteration: 150, loss 2.5879838466644287\n",
      "Iteration: 151, loss 1.029515027999878\n",
      "Iteration: 152, loss 1.059363603591919\n",
      "Iteration: 153, loss 1.1987671852111816\n",
      "Iteration: 154, loss 1.0957062244415283\n",
      "Iteration: 155, loss 0.7507573962211609\n",
      "Iteration: 156, loss 0.8860232830047607\n",
      "Iteration: 157, loss 0.3861067295074463\n",
      "Iteration: 158, loss 2.7729954719543457\n",
      "Iteration: 159, loss 1.1333467960357666\n",
      "Iteration: 160, loss 0.31671932339668274\n",
      "Iteration: 161, loss 0.7431005239486694\n",
      "Iteration: 162, loss 0.3459242582321167\n",
      "Iteration: 163, loss 0.3876992464065552\n",
      "Iteration: 164, loss 0.4090942144393921\n",
      "Iteration: 165, loss 0.36768487095832825\n",
      "Iteration: 166, loss 0.4095788896083832\n",
      "Iteration: 167, loss 0.26392850279808044\n",
      "Iteration: 168, loss 0.33859384059906006\n",
      "Iteration: 169, loss 1.9248126745224\n",
      "Iteration: 170, loss 0.34249234199523926\n",
      "Iteration: 171, loss 1.8023638725280762\n",
      "Iteration: 172, loss 1.6661338806152344\n",
      "Iteration: 173, loss 1.4444977045059204\n",
      "Iteration: 174, loss 0.5680373907089233\n",
      "Iteration: 175, loss 0.632997989654541\n",
      "Iteration: 176, loss 1.0908085107803345\n",
      "Iteration: 177, loss 0.7119535207748413\n",
      "Iteration: 178, loss 0.9330543279647827\n",
      "Iteration: 179, loss 0.8175445795059204\n",
      "Iteration: 180, loss 0.5577245354652405\n",
      "Iteration: 181, loss 0.8778613209724426\n",
      "Iteration: 182, loss 0.832299530506134\n",
      "Iteration: 183, loss 0.71799635887146\n",
      "Iteration: 184, loss 0.4101036787033081\n",
      "Iteration: 185, loss 0.7135466933250427\n",
      "Iteration: 186, loss 0.7012303471565247\n",
      "Iteration: 187, loss 0.6386659145355225\n",
      "Iteration: 188, loss 0.5569066405296326\n",
      "Iteration: 189, loss 0.34025973081588745\n",
      "Iteration: 190, loss 0.3136264979839325\n",
      "Iteration: 191, loss 0.904739260673523\n",
      "Iteration: 192, loss 0.2254752665758133\n",
      "Iteration: 193, loss 0.5936075448989868\n",
      "Iteration: 194, loss 0.14914235472679138\n",
      "Iteration: 195, loss 0.44864422082901\n",
      "Iteration: 196, loss 0.7927065491676331\n",
      "Iteration: 197, loss 0.4682729244232178\n",
      "Iteration: 198, loss 0.1252824366092682\n",
      "Iteration: 199, loss 0.11133965104818344\n",
      "Iteration: 200, loss 0.4547502100467682\n",
      "Iteration: 201, loss 0.08690645545721054\n",
      "Iteration: 202, loss 0.08334062993526459\n",
      "Iteration: 203, loss 0.6951662302017212\n",
      "Iteration: 204, loss 0.661899983882904\n",
      "Iteration: 205, loss 0.43924468755722046\n",
      "Iteration: 206, loss 0.5388262867927551\n",
      "Iteration: 207, loss 0.47504571080207825\n",
      "Iteration: 208, loss 0.548499584197998\n",
      "Iteration: 209, loss 0.5621503591537476\n",
      "Iteration: 210, loss 0.5390592813491821\n",
      "Iteration: 211, loss 0.3566906154155731\n",
      "Iteration: 212, loss 0.4905637204647064\n",
      "Iteration: 213, loss 0.44744545221328735\n",
      "Iteration: 214, loss 0.3909989893436432\n",
      "Iteration: 215, loss 0.47629573941230774\n",
      "Iteration: 216, loss 0.3231746256351471\n",
      "Iteration: 217, loss 0.23221802711486816\n",
      "Iteration: 218, loss 0.2042403668165207\n",
      "Iteration: 219, loss 0.2603280544281006\n",
      "Iteration: 220, loss 0.2415057271718979\n",
      "Iteration: 221, loss 0.12969954311847687\n",
      "Iteration: 222, loss 0.11167284101247787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 223, loss 0.20190034806728363\n",
      "Iteration: 224, loss 0.19052660465240479\n",
      "Iteration: 225, loss 0.07675222307443619\n",
      "Iteration: 226, loss 0.9073489308357239\n",
      "Iteration: 227, loss 0.8573487997055054\n",
      "Iteration: 228, loss 0.1881355345249176\n",
      "Iteration: 229, loss 0.6658128499984741\n",
      "Iteration: 230, loss 0.23005148768424988\n",
      "Iteration: 231, loss 0.11765623092651367\n",
      "Iteration: 232, loss 0.12717142701148987\n",
      "Iteration: 233, loss 0.11684507876634598\n",
      "Iteration: 234, loss 0.09383505582809448\n",
      "Iteration: 235, loss 0.41961756348609924\n",
      "Iteration: 236, loss 0.06217259168624878\n",
      "Iteration: 237, loss 0.30967479944229126\n",
      "Iteration: 238, loss 0.3754677474498749\n",
      "Iteration: 239, loss 0.30436986684799194\n",
      "Iteration: 240, loss 0.07144042104482651\n",
      "Iteration: 241, loss 0.28662416338920593\n",
      "Iteration: 242, loss 0.38532277941703796\n",
      "Iteration: 243, loss 0.25485536456108093\n",
      "Iteration: 244, loss 0.2425464242696762\n",
      "Iteration: 245, loss 0.22224998474121094\n",
      "Iteration: 246, loss 0.1965772807598114\n",
      "Iteration: 247, loss 0.17195387184619904\n",
      "Iteration: 248, loss 0.10641424357891083\n",
      "Iteration: 249, loss 0.13748645782470703\n",
      "Iteration: 250, loss 0.6223057508468628\n",
      "Iteration: 251, loss 0.10163958370685577\n",
      "Iteration: 252, loss 0.1134459599852562\n",
      "Iteration: 253, loss 0.07222936302423477\n",
      "Iteration: 254, loss 0.5588093400001526\n",
      "Iteration: 255, loss 0.5047929883003235\n",
      "Iteration: 256, loss 0.042449548840522766\n",
      "Iteration: 257, loss 0.20743373036384583\n",
      "Iteration: 258, loss 0.346994549036026\n",
      "Iteration: 259, loss 0.3051859736442566\n",
      "Iteration: 260, loss 0.08733813464641571\n",
      "Iteration: 261, loss 0.2268170863389969\n",
      "Iteration: 262, loss 0.11531893908977509\n",
      "Iteration: 263, loss 0.1715526282787323\n",
      "Iteration: 264, loss 0.14868047833442688\n",
      "Iteration: 265, loss 0.12566323578357697\n",
      "Iteration: 266, loss 0.15962228178977966\n",
      "Iteration: 267, loss 0.09246540814638138\n",
      "Iteration: 268, loss 0.16264651715755463\n",
      "Iteration: 269, loss 0.07459256798028946\n",
      "Iteration: 270, loss 0.5621644854545593\n",
      "Iteration: 271, loss 0.12276077270507812\n",
      "Iteration: 272, loss 0.10052280128002167\n",
      "Iteration: 273, loss 0.09764604270458221\n",
      "Iteration: 274, loss 0.10384034365415573\n",
      "Iteration: 275, loss 0.09902991354465485\n",
      "Iteration: 276, loss 0.052350133657455444\n",
      "Iteration: 277, loss 0.4330003559589386\n",
      "Iteration: 278, loss 0.40680423378944397\n",
      "Iteration: 279, loss 0.3594105839729309\n",
      "Iteration: 280, loss 0.3082064390182495\n",
      "Iteration: 281, loss 0.2582870125770569\n",
      "Iteration: 282, loss 0.213101327419281\n",
      "Iteration: 283, loss 0.17975781857967377\n",
      "Iteration: 284, loss 0.15862992405891418\n",
      "Iteration: 285, loss 0.5391531586647034\n",
      "Iteration: 286, loss 0.12881669402122498\n",
      "Iteration: 287, loss 0.5698608160018921\n",
      "Iteration: 288, loss 0.1040588989853859\n",
      "Iteration: 289, loss 0.09749702364206314\n",
      "Iteration: 290, loss 0.474134624004364\n",
      "Iteration: 291, loss 0.10244543850421906\n",
      "Iteration: 292, loss 0.22008900344371796\n",
      "Iteration: 293, loss 0.12120229005813599\n",
      "Iteration: 294, loss 0.3222845494747162\n",
      "Iteration: 295, loss 0.13895925879478455\n",
      "Iteration: 296, loss 0.264180988073349\n",
      "Iteration: 297, loss 0.15586520731449127\n",
      "Iteration: 298, loss 0.08362485468387604\n",
      "Iteration: 299, loss 0.20724807679653168\n",
      "Iteration: 300, loss 0.1865644007921219\n",
      "Iteration: 301, loss 0.2062036693096161\n",
      "Iteration: 302, loss 0.14680498838424683\n",
      "Iteration: 303, loss 0.05561323091387749\n",
      "Iteration: 304, loss 0.05445778742432594\n",
      "Iteration: 305, loss 0.2546458840370178\n",
      "Iteration: 306, loss 0.24413996934890747\n",
      "Iteration: 307, loss 0.04096277430653572\n",
      "Iteration: 308, loss 0.2031591385602951\n",
      "Iteration: 309, loss 0.15527687966823578\n",
      "Iteration: 310, loss 0.16165366768836975\n",
      "Iteration: 311, loss 0.15698805451393127\n",
      "Iteration: 312, loss 0.03696378692984581\n",
      "Iteration: 313, loss 0.16099900007247925\n",
      "Iteration: 314, loss 0.14299550652503967\n",
      "Iteration: 315, loss 0.14932115375995636\n",
      "Iteration: 316, loss 0.13942739367485046\n",
      "Iteration: 317, loss 0.13713106513023376\n",
      "Iteration: 318, loss 0.1264483481645584\n",
      "Iteration: 319, loss 0.04663200303912163\n",
      "Iteration: 320, loss 0.17499729990959167\n",
      "Iteration: 321, loss 0.043731361627578735\n",
      "Iteration: 322, loss 0.17323076725006104\n",
      "Iteration: 323, loss 0.0324457623064518\n",
      "Iteration: 324, loss 0.11415347456932068\n",
      "Iteration: 325, loss 0.11219269782304764\n",
      "Iteration: 326, loss 0.1496715247631073\n",
      "Iteration: 327, loss 0.14489002525806427\n",
      "Iteration: 328, loss 0.12204322963953018\n",
      "Iteration: 329, loss 0.021910468116402626\n",
      "Iteration: 330, loss 0.12208893150091171\n",
      "Iteration: 331, loss 0.021278418600559235\n",
      "Iteration: 332, loss 0.11128173768520355\n",
      "Iteration: 333, loss 0.13909506797790527\n",
      "Iteration: 334, loss 0.13917043805122375\n",
      "Iteration: 335, loss 0.10398189723491669\n",
      "Iteration: 336, loss 0.12750287353992462\n",
      "Iteration: 337, loss 0.020180918276309967\n",
      "Iteration: 338, loss 0.02064491994678974\n",
      "Iteration: 339, loss 0.10519308596849442\n",
      "Iteration: 340, loss 0.12343508750200272\n",
      "Iteration: 341, loss 0.09131363779306412\n",
      "Iteration: 342, loss 0.02220105566084385\n",
      "Iteration: 343, loss 0.1368449479341507\n",
      "Iteration: 344, loss 0.08103066682815552\n",
      "Iteration: 345, loss 0.0778946578502655\n",
      "Iteration: 346, loss 0.021857427433133125\n",
      "Iteration: 347, loss 0.1416287124156952\n",
      "Iteration: 348, loss 0.06961862742900848\n",
      "Iteration: 349, loss 0.13614633679389954\n",
      "Iteration: 350, loss 0.12858325242996216\n",
      "Iteration: 351, loss 0.11613984405994415\n",
      "Iteration: 352, loss 0.018785979598760605\n",
      "Iteration: 353, loss 0.11185534298419952\n",
      "Iteration: 354, loss 0.018909256905317307\n",
      "Iteration: 355, loss 0.017138471826910973\n",
      "Iteration: 356, loss 0.014252034947276115\n",
      "Iteration: 357, loss 0.12389954924583435\n",
      "Iteration: 358, loss 0.00852193497121334\n",
      "Iteration: 359, loss 0.10815630108118057\n",
      "Iteration: 360, loss 0.09550675749778748\n",
      "Iteration: 361, loss 0.012372082099318504\n",
      "Iteration: 362, loss 0.07091870158910751\n",
      "Iteration: 363, loss 0.06045493111014366\n",
      "Iteration: 364, loss 0.1680346131324768\n",
      "Iteration: 365, loss 0.1734248548746109\n",
      "Iteration: 366, loss 0.033246301114559174\n",
      "Iteration: 367, loss 0.04485742747783661\n",
      "Iteration: 368, loss 0.1497908979654312\n",
      "Iteration: 369, loss 0.136668398976326\n",
      "Iteration: 370, loss 0.016374170780181885\n",
      "Iteration: 371, loss 0.07523123919963837\n",
      "Iteration: 372, loss 0.08214332908391953\n",
      "Iteration: 373, loss 0.012965168803930283\n",
      "Iteration: 374, loss 0.011339223943650723\n",
      "Iteration: 375, loss 0.0764421746134758\n",
      "Iteration: 376, loss 0.0691835805773735\n",
      "Iteration: 377, loss 0.05869821086525917\n",
      "Iteration: 378, loss 0.04776431992650032\n",
      "Iteration: 379, loss 0.02178369276225567\n",
      "Iteration: 380, loss 0.160064697265625\n",
      "Iteration: 381, loss 0.02798282355070114\n",
      "Iteration: 382, loss 0.030999701470136642\n",
      "Iteration: 383, loss 0.15841077268123627\n",
      "Iteration: 384, loss 0.02976401150226593\n",
      "Iteration: 385, loss 0.015779927372932434\n",
      "Iteration: 386, loss 0.13721777498722076\n",
      "Iteration: 387, loss 0.12577314674854279\n",
      "Iteration: 388, loss 0.010160318575799465\n",
      "Iteration: 389, loss 0.06394167244434357\n",
      "Iteration: 390, loss 0.0693986713886261\n",
      "Iteration: 391, loss 0.06692446023225784\n",
      "Iteration: 392, loss 0.05876711755990982\n",
      "Iteration: 393, loss 0.008644898422062397\n",
      "Iteration: 394, loss 0.10995415598154068\n",
      "Iteration: 395, loss 0.009954673238098621\n",
      "Iteration: 396, loss 0.03874971717596054\n",
      "Iteration: 397, loss 0.1189952865242958\n",
      "Iteration: 398, loss 0.03649618849158287\n",
      "Iteration: 399, loss 0.03491901978850365\n",
      "Iteration: 400, loss 0.010266190394759178\n",
      "Iteration: 401, loss 0.030291488394141197\n",
      "Iteration: 402, loss 0.027625655755400658\n",
      "Iteration: 403, loss 0.024281911551952362\n",
      "Iteration: 404, loss 0.020617416128516197\n",
      "Iteration: 405, loss 0.01803911291062832\n",
      "Iteration: 406, loss 0.15593792498111725\n",
      "Iteration: 407, loss 0.014766671694815159\n",
      "Iteration: 408, loss 0.014433943666517735\n",
      "Iteration: 409, loss 0.013525936752557755\n",
      "Iteration: 410, loss 0.018775245174765587\n",
      "Iteration: 411, loss 0.017850518226623535\n",
      "Iteration: 412, loss 0.015479723922908306\n",
      "Iteration: 413, loss 0.013253314420580864\n",
      "Iteration: 414, loss 0.010639333166182041\n",
      "Iteration: 415, loss 0.014753912575542927\n",
      "Iteration: 416, loss 0.14925439655780792\n",
      "Iteration: 417, loss 0.006045146845281124\n",
      "Iteration: 418, loss 0.004956630989909172\n",
      "Iteration: 419, loss 0.12915168702602386\n",
      "Iteration: 420, loss 0.029084082692861557\n",
      "Iteration: 421, loss 0.004854472819715738\n",
      "Iteration: 422, loss 0.10673458129167557\n",
      "Iteration: 423, loss 0.09858258813619614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 424, loss 0.009740139357745647\n",
      "Iteration: 425, loss 0.011780476197600365\n",
      "Iteration: 426, loss 0.07630881667137146\n",
      "Iteration: 427, loss 0.06987674534320831\n",
      "Iteration: 428, loss 0.06270827353000641\n",
      "Iteration: 429, loss 0.021902523934841156\n",
      "Iteration: 430, loss 0.09731347858905792\n",
      "Iteration: 431, loss 0.09522297233343124\n",
      "Iteration: 432, loss 0.01769595593214035\n",
      "Iteration: 433, loss 0.05272418633103371\n",
      "Iteration: 434, loss 0.012199736200273037\n",
      "Iteration: 435, loss 0.05659615620970726\n",
      "Iteration: 436, loss 0.0559467189013958\n",
      "Iteration: 437, loss 0.008888212963938713\n",
      "Iteration: 438, loss 0.06985138356685638\n",
      "Iteration: 439, loss 0.006937732454389334\n",
      "Iteration: 440, loss 0.005917607806622982\n",
      "Iteration: 441, loss 0.05449575558304787\n",
      "Iteration: 442, loss 0.05216658115386963\n",
      "Iteration: 443, loss 0.004408626351505518\n",
      "Iteration: 444, loss 0.004428423009812832\n",
      "Iteration: 445, loss 0.07483286410570145\n",
      "Iteration: 446, loss 0.0713181346654892\n",
      "Iteration: 447, loss 0.0629725307226181\n",
      "Iteration: 448, loss 0.05519028753042221\n",
      "Iteration: 449, loss 0.059462666511535645\n",
      "Iteration: 450, loss 0.048422638326883316\n",
      "Iteration: 451, loss 0.04382965341210365\n",
      "Iteration: 452, loss 0.037684135138988495\n",
      "Iteration: 453, loss 0.07859963923692703\n",
      "Iteration: 454, loss 0.029408486559987068\n",
      "Iteration: 455, loss 0.026157526299357414\n",
      "Iteration: 456, loss 0.026065729558467865\n",
      "Iteration: 457, loss 0.02056523784995079\n",
      "Iteration: 458, loss 0.018067218363285065\n",
      "Iteration: 459, loss 0.09898260980844498\n",
      "Iteration: 460, loss 0.02732831798493862\n",
      "Iteration: 461, loss 0.02367176115512848\n",
      "Iteration: 462, loss 0.017680831253528595\n",
      "Iteration: 463, loss 0.08709056675434113\n",
      "Iteration: 464, loss 0.01263625081628561\n",
      "Iteration: 465, loss 0.025908755138516426\n",
      "Iteration: 466, loss 0.07336769998073578\n",
      "Iteration: 467, loss 0.03147990629076958\n",
      "Iteration: 468, loss 0.06512028723955154\n",
      "Iteration: 469, loss 0.06038161367177963\n",
      "Iteration: 470, loss 0.04100312665104866\n",
      "Iteration: 471, loss 0.05185776576399803\n",
      "Iteration: 472, loss 0.008924858644604683\n",
      "Iteration: 473, loss 0.04852234572172165\n",
      "Iteration: 474, loss 0.008433233946561813\n",
      "Iteration: 475, loss 0.04595182463526726\n",
      "Iteration: 476, loss 0.006260325200855732\n",
      "Iteration: 477, loss 0.03800470754504204\n",
      "Iteration: 478, loss 0.056717537343502045\n",
      "Iteration: 479, loss 0.058165621012449265\n",
      "Iteration: 480, loss 0.004781216848641634\n",
      "Iteration: 481, loss 0.0041771261021494865\n",
      "Iteration: 482, loss 0.05394306778907776\n",
      "Iteration: 483, loss 0.04843800142407417\n",
      "Iteration: 484, loss 0.04313846305012703\n",
      "Iteration: 485, loss 0.03879673406481743\n",
      "Iteration: 486, loss 0.007017957512289286\n",
      "Iteration: 487, loss 0.06797102838754654\n",
      "Iteration: 488, loss 0.0087747136130929\n",
      "Iteration: 489, loss 0.06918901950120926\n",
      "Iteration: 490, loss 0.005499761085957289\n",
      "Iteration: 491, loss 0.0037998126354068518\n",
      "Iteration: 492, loss 0.002831975696608424\n",
      "Iteration: 493, loss 0.034558020532131195\n",
      "Iteration: 494, loss 0.002722532721236348\n",
      "Iteration: 495, loss 0.002826008712872863\n",
      "Iteration: 496, loss 0.0026516627985984087\n",
      "Iteration: 497, loss 0.04618225246667862\n",
      "Iteration: 498, loss 0.043455831706523895\n",
      "Iteration: 499, loss 0.04177006706595421\n"
     ]
    }
   ],
   "source": [
    "for i in range(500):\n",
    "    pred = model(X)\n",
    "    loss = loss_function(pred, Y)\n",
    "    print('Iteration: {}, loss {}'.format(i, loss.item()))\n",
    "    optimizer.zero_grad() # zero the gradients before the computation\n",
    "    loss.backward() # compute gradients for all learnable parameters in the model\n",
    "    optimizer.step() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
